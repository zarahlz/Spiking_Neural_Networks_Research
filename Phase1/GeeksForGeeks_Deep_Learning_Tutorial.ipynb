{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3enf5CQ7U6y"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/eb/GeeksForGeeks_logo.png\" width=\"240\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <font size=\"8\"><b>Deep Learning Tutorial</b></font><br>\n",
        "  <font size=\"5\"><i>Concise Explanations, Key Concepts, and Essential Formulas for Fast Mastery</i></font><br><br>\n",
        "</p>\n",
        "\n",
        "<blockquote>\n",
        "  This notebook distills the GeeksforGeeks Deep Learning tutorials into a focused, high-efficiency format.  \n",
        "  It is designed to provide clear theoretical insight and all essential formulas for quick learning, effective revision, and technical interviews.\n",
        "</blockquote>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Table of Contents\n",
        "\n",
        "## Introduction to Neural Networks:\n",
        "[Neural Networks](https://www.geeksforgeeks.org/neural-networks-a-beginners-guide/)  \n",
        "[Perceptron](https://www.geeksforgeeks.org/what-is-perceptron-the-simplest-artificial-neural-network/)  \n",
        "[Feedforward Neural Networks](https://www.geeksforgeeks.org/feedforward-neural-network/)  \n",
        "[Single-Layer Perceptron](https://www.geeksforgeeks.org/single-layer-perceptron-in-tensorflow/)  \n",
        "[Multi-Layer Perceptron](https://www.geeksforgeeks.org/multi-layer-perceptron-learning-in-tensorflow/)  \n",
        "[Convolutional Neural Networks](https://www.geeksforgeeks.org/introduction-convolution-neural-network/)  \n",
        "[Recurrent Neural Networks](https://www.geeksforgeeks.org/introduction-to-recurrent-neural-network/)  \n",
        "[Long Short-Term Memory (LSTM)](https://www.geeksforgeeks.org/deep-learning-introduction-to-long-short-term-memory/)  \n",
        "[Biological Neurons vs Artificial Neurons](https://www.geeksforgeeks.org/difference-between-ann-and-bnn/)  \n",
        "[Artificial Neural Networks (ANNs)](https://www.geeksforgeeks.org/artificial-neural-networks-and-its-applications/)  \n",
        "\n",
        "### Basic Components of Neural Networks:\n",
        "[Layers in Neural Networks](https://www.geeksforgeeks.org/layers-in-artificial-neural-networks-ann/)  \n",
        "[Weights and Biases](https://www.geeksforgeeks.org/the-role-of-weights-and-bias-in-neural-networks/)  \n",
        "[Forward Propagation](https://www.geeksforgeeks.org/what-is-forward-propagation-in-neural-networks/)  \n",
        "[Activation Functions](https://www.geeksforgeeks.org/activation-functions-neural-networks/)  \n",
        "[Loss Functions](https://www.geeksforgeeks.org/loss-functions-in-deep-learning/)  \n",
        "[Backpropagation](https://www.geeksforgeeks.org/backpropagation-in-neural-network/)  \n",
        "[Learning Rate](https://www.geeksforgeeks.org/impact-of-learning-rate-on-a-model/)  \n",
        "\n",
        "### Optimization Algorithm in Deep Learning:\n",
        "[Optimization algorithms in deep learning](https://www.geeksforgeeks.org/optimization-algorithms-in-machine-learning/)  \n",
        "[Gradient Descent](https://www.geeksforgeeks.org/gradient-descent-algorithm-and-its-variants/)  \n",
        "[Stochastic Gradient Descent (SGD)](https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/)  \n",
        "[Mini-batch Gradient Descent](https://www.geeksforgeeks.org/ml-mini-batch-gradient-descent-with-python/)  \n",
        "[RMSprop (Root Mean Square Propagation)](https://www.geeksforgeeks.org/rmsprop-optimizer-in-deep-learning/)  \n",
        "[Adam (Adaptive Moment Estimation)](https://www.geeksforgeeks.org/adam-optimizer/)  \n",
        "\n",
        "## Convolutional Neural Networks (CNNs):\n",
        "[Basics of Digital Image Processing](https://www.geeksforgeeks.org/digital-image-processing-basics/)  \n",
        "[Need for CNN](https://www.geeksforgeeks.org/importance-of-convolutional-neural-network-ml/)  \n",
        "[Strides](https://www.geeksforgeeks.org/ml-introduction-to-strided-convolutions/)  \n",
        "[Padding](https://www.geeksforgeeks.org/cnn-introduction-to-padding/)  \n",
        "[Convolutional Layers](https://www.geeksforgeeks.org/what-are-convolution-layers/)  \n",
        "[Pooling Layers](https://www.geeksforgeeks.org/cnn-introduction-to-pooling-layer/)  \n",
        "[Fully Connected Layers](https://www.geeksforgeeks.org/what-is-fully-connected-layer-in-deep-learning/)  \n",
        "[Batch Normalization](https://www.geeksforgeeks.org/what-is-batch-normalization-in-cnn/)  \n",
        "[Backpropagation in CNNs](https://www.geeksforgeeks.org/backpropagation-in-convolutional-neural-networks/)  \n",
        "\n",
        "### CNN Based Architectures:\n",
        "[LeNet-5](https://www.geeksforgeeks.org/lenet-5-architecture/)  \n",
        "[AlexNet](https://www.geeksforgeeks.org/ml-getting-started-with-alexnet/)  \n",
        "[VGG-16 Network](https://www.geeksforgeeks.org/vgg-16-cnn-model/)  \n",
        "[VGG-19 Network](https://www.geeksforgeeks.org/vgg-net-architecture-explained/)  \n",
        "[GoogLeNet/Inception](https://www.geeksforgeeks.org/understanding-googlenet-model-cnn-architecture/)  \n",
        "[ResNet (Residual Network)](https://www.geeksforgeeks.org/residual-networks-resnet-deep-learning/)  \n",
        "[MobileNet](https://www.geeksforgeeks.org/mobilenet-v2-architecture-in-computer-vision/)  \n",
        "\n",
        "## Recurrent Neural Networks (RNNs):\n",
        "[Vanishing Gradient and Exploding Gradient Problem](https://www.geeksforgeeks.org/vanishing-and-exploding-gradients-problems-in-deep-learning/)  \n",
        "[How RNN Differs from Feedforward Neural Networks](https://www.geeksforgeeks.org/difference-between-feed-forward-neural-networks-and-recurrent-neural-networks/)  \n",
        "[Backpropagation Through Time (BPTT)](https://www.geeksforgeeks.org/ml-back-propagation-through-time/)  \n",
        "[Types of Recurrent Neural Networks](https://www.geeksforgeeks.org/types-of-recurrent-neural-networks-rnn-in-tensorflow/)  \n",
        "[Bidirectional RNNs](https://www.geeksforgeeks.org/bidirectional-recurrent-neural-network/)  \n",
        "[Bidirectional Long Short-Term Memory (Bi-LSTM)](https://www.geeksforgeeks.org/bidirectional-lstm-in-nlp/)  \n",
        "[Gated Recurrent Units (GRU)](https://www.geeksforgeeks.org/gated-recurrent-unit-networks/)  \n",
        "\n",
        "## Generative Models in Deep Learning:\n",
        "[Generative Adversarial Networks (GANs)](https://www.geeksforgeeks.org/generative-adversarial-network-gan/)  \n",
        "[Autoencoders](https://www.geeksforgeeks.org/auto-encoders/)  \n",
        "[Restricted Boltzmann Machines (RBMs)](https://www.geeksforgeeks.org/restricted-boltzmann-machine/)  \n",
        "\n",
        "### Variants of Generative Adversarial Networks (GANs):\n",
        "[Deep Convolutional GAN (DCGAN)](https://www.geeksforgeeks.org/deep-convolutional-gan-with-keras/)  \n",
        "[Conditional GAN (cGAN)](https://www.geeksforgeeks.org/conditional-generative-adversarial-network/)  \n",
        "[Cycle-Consistent GAN (CycleGAN)](https://www.geeksforgeeks.org/cycle-generative-adversarial-network-cyclegan-2/)  \n",
        "[Super-Resolution GAN (SRGAN)](https://www.geeksforgeeks.org/super-resolution-gan-srgan/)  \n",
        "[Wasserstein GAN (WGAN)](https://www.geeksforgeeks.org/wasserstein-generative-adversarial-networks-wgans-convergence-and-optimization/)  \n",
        "[StyleGAN](https://www.geeksforgeeks.org/stylegan-style-generative-adversarial-networks/)  \n",
        "\n",
        "### Types of Autoencoders:\n",
        "[Sparse Autoencoder](https://www.geeksforgeeks.org/sparse-autoencoders-in-deep-learning/)  \n",
        "[Denoising Autoencoder](https://www.geeksforgeeks.org/denoising-autoencoders-in-machine-learning/)  \n",
        "[Undercomplete Autoencoder](https://www.geeksforgeeks.org/undercomplete-autoencoder/)  \n",
        "[Contractive Autoencoder](https://www.geeksforgeeks.org/contractive-autoencoder-cae/)  \n",
        "[Convolutional Autoencoder](https://www.geeksforgeeks.org/implement-convolutional-autoencoder-in-pytorch-with-cuda/)  \n",
        "[Variational Autoencoder](https://www.geeksforgeeks.org/variational-autoencoders/)  \n",
        "\n",
        "## Deep Reinforcement Learning (DRL):\n",
        "[Deep Reinforcement Learning](https://www.geeksforgeeks.org/a-beginners-guide-to-deep-reinforcement-learning/)  \n",
        "[Reinforcement Learning](https://www.geeksforgeeks.org/what-is-reinforcement-learning/)  \n",
        "[Markov Decision Processes](https://www.geeksforgeeks.org/markov-decision-process/)  \n",
        "[Function Approximation](https://www.geeksforgeeks.org/function-approximation-in-reinforcement-learning/)  \n",
        "\n",
        "### Key Algorithms in Deep Reinforcement Learning:\n",
        "[Deep Q-Networks (DQN)](https://www.geeksforgeeks.org/deep-q-learning/)  \n",
        "[REINFORCE](https://www.geeksforgeeks.org/reinforce-algorithm/)  \n",
        "[Actor-Critic Methods](https://www.geeksforgeeks.org/actor-critic-algorithm-in-reinforcement-learning/)  \n",
        "[Proximal Policy Optimization (PPO)](https://www.geeksforgeeks.org/a-brief-introduction-to-proximal-policy-optimization/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Introduction to Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz7y9bt09nYE"
      },
      "source": [
        "# Neural Networks: A Beginner's Guide\n",
        "\n",
        "## Introduction to Neural Networks\n",
        "\n",
        "Neural Networks are computational models inspired by the human brain's network of neurons. They consist of layers of interconnected nodes (neurons) that process data and identify patterns. Neural networks excel in tasks like classification, regression, and function approximation.\n",
        "\n",
        "## Structure of a Neural Network\n",
        "\n",
        "A typical neural network consists of three types of layers:\n",
        "\n",
        "1. **Input Layer**: Receives the input features.\n",
        "2. **Hidden Layers**: One or more layers that perform nonlinear transformations on inputs.\n",
        "3. **Output Layer**: Produces the final output.\n",
        "\n",
        "Each neuron in a layer is connected to neurons in the subsequent layer through weighted connections.\n",
        "\n",
        "## Neuron Model and Computation\n",
        "\n",
        "Each neuron receives inputs, multiplies each input by its associated weight, adds a bias term, and then applies an activation function.\n",
        "\n",
        "The output $y$ of a single neuron is calculated as:\n",
        "\n",
        "$$\n",
        "y = \\varphi \\left( \\sum_{i=1}^n w_i x_i + b \\right)\n",
        "$$\n",
        "\n",
        "where  \n",
        "$x_i$ are inputs,  \n",
        "$w_i$ are weights,  \n",
        "$b$ is the bias term,  \n",
        "$\\varphi$ is the activation function.\n",
        "\n",
        "## Activation Functions\n",
        "\n",
        "Activation functions introduce nonlinearity into the network, enabling it to learn complex patterns. Common activation functions include:\n",
        "\n",
        "### Sigmoid Function\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Range: $(0, 1)$, useful for binary classification.\n",
        "\n",
        "### Hyperbolic Tangent (tanh)\n",
        "\n",
        "$$\n",
        "\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n",
        "$$\n",
        "\n",
        "Range: $(-1, 1)$, zero-centered output.\n",
        "\n",
        "### ReLU (Rectified Linear Unit)\n",
        "\n",
        "$$\n",
        "\\text{ReLU}(z) = \\max(0, z)\n",
        "$$\n",
        "\n",
        "Efficient and reduces vanishing gradient problems.\n",
        "\n",
        "## Forward Propagation\n",
        "\n",
        "The forward pass involves computing outputs layer by layer:\n",
        "\n",
        "For each layer $l$:\n",
        "\n",
        "$$\n",
        "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "a^{(l)} = \\varphi(z^{(l)})\n",
        "$$\n",
        "\n",
        "where  \n",
        "$W^{(l)}$ is the weight matrix of layer $l$,  \n",
        "$b^{(l)}$ is the bias vector,  \n",
        "$a^{(l-1)}$ is the activation from the previous layer,  \n",
        "$a^{(0)}$ is the input vector.\n",
        "\n",
        "## Loss Function\n",
        "\n",
        "To train the network, a loss function quantifies the difference between predicted outputs and true labels.\n",
        "\n",
        "For example, the **Mean Squared Error (MSE)** for regression is:\n",
        "\n",
        "$$\n",
        "L = \\frac{1}{m} \\sum_{j=1}^m (y_j - \\hat{y}_j)^2\n",
        "$$\n",
        "\n",
        "where  \n",
        "$m$ is the number of samples,  \n",
        "$y_j$ is the true value,  \n",
        "$\\hat{y}_j$ is the predicted value.\n",
        "\n",
        "For classification, **Cross-Entropy Loss** is often used.\n",
        "\n",
        "## Backpropagation and Gradient Descent\n",
        "\n",
        "Training updates weights to minimize the loss. The backpropagation algorithm computes gradients of the loss with respect to each weight.\n",
        "\n",
        "For a weight $w$, update rule using gradient descent is:\n",
        "\n",
        "$$\n",
        "w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}\n",
        "$$\n",
        "\n",
        "where $\\eta$ is the learning rate.\n",
        "\n",
        "The gradients are computed layer by layer from the output towards the input using the chain rule.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Neural Networks are layered structures of neurons that transform inputs through weighted sums and nonlinear activations. Training involves minimizing a loss function by updating weights using gradient descent and backpropagation.\n",
        "\n",
        "Key formulas:\n",
        "\n",
        "Neuron output:  \n",
        "$$\n",
        "y = \\varphi \\left( \\sum_{i=1}^n w_i x_i + b \\right)\n",
        "$$\n",
        "\n",
        "Activation functions:  \n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad \\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}, \\quad \\text{ReLU}(z) = \\max(0, z)\n",
        "$$\n",
        "\n",
        "Forward pass:  \n",
        "$$\n",
        "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}, \\quad a^{(l)} = \\varphi(z^{(l)})\n",
        "$$\n",
        "\n",
        "Loss function (MSE example):  \n",
        "$$\n",
        "L = \\frac{1}{m} \\sum_{j=1}^m (y_j - \\hat{y}_j)^2\n",
        "$$\n",
        "\n",
        "Gradient descent update:  \n",
        "$$\n",
        "w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXA4IESk-xZf"
      },
      "source": [
        "# Perceptron: The Simplest Artificial Neural Network\n",
        "\n",
        "## Introduction to Perceptron\n",
        "\n",
        "The Perceptron is the fundamental building block of artificial neural networks. It is a simple model designed for binary classification tasks. It takes multiple inputs, applies weights, adds a bias, and passes the result through an activation function to produce a binary output.\n",
        "\n",
        "## Structure of a Perceptron\n",
        "\n",
        "A perceptron consists of:\n",
        "\n",
        "- Input features $x_1, x_2, \\dots, x_n$\n",
        "- Associated weights $w_1, w_2, \\dots, w_n$\n",
        "- A bias term $b$\n",
        "- An activation function (usually a step function)\n",
        "\n",
        "The perceptron computes a weighted sum of the inputs plus bias and then applies the activation function to decide the output.\n",
        "\n",
        "## Perceptron Output Computation\n",
        "\n",
        "The weighted sum (also called the net input) is calculated as:\n",
        "\n",
        "$$\n",
        "z = \\sum_{i=1}^n w_i x_i + b\n",
        "$$\n",
        "\n",
        "The output $y$ is determined by the activation function, which for a basic perceptron is a step function defined as:\n",
        "\n",
        "$$\n",
        "y =\n",
        "\\begin{cases}\n",
        "1, & \\text{if } z \\geq 0 \\\\\n",
        "0, & \\text{if } z < 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "This means the perceptron outputs 1 if the weighted sum is non-negative, and 0 otherwise.\n",
        "\n",
        "## Training the Perceptron\n",
        "\n",
        "The goal is to find weights and bias that correctly classify training examples. The training uses an iterative update rule based on the perceptron learning algorithm.\n",
        "\n",
        "For each training example with input vector $\\mathbf{x}$ and target label $t$, the perceptron computes the prediction $y$. If $y$ differs from $t$, the weights and bias are updated as:\n",
        "\n",
        "$$\n",
        "w_i \\leftarrow w_i + \\eta (t - y) x_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "b \\leftarrow b + \\eta (t - y)\n",
        "$$\n",
        "\n",
        "where $\\eta$ is the learning rate, controlling the step size of updates.\n",
        "\n",
        "## Decision Boundary\n",
        "\n",
        "The perceptron defines a linear decision boundary that separates classes. The equation of the decision boundary is:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^n w_i x_i + b = 0\n",
        "$$\n",
        "\n",
        "Points on one side are classified as 1, and points on the other as 0.\n",
        "\n",
        "## Limitations\n",
        "\n",
        "The perceptron can only classify linearly separable data. For problems where classes are not linearly separable, more complex architectures like multi-layer neural networks are needed.\n",
        "\n",
        "## Summary\n",
        "\n",
        "The perceptron is a basic binary classifier that calculates a weighted sum of inputs, applies a threshold activation, and updates weights using a simple learning rule.\n",
        "\n",
        "Key formulas:\n",
        "\n",
        "Weighted sum:  \n",
        "$$\n",
        "z = \\sum_{i=1}^n w_i x_i + b\n",
        "$$\n",
        "\n",
        "Activation (step function):  \n",
        "$$\n",
        "y =\n",
        "\\begin{cases}\n",
        "1, & z \\geq 0 \\\\\n",
        "0, & z < 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Weight update rule:  \n",
        "$$\n",
        "w_i \\leftarrow w_i + \\eta (t - y) x_i\n",
        "$$\n",
        "\n",
        "Bias update rule:  \n",
        "$$\n",
        "b \\leftarrow b + \\eta (t - y)\n",
        "$$\n",
        "\n",
        "Decision boundary:  \n",
        "$$\n",
        "\\sum_{i=1}^n w_i x_i + b = 0\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feedforward Neural Network\n",
        "\n",
        "## Overview\n",
        "\n",
        "A Feedforward Neural Network (FNN) is the simplest type of artificial neural network where connections between nodes do not form cycles. Information flows in one direction—from input layer through hidden layers to the output layer.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "The network consists of an input layer, one or more hidden layers, and an output layer. Each layer contains neurons that compute a weighted sum of inputs, add a bias, and apply an activation function.\n",
        "\n",
        "## Mathematical Model\n",
        "\n",
        "For layer $l$, let the input to neurons be $a^{(l-1)}$, weights be $W^{(l)}$, and biases be $b^{(l)}$. The pre-activation vector $z^{(l)}$ is:\n",
        "\n",
        "$$\n",
        "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
        "$$\n",
        "\n",
        "The activation vector $a^{(l)}$ is computed by applying an activation function $\\varphi$ element-wise:\n",
        "\n",
        "$$\n",
        "a^{(l)} = \\varphi(z^{(l)})\n",
        "$$\n",
        "\n",
        "For the input layer, $a^{(0)}$ is the input feature vector.\n",
        "\n",
        "## Activation Functions\n",
        "\n",
        "Common activation functions $\\varphi$ include sigmoid, tanh, and ReLU, introducing nonlinearity to enable learning complex functions.\n",
        "\n",
        "## Forward Propagation Process\n",
        "\n",
        "The forward pass starts from input layer and moves sequentially through all layers:\n",
        "\n",
        "- Compute $z^{(1)} = W^{(1)} a^{(0)} + b^{(1)}$\n",
        "- Compute $a^{(1)} = \\varphi(z^{(1)})$\n",
        "- Repeat for each subsequent layer until output layer $L$:\n",
        "  \n",
        "$$\n",
        "z^{(L)} = W^{(L)} a^{(L-1)} + b^{(L)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "a^{(L)} = \\varphi(z^{(L)})\n",
        "$$\n",
        "\n",
        "The output $a^{(L)}$ represents the network prediction.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Feedforward Neural Networks process input data layer by layer in a single forward direction. Each neuron's output is a nonlinear transformation of the weighted sum of inputs plus bias.\n",
        "\n",
        "Key formulas:\n",
        "\n",
        "Neuron input at layer $l$:  \n",
        "$$\n",
        "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
        "$$\n",
        "\n",
        "Neuron activation at layer $l$:  \n",
        "$$\n",
        "a^{(l)} = \\varphi(z^{(l)})\n",
        "$$\n",
        "\n",
        "Forward pass repeats this from input layer ($l=1$) to output layer ($l=L$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Single Layer Perceptron\n",
        "\n",
        "## Concept\n",
        "\n",
        "A Single Layer Perceptron (SLP) is the simplest form of neural network with one layer of output neurons. It is a linear classifier that maps input features directly to outputs using weighted sums and a threshold activation function.\n",
        "\n",
        "## Model Description\n",
        "\n",
        "Given an input vector $\\mathbf{x} = (x_1, x_2, \\dots, x_n)$, weights $\\mathbf{w} = (w_1, w_2, \\dots, w_n)$, and bias $b$, the perceptron computes:\n",
        "\n",
        "$$\n",
        "z = \\sum_{i=1}^n w_i x_i + b\n",
        "$$\n",
        "\n",
        "The output $y$ is determined by applying a step activation function:\n",
        "\n",
        "$$\n",
        "y = \n",
        "\\begin{cases}\n",
        "1, & z \\geq 0 \\\\\n",
        "0, & z < 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "## Training Objective\n",
        "\n",
        "The SLP aims to learn weights and bias such that the linear combination separates classes correctly. This requires the data to be linearly separable.\n",
        "\n",
        "## Learning Rule\n",
        "\n",
        "The perceptron learning algorithm updates weights and bias iteratively based on the prediction error. For a training sample with input $\\mathbf{x}$ and true label $t$, if prediction $y$ differs from $t$:\n",
        "\n",
        "$$\n",
        "w_i \\leftarrow w_i + \\eta (t - y) x_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "b \\leftarrow b + \\eta (t - y)\n",
        "$$\n",
        "\n",
        "where $\\eta$ is the learning rate controlling the size of updates.\n",
        "\n",
        "## Decision Boundary\n",
        "\n",
        "The equation for the decision boundary that separates classes is:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^n w_i x_i + b = 0\n",
        "$$\n",
        "\n",
        "Points for which $z \\geq 0$ belong to one class, and those for which $z < 0$ belong to the other.\n",
        "\n",
        "## Summary\n",
        "\n",
        "The Single Layer Perceptron is a foundational linear classifier. It calculates a weighted sum of inputs, applies a threshold to produce binary outputs, and iteratively adjusts weights to minimize classification errors on linearly separable data.\n",
        "\n",
        "Key formulas:\n",
        "\n",
        "Weighted sum:  \n",
        "$$\n",
        "z = \\sum_{i=1}^n w_i x_i + b\n",
        "$$\n",
        "\n",
        "Activation function:  \n",
        "$$\n",
        "y = \n",
        "\\begin{cases}\n",
        "1, & z \\geq 0 \\\\\n",
        "0, & z < 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Weight update:  \n",
        "$$\n",
        "w_i \\leftarrow w_i + \\eta (t - y) x_i\n",
        "$$\n",
        "\n",
        "Bias update:  \n",
        "$$\n",
        "b \\leftarrow b + \\eta (t - y)\n",
        "$$\n",
        "\n",
        "Decision boundary:  \n",
        "$$\n",
        "\\sum_{i=1}^n w_i x_i + b = 0\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Layer Perceptron Learning\n",
        "\n",
        "## Overview\n",
        "\n",
        "A Multi-Layer Perceptron (MLP) is a feedforward neural network with one or more hidden layers between input and output layers. Unlike a single-layer perceptron, an MLP can model complex nonlinear decision boundaries.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "An MLP consists of:\n",
        "\n",
        "- Input layer receiving feature vector $\\mathbf{x}$\n",
        "- One or more hidden layers with neurons applying nonlinear activation\n",
        "- Output layer producing final predictions\n",
        "\n",
        "Each neuron in layer $l$ computes a weighted sum of inputs from the previous layer $(l-1)$ plus a bias:\n",
        "\n",
        "$$\n",
        "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
        "$$\n",
        "\n",
        "The activation of neurons in layer $l$ is:\n",
        "\n",
        "$$\n",
        "a^{(l)} = \\varphi(z^{(l)})\n",
        "$$\n",
        "\n",
        "where $\\varphi$ is a nonlinear activation function such as ReLU, sigmoid, or tanh.\n",
        "\n",
        "## Forward Propagation\n",
        "\n",
        "The input $\\mathbf{x}$ is fed into the network and transformed through each layer by computing:\n",
        "\n",
        "$$\n",
        "a^{(0)} = \\mathbf{x}\n",
        "$$\n",
        "\n",
        "$$\n",
        "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)} \\quad \\text{for } l=1,2,\\dots,L\n",
        "$$\n",
        "\n",
        "$$\n",
        "a^{(l)} = \\varphi(z^{(l)})\n",
        "$$\n",
        "\n",
        "The final output $a^{(L)}$ represents the network’s prediction.\n",
        "\n",
        "## Loss Function\n",
        "\n",
        "The network training aims to minimize a loss function that measures the difference between predicted output $a^{(L)}$ and true target $y$. Common loss functions include Mean Squared Error (MSE) for regression and Cross-Entropy for classification.\n",
        "\n",
        "## Backpropagation and Weight Updates\n",
        "\n",
        "MLP uses backpropagation to compute gradients of the loss function with respect to weights and biases. Using gradient descent, parameters are updated to minimize loss.\n",
        "\n",
        "For each layer $l$, weights and biases are updated as:\n",
        "\n",
        "$$\n",
        "W^{(l)} \\leftarrow W^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b^{(l)} \\leftarrow b^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b^{(l)}}\n",
        "$$\n",
        "\n",
        "where $\\eta$ is the learning rate, and $\\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}}$, $\\frac{\\partial \\mathcal{L}}{\\partial b^{(l)}}$ are the gradients of the loss $\\mathcal{L}$.\n",
        "\n",
        "## Summary\n",
        "\n",
        "The Multi-Layer Perceptron is a powerful neural network capable of learning complex patterns by stacking layers of nonlinear neurons. It uses forward propagation to compute outputs and backpropagation with gradient descent for learning.\n",
        "\n",
        "Key formulas:\n",
        "\n",
        "Neuron input:  \n",
        "$$\n",
        "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
        "$$\n",
        "\n",
        "Neuron activation:  \n",
        "$$\n",
        "a^{(l)} = \\varphi(z^{(l)})\n",
        "$$\n",
        "\n",
        "Loss minimization (gradient descent updates):  \n",
        "$$\n",
        "W^{(l)} \\leftarrow W^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b^{(l)} \\leftarrow b^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b^{(l)}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Convolutional Neural Networks\n",
        "\n",
        "## Overview\n",
        "\n",
        "Convolutional Neural Networks (CNNs) are specialized neural networks designed to process data with a grid-like structure, such as images. They efficiently capture spatial hierarchies by applying convolutional filters that detect local patterns.\n",
        "\n",
        "## Core Components\n",
        "\n",
        "CNNs mainly consist of convolutional layers, pooling layers, and fully connected layers. The convolutional layers extract features, pooling layers reduce spatial dimensions, and fully connected layers perform final classification or regression.\n",
        "\n",
        "## Convolution Operation\n",
        "\n",
        "The convolutional layer applies a set of learnable filters (kernels) to the input data. Each filter slides over the input, computing the dot product between the filter weights and the input patch, producing a feature map.\n",
        "\n",
        "Mathematically, for a 2D input $I$ and filter (kernel) $K$ of size $m \\times n$, the convolution output at position $(i,j)$ is:\n",
        "\n",
        "$$\n",
        "S(i,j) = (I * K)(i,j) = \\sum_{u=0}^{m-1} \\sum_{v=0}^{n-1} I(i+u, j+v) \\cdot K(u,v)\n",
        "$$\n",
        "\n",
        "This operation detects spatial features like edges or textures.\n",
        "\n",
        "## Activation Function\n",
        "\n",
        "After convolution, an activation function $\\varphi$ is applied element-wise to introduce nonlinearity:\n",
        "\n",
        "$$\n",
        "A(i,j) = \\varphi(S(i,j))\n",
        "$$\n",
        "\n",
        "Common choices include ReLU defined as:\n",
        "\n",
        "$$\n",
        "\\text{ReLU}(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "## Pooling Layer\n",
        "\n",
        "Pooling reduces the spatial size of feature maps, helping to decrease computation and control overfitting. Max pooling selects the maximum value within a window; average pooling computes the average.\n",
        "\n",
        "For max pooling over a window of size $p \\times p$:\n",
        "\n",
        "$$\n",
        "P(i,j) = \\max_{\\substack{0 \\leq u < p \\\\ 0 \\leq v < p}} A(p \\cdot i + u, p \\cdot j + v)\n",
        "$$\n",
        "\n",
        "## Fully Connected Layer\n",
        "\n",
        "At the end, feature maps are flattened into a vector and passed to fully connected layers, which perform high-level reasoning and output predictions.\n",
        "\n",
        "## Summary\n",
        "\n",
        "CNNs exploit spatial correlations in data through convolution and pooling layers, followed by fully connected layers for output. The convolution operation involves sliding kernels over inputs to extract features, followed by nonlinear activation and dimension reduction via pooling.\n",
        "\n",
        "Key formulas:\n",
        "\n",
        "Convolution operation:  \n",
        "$$\n",
        "S(i,j) = \\sum_{u=0}^{m-1} \\sum_{v=0}^{n-1} I(i+u, j+v) \\cdot K(u,v)\n",
        "$$\n",
        "\n",
        "Activation function (ReLU example):  \n",
        "$$\n",
        "\\varphi(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "Max pooling:  \n",
        "$$\n",
        "P(i,j) = \\max_{0 \\leq u,v < p} A(p \\cdot i + u, p \\cdot j + v)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Recurrent Neural Networks\n",
        "\n",
        "## Overview\n",
        "\n",
        "Recurrent Neural Networks (RNNs) are designed to process sequential data by maintaining a hidden state that captures information from previous time steps. This capability makes RNNs suitable for tasks such as time series analysis, language modeling, and speech recognition.\n",
        "\n",
        "## Architecture and Operation\n",
        "\n",
        "At each time step $t$, an RNN takes an input vector $\\mathbf{x}_t$ and the previous hidden state $\\mathbf{h}_{t-1}$ to compute the current hidden state $\\mathbf{h}_t$:\n",
        "\n",
        "$$\n",
        "\\mathbf{h}_t = \\varphi \\left( W_{xh} \\mathbf{x}_t + W_{hh} \\mathbf{h}_{t-1} + \\mathbf{b}_h \\right)\n",
        "$$\n",
        "\n",
        "where \n",
        "\n",
        "- $W_{xh}$ is the input-to-hidden weight matrix,\n",
        "- $W_{hh}$ is the hidden-to-hidden recurrent weight matrix,\n",
        "- $\\mathbf{b}_h$ is the bias vector,\n",
        "- $\\varphi$ is an activation function, commonly $\\tanh$ or ReLU.\n",
        "\n",
        "The hidden state $\\mathbf{h}_t$ serves as a memory that aggregates past inputs.\n",
        "\n",
        "## Output Computation\n",
        "\n",
        "The output vector at time $t$, $\\mathbf{y}_t$, is computed from the hidden state:\n",
        "\n",
        "$$\n",
        "\\mathbf{y}_t = \\psi \\left( W_{hy} \\mathbf{h}_t + \\mathbf{b}_y \\right)\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "- $W_{hy}$ is the hidden-to-output weight matrix,\n",
        "- $\\mathbf{b}_y$ is the output bias,\n",
        "- $\\psi$ is an activation function depending on the task (e.g., softmax for classification).\n",
        "\n",
        "## Sequential Processing\n",
        "\n",
        "The RNN processes the entire input sequence $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_T\\}$ recursively through time steps $t=1$ to $T$, updating hidden states and producing outputs.\n",
        "\n",
        "## Training via Backpropagation Through Time\n",
        "\n",
        "RNN parameters are optimized by minimizing a loss function over all time steps using Backpropagation Through Time (BPTT), which unfolds the network over time and applies gradient descent.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Recurrent Neural Networks extend standard neural networks to sequential data by maintaining a dynamic hidden state updated at each time step. This enables the network to capture temporal dependencies.\n",
        "\n",
        "Key formulas:\n",
        "\n",
        "Hidden state update:  \n",
        "$$\n",
        "\\mathbf{h}_t = \\varphi \\left( W_{xh} \\mathbf{x}_t + W_{hh} \\mathbf{h}_{t-1} + \\mathbf{b}_h \\right)\n",
        "$$\n",
        "\n",
        "Output computation:  \n",
        "$$\n",
        "\\mathbf{y}_t = \\psi \\left( W_{hy} \\mathbf{h}_t + \\mathbf{b}_y \\right)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Long Short-Term Memory (LSTM)\n",
        "\n",
        "## Overview\n",
        "\n",
        "Long Short-Term Memory (LSTM) networks are a special type of Recurrent Neural Network (RNN) designed to overcome the vanishing and exploding gradient problems in learning long-term dependencies. LSTMs use gated cells to control information flow and maintain memory over extended sequences.\n",
        "\n",
        "## LSTM Cell Structure\n",
        "\n",
        "An LSTM cell contains three gates and a cell state that regulate information:\n",
        "\n",
        "- Forget gate ($f_t$): Decides what information to discard from the previous cell state.\n",
        "- Input gate ($i_t$): Decides what new information to add.\n",
        "- Output gate ($o_t$): Decides what part of the cell state to output.\n",
        "\n",
        "Given input $\\mathbf{x}_t$, previous hidden state $\\mathbf{h}_{t-1}$, and previous cell state $\\mathbf{C}_{t-1}$, the gates and states are computed as:\n",
        "\n",
        "Forget gate:\n",
        "\n",
        "$$\n",
        "f_t = \\sigma \\left( W_f \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + b_f \\right)\n",
        "$$\n",
        "\n",
        "Input gate:\n",
        "\n",
        "$$\n",
        "i_t = \\sigma \\left( W_i \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + b_i \\right)\n",
        "$$\n",
        "\n",
        "Candidate cell state:\n",
        "\n",
        "$$\n",
        "\\tilde{C}_t = \\tanh \\left( W_C \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + b_C \\right)\n",
        "$$\n",
        "\n",
        "Cell state update:\n",
        "\n",
        "$$\n",
        "\\mathbf{C}_t = f_t \\odot \\mathbf{C}_{t-1} + i_t \\odot \\tilde{C}_t\n",
        "$$\n",
        "\n",
        "Output gate:\n",
        "\n",
        "$$\n",
        "o_t = \\sigma \\left( W_o \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + b_o \\right)\n",
        "$$\n",
        "\n",
        "Hidden state update:\n",
        "\n",
        "$$\n",
        "\\mathbf{h}_t = o_t \\odot \\tanh(\\mathbf{C}_t)\n",
        "$$\n",
        "\n",
        "Here, $\\sigma$ is the sigmoid activation function, $\\tanh$ is the hyperbolic tangent function, and $\\odot$ denotes element-wise multiplication.\n",
        "\n",
        "## Functional Summary\n",
        "\n",
        "- The forget gate $f_t$ controls which information from the previous cell state $\\mathbf{C}_{t-1}$ should be retained.\n",
        "- The input gate $i_t$ and candidate cell state $\\tilde{C}_t$ decide what new information is added to the cell state.\n",
        "- The output gate $o_t$ regulates the hidden state $\\mathbf{h}_t$ that is passed to the next step and output layers.\n",
        "\n",
        "## Advantages of LSTM\n",
        "\n",
        "LSTMs effectively preserve and manipulate long-term dependencies in sequential data by using gates to control information flow, thus preventing the gradient problems encountered in vanilla RNNs.\n",
        "\n",
        "## Summary\n",
        "\n",
        "LSTM networks enhance RNNs with gated memory cells, enabling learning over long sequences. The gates (forget, input, output) regulate the cell state and hidden state updates, allowing the network to remember or forget information adaptively.\n",
        "\n",
        "Key formulas:\n",
        "\n",
        "Forget gate:  \n",
        "$$\n",
        "f_t = \\sigma \\left( W_f \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + b_f \\right)\n",
        "$$\n",
        "\n",
        "Input gate:  \n",
        "$$\n",
        "i_t = \\sigma \\left( W_i \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + b_i \\right)\n",
        "$$\n",
        "\n",
        "Candidate cell state:  \n",
        "$$\n",
        "\\tilde{C}_t = \\tanh \\left( W_C \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + b_C \\right)\n",
        "$$\n",
        "\n",
        "Cell state update:  \n",
        "$$\n",
        "\\mathbf{C}_t = f_t \\odot \\mathbf{C}_{t-1} + i_t \\odot \\tilde{C}_t\n",
        "$$\n",
        "\n",
        "Output gate:  \n",
        "$$\n",
        "o_t = \\sigma \\left( W_o \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + b_o \\right)\n",
        "$$\n",
        "\n",
        "Hidden state update:  \n",
        "$$\n",
        "\\mathbf{h}_t = o_t \\odot \\tanh(\\mathbf{C}_t)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Difference Between Artificial Neural Networks (ANN) and Biological Neural Networks (BNN)\n",
        "\n",
        "## Overview\n",
        "\n",
        "Artificial Neural Networks (ANNs) are computational models inspired by the structure and functioning of Biological Neural Networks (BNNs), which are networks of neurons found in living organisms. While both share conceptual similarities, their implementations, complexity, and operating principles differ significantly.\n",
        "\n",
        "## Biological Neural Networks (BNN)\n",
        "\n",
        "BNNs consist of neurons interconnected by synapses in biological organisms. Each neuron receives signals via dendrites, processes them, and sends output signals through the axon.\n",
        "\n",
        "The signal processing in a biological neuron can be abstractly represented as:\n",
        "\n",
        "$$\n",
        "V_{\\text{membrane}}(t) = \\sum_{i} w_i \\cdot x_i(t) - \\theta\n",
        "$$\n",
        "\n",
        "where \n",
        "\n",
        "- $x_i(t)$ represents the incoming signals from other neurons at time $t$,\n",
        "- $w_i$ are synaptic weights representing the strength of connections,\n",
        "- $\\theta$ is a threshold potential,\n",
        "- The neuron fires if $V_{\\text{membrane}}(t)$ exceeds $\\theta$.\n",
        "\n",
        "BNNs exhibit complex dynamics including non-linear behavior, plasticity, and biochemical modulation.\n",
        "\n",
        "## Artificial Neural Networks (ANN)\n",
        "\n",
        "ANNs are simplified mathematical models consisting of layers of artificial neurons (nodes). Each neuron computes a weighted sum of inputs and passes the result through an activation function.\n",
        "\n",
        "The operation of an artificial neuron is:\n",
        "\n",
        "$$\n",
        "y = \\varphi \\left( \\sum_{i} w_i x_i + b \\right)\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "- $x_i$ are inputs,\n",
        "- $w_i$ are weights,\n",
        "- $b$ is a bias term,\n",
        "- $\\varphi$ is an activation function such as sigmoid, ReLU, or tanh,\n",
        "- $y$ is the output of the neuron.\n",
        "\n",
        "ANNs are trained by adjusting weights $w_i$ and biases $b$ to minimize a loss function using optimization methods like gradient descent.\n",
        "\n",
        "## Key Differences\n",
        "\n",
        "- **Structure and Complexity**: BNNs have highly complex, nonlinear, and dynamic interconnections involving biochemical processes. ANNs use simplified, static mathematical models.\n",
        "\n",
        "- **Learning Mechanism**: BNNs learn through synaptic plasticity and complex biological processes. ANNs learn by numerical optimization of weights via algorithms like backpropagation.\n",
        "\n",
        "- **Signal Nature**: BNNs use spiking signals and neurotransmitters. ANNs use continuous numerical values.\n",
        "\n",
        "- **Scale**: BNNs involve billions of neurons and trillions of connections. ANNs typically have orders of magnitude fewer neurons.\n",
        "\n",
        "## Summary\n",
        "\n",
        "While ANNs are inspired by BNNs, they are abstracted and simplified computational models designed for practical machine learning tasks. The main formulas highlight the difference between the biological membrane potential and the artificial weighted sum with activation.\n",
        "\n",
        "Biological neuron potential:  \n",
        "$$\n",
        "V_{\\text{membrane}}(t) = \\sum_{i} w_i \\cdot x_i(t) - \\theta\n",
        "$$\n",
        "\n",
        "Artificial neuron output:  \n",
        "$$\n",
        "y = \\varphi \\left( \\sum_{i} w_i x_i + b \\right)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Artificial Neural Networks and Their Applications\n",
        "\n",
        "## Overview\n",
        "\n",
        "Artificial Neural Networks (ANNs) are computational models inspired by the human brain’s neural structure, capable of modeling complex patterns and solving various problems in machine learning and artificial intelligence. ANNs consist of interconnected layers of artificial neurons that process inputs to produce outputs through learned weights and activation functions.\n",
        "\n",
        "## Structure of ANN\n",
        "\n",
        "An ANN typically consists of an input layer, one or more hidden layers, and an output layer. Each neuron in a layer receives weighted inputs, adds a bias, and passes the result through an activation function.\n",
        "\n",
        "The output of a neuron is given by:\n",
        "\n",
        "$$\n",
        "y = \\varphi \\left( \\sum_{i=1}^n w_i x_i + b \\right)\n",
        "$$\n",
        "\n",
        "where $x_i$ are inputs, $w_i$ are weights, $b$ is bias, and $\\varphi$ is the activation function.\n",
        "\n",
        "## Learning Process\n",
        "\n",
        "ANNs learn by adjusting weights and biases to minimize a loss function, often using the backpropagation algorithm combined with optimization techniques like gradient descent.\n",
        "\n",
        "The weight update rule in gradient descent is:\n",
        "\n",
        "$$\n",
        "w_i \\leftarrow w_i - \\eta \\frac{\\partial L}{\\partial w_i}\n",
        "$$\n",
        "\n",
        "where $\\eta$ is the learning rate and $L$ is the loss function.\n",
        "\n",
        "## Common Activation Functions\n",
        "\n",
        "- Sigmoid:\n",
        "\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
        "$$\n",
        "\n",
        "- Hyperbolic Tangent (tanh):\n",
        "\n",
        "$$\n",
        "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "$$\n",
        "\n",
        "- Rectified Linear Unit (ReLU):\n",
        "\n",
        "$$\n",
        "\\text{ReLU}(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "## Applications of Artificial Neural Networks\n",
        "\n",
        "ANNs have been successfully applied in diverse fields including:\n",
        "\n",
        "Image and speech recognition, where they identify patterns in complex data.\n",
        "\n",
        "Natural language processing for translation, sentiment analysis, and chatbots.\n",
        "\n",
        "Medical diagnosis by analyzing symptoms and medical images.\n",
        "\n",
        "Financial forecasting to predict market trends.\n",
        "\n",
        "Autonomous systems such as self-driving cars for decision-making.\n",
        "\n",
        "Robotics and control systems for adaptive behavior.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Artificial Neural Networks model complex input-output relationships by processing data through layers of interconnected neurons. They learn by iteratively updating weights to minimize errors. Their flexibility and power make them suitable for a broad range of applications from vision to language and control.\n",
        "\n",
        "Key formulas:  \n",
        "\n",
        "Neuron output:  \n",
        "$$\n",
        "y = \\varphi \\left( \\sum_{i=1}^n w_i x_i + b \\right)\n",
        "$$\n",
        "\n",
        "Weight update rule:  \n",
        "$$\n",
        "w_i \\leftarrow w_i - \\eta \\frac{\\partial L}{\\partial w_i}\n",
        "$$\n",
        "\n",
        "Common activation functions:  \n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1 + e^{-x}} \\quad\n",
        "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\quad\n",
        "\\text{ReLU}(x) = \\max(0, x)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Layers in Artificial Neural Networks (ANN)\n",
        "\n",
        "## Overview\n",
        "\n",
        "Artificial Neural Networks (ANNs) are organized as layers of neurons, where each layer transforms its input and passes the result to the next layer. Understanding these layers is essential to grasp how ANNs process data and learn representations.\n",
        "\n",
        "## Types of Layers in ANN\n",
        "\n",
        "ANNs typically consist of three types of layers:\n",
        "\n",
        "1. Input Layer  \n",
        "2. Hidden Layer(s)  \n",
        "3. Output Layer  \n",
        "\n",
        "### Input Layer\n",
        "\n",
        "The input layer receives raw data features. Each neuron in this layer represents one feature of the input vector. This layer does not perform computations but serves as the entry point of data into the network.\n",
        "\n",
        "If the input vector is:\n",
        "\n",
        "$$\n",
        "\\mathbf{x} = (x_1, x_2, \\dots, x_n)\n",
        "$$\n",
        "\n",
        "then the input layer has $n$ neurons corresponding to each $x_i$.\n",
        "\n",
        "### Hidden Layer(s)\n",
        "\n",
        "Hidden layers perform transformations and extract features by applying weighted sums and activation functions. The complexity and depth of the ANN increase with the number of hidden layers.\n",
        "\n",
        "For a neuron $j$ in a hidden layer, the output is:\n",
        "\n",
        "$$\n",
        "h_j = \\varphi \\left( \\sum_{i=1}^n w_{ij} x_i + b_j \\right)\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "- $w_{ij}$ is the weight connecting input neuron $i$ to hidden neuron $j$,\n",
        "- $b_j$ is the bias of neuron $j$,\n",
        "- $\\varphi$ is the activation function, such as sigmoid, ReLU, or tanh,\n",
        "- $x_i$ are inputs from the previous layer.\n",
        "\n",
        "Multiple hidden layers enable the network to learn hierarchical features, increasing its ability to model complex functions.\n",
        "\n",
        "### Output Layer\n",
        "\n",
        "The output layer produces the final prediction or classification result. The number of neurons in this layer depends on the task:\n",
        "\n",
        "- For regression, usually one neuron with a linear activation function.\n",
        "- For classification, multiple neurons with softmax or sigmoid activations.\n",
        "\n",
        "The output of neuron $k$ in the output layer is:\n",
        "\n",
        "$$\n",
        "y_k = \\varphi \\left( \\sum_{j=1}^m w_{jk} h_j + b_k \\right)\n",
        "$$\n",
        "\n",
        "where $h_j$ are outputs from the last hidden layer, $w_{jk}$ and $b_k$ are weights and bias for output neuron $k$.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Layers in ANNs form a chain of transformations from raw input to output predictions. The input layer takes raw data, hidden layers perform nonlinear transformations, and the output layer provides the result. The formula for each neuron’s output generalizes as:\n",
        "\n",
        "$$\n",
        "y = \\varphi \\left( \\sum_i w_i x_i + b \\right)\n",
        "$$\n",
        "\n",
        "where inputs $x_i$ may be raw features or outputs from the previous layer.\n",
        "\n",
        "Understanding these layers and their functions is foundational to designing and training effective neural networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Role of Weights and Bias in Neural Networks\n",
        "\n",
        "## Overview\n",
        "\n",
        "Weights and biases are fundamental parameters in neural networks that determine how input signals are transformed as they propagate through the network. Their values are adjusted during training to enable the network to learn patterns and make accurate predictions.\n",
        "\n",
        "## Weights\n",
        "\n",
        "Weights represent the strength of the connection between neurons. Each input to a neuron is multiplied by a corresponding weight, controlling the influence of that input on the neuron's output.\n",
        "\n",
        "Given inputs $\\mathbf{x} = (x_1, x_2, \\dots, x_n)$ and weights $\\mathbf{w} = (w_1, w_2, \\dots, w_n)$, the weighted sum for a neuron is:\n",
        "\n",
        "$$\n",
        "z = \\sum_{i=1}^n w_i x_i\n",
        "$$\n",
        "\n",
        "Weights can be positive or negative, enabling excitation or inhibition of the neuron’s output. During training, weights are iteratively updated to minimize the error between predicted and actual outputs.\n",
        "\n",
        "## Bias\n",
        "\n",
        "Bias acts as an adjustable constant added to the weighted sum before the activation function, allowing the activation threshold to shift. This flexibility enables the network to model patterns that do not pass through the origin.\n",
        "\n",
        "The bias term is denoted as $b$, and the neuron's net input becomes:\n",
        "\n",
        "$$\n",
        "z = \\sum_{i=1}^n w_i x_i + b\n",
        "$$\n",
        "\n",
        "Including bias improves the model’s ability to fit data by enabling neurons to activate even when all inputs are zero.\n",
        "\n",
        "## Neuron Output\n",
        "\n",
        "After computing the weighted sum plus bias, the neuron applies a nonlinear activation function $\\varphi$ to produce the output:\n",
        "\n",
        "$$\n",
        "y = \\varphi \\left( \\sum_{i=1}^n w_i x_i + b \\right)\n",
        "$$\n",
        "\n",
        "Common activation functions include sigmoid, tanh, and ReLU.\n",
        "\n",
        "## Importance of Weights and Bias\n",
        "\n",
        "- Weights determine how strongly each input feature affects the neuron's response.\n",
        "- Bias provides the neuron with a degree of freedom to adjust activation thresholds.\n",
        "- Together, they enable the network to approximate complex nonlinear functions.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Weights and bias are trainable parameters that transform inputs within neural networks. The neuron's activation depends on the weighted sum of inputs plus bias, passed through an activation function:\n",
        "\n",
        "$$\n",
        "y = \\varphi \\left( \\sum_{i=1}^n w_i x_i + b \\right)\n",
        "$$\n",
        "\n",
        "Adjusting weights and bias during training allows the network to learn from data and generalize to new inputs effectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Forward Propagation in Neural Networks\n",
        "\n",
        "## Overview\n",
        "\n",
        "Forward propagation is the fundamental process by which input data is passed through the layers of a neural network to generate an output prediction. It involves computing the weighted sums, adding biases, and applying activation functions layer by layer from the input to the output.\n",
        "\n",
        "## Process of Forward Propagation\n",
        "\n",
        "Given an input vector $\\mathbf{x} = (x_1, x_2, \\dots, x_n)$, the network computes the output of each neuron sequentially.\n",
        "\n",
        "For neuron $j$ in layer $l$, the input to the neuron, called the net input $z_j^{(l)}$, is:\n",
        "\n",
        "$$\n",
        "z_j^{(l)} = \\sum_{i=1}^{n_{l-1}} w_{ij}^{(l)} a_i^{(l-1)} + b_j^{(l)}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "- $w_{ij}^{(l)}$ is the weight from neuron $i$ in layer $(l-1)$ to neuron $j$ in layer $l$,\n",
        "- $a_i^{(l-1)}$ is the activation output of neuron $i$ in the previous layer,\n",
        "- $b_j^{(l)}$ is the bias of neuron $j$ in layer $l$,\n",
        "- $n_{l-1}$ is the number of neurons in layer $(l-1)$.\n",
        "\n",
        "The activation output $a_j^{(l)}$ of neuron $j$ in layer $l$ is obtained by applying an activation function $\\varphi$ to the net input:\n",
        "\n",
        "$$\n",
        "a_j^{(l)} = \\varphi \\left( z_j^{(l)} \\right) = \\varphi \\left( \\sum_{i=1}^{n_{l-1}} w_{ij}^{(l)} a_i^{(l-1)} + b_j^{(l)} \\right)\n",
        "$$\n",
        "\n",
        "For the input layer $(l=0)$, activations $a_i^{(0)}$ are simply the input features $x_i$.\n",
        "\n",
        "## Activation Functions\n",
        "\n",
        "Common activation functions $\\varphi$ include:\n",
        "\n",
        "- Sigmoid: $ \\varphi(z) = \\frac{1}{1 + e^{-z}} $\n",
        "- ReLU (Rectified Linear Unit): $ \\varphi(z) = \\max(0, z) $\n",
        "- Tanh: $ \\varphi(z) = \\tanh(z) $\n",
        "\n",
        "These functions introduce non-linearity, allowing the network to learn complex patterns.\n",
        "\n",
        "## Output Calculation\n",
        "\n",
        "The forward propagation continues through all layers until the output layer produces the final prediction vector:\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = (a_1^{(L)}, a_2^{(L)}, \\dots, a_m^{(L)})\n",
        "$$\n",
        "\n",
        "where $L$ is the total number of layers and $m$ is the number of output neurons.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Forward propagation is the stepwise computation of neuron outputs from input to output layers, governed by:\n",
        "\n",
        "$$\n",
        "a_j^{(l)} = \\varphi \\left( \\sum_{i=1}^{n_{l-1}} w_{ij}^{(l)} a_i^{(l-1)} + b_j^{(l)} \\right)\n",
        "$$\n",
        "\n",
        "This process transforms input data into a predicted output by applying weights, biases, and activation functions throughout the network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Activation Functions in Neural Networks\n",
        "\n",
        "## Overview\n",
        "\n",
        "Activation functions introduce non-linearity into neural networks, enabling them to learn and model complex patterns beyond linear relationships. They determine the output of each neuron by transforming the weighted sum of inputs and bias.\n",
        "\n",
        "## Role of Activation Functions\n",
        "\n",
        "Without activation functions, a neural network behaves like a linear regression model regardless of its depth. Activation functions allow the network to approximate any continuous function by stacking multiple layers.\n",
        "\n",
        "The general form for a neuron output is:\n",
        "\n",
        "$$\n",
        "y = \\varphi \\left( \\sum_{i=1}^n w_i x_i + b \\right)\n",
        "$$\n",
        "\n",
        "where $\\varphi$ is the activation function.\n",
        "\n",
        "## Common Activation Functions\n",
        "\n",
        "### Sigmoid Function\n",
        "\n",
        "The sigmoid maps input values to the range $(0, 1)$, making it suitable for binary classification outputs.\n",
        "\n",
        "$$\n",
        "\\varphi(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Derivative:\n",
        "\n",
        "$$\n",
        "\\varphi'(z) = \\varphi(z) \\times (1 - \\varphi(z))\n",
        "$$\n",
        "\n",
        "### Hyperbolic Tangent (Tanh)\n",
        "\n",
        "The tanh function maps inputs to $(-1, 1)$, centering activations around zero, which can improve convergence.\n",
        "\n",
        "$$\n",
        "\\varphi(z) = \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
        "$$\n",
        "\n",
        "Derivative:\n",
        "\n",
        "$$\n",
        "\\varphi'(z) = 1 - \\tanh^2(z)\n",
        "$$\n",
        "\n",
        "### Rectified Linear Unit (ReLU)\n",
        "\n",
        "ReLU outputs zero for negative inputs and linear for positive inputs, helping alleviate the vanishing gradient problem.\n",
        "\n",
        "$$\n",
        "\\varphi(z) = \\max(0, z)\n",
        "$$\n",
        "\n",
        "Derivative:\n",
        "\n",
        "$$\n",
        "\\varphi'(z) =\n",
        "\\begin{cases}\n",
        "1 & z > 0 \\\\\n",
        "0 & z \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "### Leaky ReLU\n",
        "\n",
        "Leaky ReLU allows a small gradient for negative inputs to avoid dying neurons.\n",
        "\n",
        "$$\n",
        "\\varphi(z) =\n",
        "\\begin{cases}\n",
        "z & z > 0 \\\\\n",
        "\\alpha z & z \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is a small constant (e.g., 0.01).\n",
        "\n",
        "### Softmax Function\n",
        "\n",
        "Used in multi-class classification, softmax converts a vector of raw scores into probabilities summing to one.\n",
        "\n",
        "For a vector $\\mathbf{z} = (z_1, z_2, \\dots, z_k)$:\n",
        "\n",
        "$$\n",
        "\\varphi(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^k e^{z_j}}\n",
        "$$\n",
        "\n",
        "## Summary\n",
        "\n",
        "Activation functions transform the linear combination of inputs into non-linear outputs:\n",
        "\n",
        "$$\n",
        "y = \\varphi \\left( \\sum_{i=1}^n w_i x_i + b \\right)\n",
        "$$\n",
        "\n",
        "Choosing the right activation function affects learning efficiency and network performance, with ReLU and its variants widely used in hidden layers and sigmoid or softmax functions often used in output layers depending on the task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loss Functions in Deep Learning\n",
        "\n",
        "## Overview\n",
        "\n",
        "Loss functions quantify the difference between the predicted output of a neural network and the actual target values. They guide the training process by providing a measure to minimize through optimization algorithms like gradient descent.\n",
        "\n",
        "## Role of Loss Functions\n",
        "\n",
        "The loss function computes a scalar value representing the error of the model's predictions. Minimizing this loss improves the model's accuracy.\n",
        "\n",
        "For predicted outputs $\\hat{y}$ and true targets $y$, the loss function $L(y, \\hat{y})$ evaluates the penalty for incorrect predictions.\n",
        "\n",
        "## Common Loss Functions\n",
        "\n",
        "### Mean Squared Error (MSE)\n",
        "\n",
        "Used for regression tasks, MSE measures the average squared difference between predicted and true values:\n",
        "\n",
        "$$\n",
        "L = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "where $n$ is the number of samples.\n",
        "\n",
        "### Mean Absolute Error (MAE)\n",
        "\n",
        "MAE calculates the average absolute difference between predictions and targets:\n",
        "\n",
        "$$\n",
        "L = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
        "$$\n",
        "\n",
        "MAE is less sensitive to outliers compared to MSE.\n",
        "\n",
        "### Binary Cross-Entropy Loss\n",
        "\n",
        "Used in binary classification, this loss measures the difference between true labels and predicted probabilities:\n",
        "\n",
        "$$\n",
        "L = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
        "$$\n",
        "\n",
        "Here, $y_i \\in \\{0,1\\}$ and $\\hat{y}_i$ is the predicted probability of class 1.\n",
        "\n",
        "### Categorical Cross-Entropy Loss\n",
        "\n",
        "Applied in multi-class classification, this loss compares the true one-hot encoded labels with predicted class probabilities:\n",
        "\n",
        "$$\n",
        "L = - \\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^k y_{ij} \\log(\\hat{y}_{ij})\n",
        "$$\n",
        "\n",
        "where $k$ is the number of classes, $y_{ij}$ is 1 if sample $i$ belongs to class $j$, else 0, and $\\hat{y}_{ij}$ is the predicted probability for class $j$.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Loss functions translate the prediction error into a scalar cost that the network seeks to minimize. Their choice depends on the task type:\n",
        "\n",
        "- Regression: Mean Squared Error, Mean Absolute Error\n",
        "- Binary Classification: Binary Cross-Entropy\n",
        "- Multi-class Classification: Categorical Cross-Entropy\n",
        "\n",
        "The loss function guides weight updates through backpropagation by defining the objective for optimization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Backpropagation in Neural Networks\n",
        "\n",
        "## Overview\n",
        "\n",
        "Backpropagation is the core algorithm used to train neural networks by efficiently computing gradients of the loss function with respect to each weight. It enables the network to update weights in the direction that minimizes the error.\n",
        "\n",
        "## Concept of Backpropagation\n",
        "\n",
        "Training a neural network involves minimizing a loss function $L$ by adjusting weights $w$ and biases $b$. Backpropagation applies the chain rule of calculus to compute the gradient $\\frac{\\partial L}{\\partial w}$ for all weights.\n",
        "\n",
        "Given a network with layers indexed by $l$, the key steps are:\n",
        "\n",
        "1. **Forward pass:** Compute activations $a^l$ using weights and inputs.\n",
        "2. **Backward pass:** Compute error terms $\\delta^l$ for each layer, starting from the output layer and moving backward.\n",
        "3. **Gradient calculation:** Use errors to compute gradients for weights and biases.\n",
        "4. **Weights update:** Adjust weights using gradient descent.\n",
        "\n",
        "## Mathematical Formulation\n",
        "\n",
        "Let the weighted input to neuron $j$ in layer $l$ be:\n",
        "\n",
        "$$\n",
        "z_j^l = \\sum_k w_{jk}^l a_k^{l-1} + b_j^l\n",
        "$$\n",
        "\n",
        "where $a_k^{l-1}$ is the activation from the previous layer.\n",
        "\n",
        "Activation output:\n",
        "\n",
        "$$\n",
        "a_j^l = \\varphi(z_j^l)\n",
        "$$\n",
        "\n",
        "where $\\varphi$ is the activation function.\n",
        "\n",
        "### Error Term $\\delta$\n",
        "\n",
        "The error term for neuron $j$ in layer $l$ is defined as:\n",
        "\n",
        "$$\n",
        "\\delta_j^l = \\frac{\\partial L}{\\partial z_j^l}\n",
        "$$\n",
        "\n",
        "For the output layer $L$, assuming loss depends directly on outputs:\n",
        "\n",
        "$$\n",
        "\\delta_j^L = \\frac{\\partial L}{\\partial a_j^L} \\cdot \\varphi'(z_j^L)\n",
        "$$\n",
        "\n",
        "For hidden layers, error terms are propagated backward:\n",
        "\n",
        "$$\n",
        "\\delta_j^l = \\left( \\sum_m w_{mj}^{l+1} \\delta_m^{l+1} \\right) \\cdot \\varphi'(z_j^l)\n",
        "$$\n",
        "\n",
        "### Gradients of Loss with Respect to Weights and Biases\n",
        "\n",
        "Gradients are computed as:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_j^l} = \\delta_j^l\n",
        "$$\n",
        "\n",
        "### Weight Update Rule\n",
        "\n",
        "Using learning rate $\\eta$, weights and biases are updated as:\n",
        "\n",
        "$$\n",
        "w_{jk}^l \\leftarrow w_{jk}^l - \\eta \\frac{\\partial L}{\\partial w_{jk}^l}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b_j^l \\leftarrow b_j^l - \\eta \\frac{\\partial L}{\\partial b_j^l}\n",
        "$$\n",
        "\n",
        "## Summary\n",
        "\n",
        "Backpropagation efficiently computes gradients by recursively applying the chain rule from the output layer to the input layer. This enables gradient-based optimization algorithms like gradient descent to minimize the loss function and train the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Impact of Learning Rate on a Model\n",
        "\n",
        "## Overview\n",
        "\n",
        "The learning rate $\\eta$ is a crucial hyperparameter in training neural networks that controls the size of weight updates during optimization. It directly affects convergence speed, stability, and model performance.\n",
        "\n",
        "## Role of Learning Rate\n",
        "\n",
        "During training, weights are updated as:\n",
        "\n",
        "$$\n",
        "w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}\n",
        "$$\n",
        "\n",
        "where $w$ represents weights and $\\frac{\\partial L}{\\partial w}$ is the gradient of the loss function $L$ with respect to the weight.\n",
        "\n",
        "The learning rate $\\eta$ scales the gradient step and influences how fast or slow the model learns.\n",
        "\n",
        "## Effects of Different Learning Rates\n",
        "\n",
        "- **Small Learning Rate:**\n",
        "\n",
        "  When $\\eta$ is very small, weight updates are tiny. This results in slow convergence and longer training times. However, it may lead to more precise convergence to a local or global minimum.\n",
        "\n",
        "- **Large Learning Rate:**\n",
        "\n",
        "  A large $\\eta$ causes big weight updates. This can speed up training but risks overshooting minima, causing oscillations or divergence. The model may fail to converge or produce suboptimal results.\n",
        "\n",
        "- **Optimal Learning Rate:**\n",
        "\n",
        "  The best learning rate balances convergence speed and stability. It allows sufficient progress without causing instability.\n",
        "\n",
        "## Learning Rate Decay\n",
        "\n",
        "To improve training, learning rate schedules reduce $\\eta$ gradually over time. This strategy enables:\n",
        "\n",
        "1. Faster convergence in early epochs with larger learning rates.\n",
        "2. Finer adjustments near minima with smaller learning rates.\n",
        "\n",
        "Common decay formulas include exponential decay:\n",
        "\n",
        "$$\n",
        "\\eta_t = \\eta_0 \\cdot \\gamma^t\n",
        "$$\n",
        "\n",
        "where $\\eta_0$ is the initial rate, $\\gamma$ is the decay factor ($0 < \\gamma < 1$), and $t$ is the epoch or iteration number.\n",
        "\n",
        "## Summary\n",
        "\n",
        "The learning rate is a sensitive parameter that critically affects model training. Choosing and tuning $\\eta$ appropriately is essential to ensure efficient and stable convergence of neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimization Algorithms in Machine Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Optimization algorithms are essential in training machine learning models, particularly neural networks. They aim to minimize the loss function $L(\\theta)$, where $\\theta$ represents model parameters (weights and biases), by iteratively updating parameters to find the optimal values.\n",
        "\n",
        "The general parameter update rule is:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta \\cdot g\n",
        "$$\n",
        "\n",
        "where $\\eta$ is the learning rate and $g = \\nabla_\\theta L(\\theta)$ is the gradient of the loss with respect to parameters.\n",
        "\n",
        "## Gradient Descent (GD)\n",
        "\n",
        "Gradient Descent updates parameters using the full dataset gradient:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta \\cdot \\frac{1}{N} \\sum_{i=1}^N \\nabla_\\theta L_i(\\theta)\n",
        "$$\n",
        "\n",
        "where $N$ is the number of training examples and $L_i$ is the loss for example $i$.\n",
        "\n",
        "GD converges smoothly but can be slow for large datasets.\n",
        "\n",
        "## Stochastic Gradient Descent (SGD)\n",
        "\n",
        "SGD updates parameters using gradients from a single training example:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta \\cdot \\nabla_\\theta L_i(\\theta)\n",
        "$$\n",
        "\n",
        "This leads to faster, noisier updates, improving convergence speed and enabling online learning.\n",
        "\n",
        "## Mini-batch Gradient Descent\n",
        "\n",
        "Combines GD and SGD by using small batches of size $m$:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta \\cdot \\frac{1}{m} \\sum_{j=1}^m \\nabla_\\theta L_j(\\theta)\n",
        "$$\n",
        "\n",
        "Balances computational efficiency and stable convergence.\n",
        "\n",
        "## Momentum\n",
        "\n",
        "Momentum accelerates convergence by accumulating a velocity vector $v$ that smooths updates:\n",
        "\n",
        "$$\n",
        "v_t = \\gamma v_{t-1} + \\eta \\nabla_\\theta L(\\theta)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - v_t\n",
        "$$\n",
        "\n",
        "where $\\gamma \\in [0,1)$ is the momentum coefficient controlling how much past gradients influence current updates.\n",
        "\n",
        "## Adaptive Methods\n",
        "\n",
        "Adaptive optimizers adjust learning rates per parameter based on past gradients.\n",
        "\n",
        "### AdaGrad\n",
        "\n",
        "Scales learning rates inversely proportional to the root of the sum of squared past gradients:\n",
        "\n",
        "$$\n",
        "G_t = G_{t-1} + g_t^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\cdot g_t\n",
        "$$\n",
        "\n",
        "where $g_t$ is the gradient at time $t$ and $\\epsilon$ is a small number to prevent division by zero.\n",
        "\n",
        "### RMSProp\n",
        "\n",
        "Improves AdaGrad by using an exponentially weighted moving average of squared gradients:\n",
        "\n",
        "$$\n",
        "E[g^2]_t = \\beta E[g^2]_{t-1} + (1-\\beta) g_t^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\cdot g_t\n",
        "$$\n",
        "\n",
        "with decay rate $\\beta$ typically around 0.9.\n",
        "\n",
        "### Adam (Adaptive Moment Estimation)\n",
        "\n",
        "Combines momentum and RMSProp by maintaining first and second moment estimates:\n",
        "\n",
        "$$\n",
        "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
        "$$\n",
        "\n",
        "$$\n",
        "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
        "$$\n",
        "\n",
        "Bias-corrected estimates:\n",
        "\n",
        "$$\n",
        "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
        "$$\n",
        "\n",
        "Update rule:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
        "$$\n",
        "\n",
        "where $\\beta_1$ and $\\beta_2$ are decay rates for the moments.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Optimization algorithms control how neural networks learn by updating parameters to reduce loss. Selecting the appropriate optimizer and tuning hyperparameters such as learning rate and momentum are critical for effective and efficient training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gradient Descent Algorithm and Its Variants\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Gradient Descent (GD) is an iterative optimization algorithm used to minimize a function, commonly the loss function $L(\\theta)$ in machine learning, where $\\theta$ denotes the parameters. The goal is to find the parameter values that minimize $L(\\theta)$ by moving in the direction of the negative gradient.\n",
        "\n",
        "The core update rule of GD is:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta L(\\theta)\n",
        "$$\n",
        "\n",
        "where $\\eta$ is the learning rate controlling the step size, and $\\nabla_\\theta L(\\theta)$ is the gradient of the loss with respect to the parameters.\n",
        "\n",
        "## Variants of Gradient Descent\n",
        "\n",
        "### Batch Gradient Descent\n",
        "\n",
        "Calculates the gradient using the entire training dataset:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta \\frac{1}{N} \\sum_{i=1}^N \\nabla_\\theta L_i(\\theta)\n",
        "$$\n",
        "\n",
        "where $N$ is the number of samples, and $L_i(\\theta)$ is the loss on sample $i$.\n",
        "\n",
        "This approach provides stable updates but can be computationally expensive and slow for large datasets.\n",
        "\n",
        "### Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Updates parameters based on the gradient from a single randomly chosen sample:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta L_i(\\theta)\n",
        "$$\n",
        "\n",
        "This introduces noise in the updates, which can help escape local minima and speed convergence, but can cause instability in the loss trajectory.\n",
        "\n",
        "### Mini-batch Gradient Descent\n",
        "\n",
        "Balances batch and stochastic methods by using a small batch of size $m$:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta \\frac{1}{m} \\sum_{j=1}^m \\nabla_\\theta L_j(\\theta)\n",
        "$$\n",
        "\n",
        "This improves computational efficiency and stability compared to SGD.\n",
        "\n",
        "## Important Concepts in Gradient Descent\n",
        "\n",
        "### Learning Rate ($\\eta$)\n",
        "\n",
        "The learning rate controls how much the parameters change in each iteration. If $\\eta$ is too large, the algorithm might overshoot the minimum and diverge. If too small, convergence will be very slow.\n",
        "\n",
        "### Convergence\n",
        "\n",
        "Gradient Descent iteratively updates parameters until the change in loss or parameters is below a threshold, indicating convergence near a minimum.\n",
        "\n",
        "## Challenges and Improvements\n",
        "\n",
        "### Local Minima and Saddle Points\n",
        "\n",
        "Gradient Descent can get stuck in local minima or saddle points. Variants like SGD introduce noise to help avoid these traps.\n",
        "\n",
        "### Momentum\n",
        "\n",
        "Momentum adds a velocity term $v_t$ to smooth updates:\n",
        "\n",
        "$$\n",
        "v_t = \\gamma v_{t-1} + \\eta \\nabla_\\theta L(\\theta)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - v_t\n",
        "$$\n",
        "\n",
        "where $\\gamma$ is the momentum factor.\n",
        "\n",
        "### Adaptive Learning Rates\n",
        "\n",
        "Methods like AdaGrad, RMSProp, and Adam adjust learning rates dynamically per parameter for improved convergence efficiency.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Gradient Descent and its variants form the backbone of training in machine learning by iteratively minimizing the loss function through parameter updates guided by gradients. Understanding the trade-offs between batch size, learning rate, and optimization variants is crucial for effective model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stochastic Gradient Descent (SGD) in Machine Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Stochastic Gradient Descent (SGD) is an optimization algorithm widely used to train machine learning models, especially neural networks. It is a variant of Gradient Descent that updates the model parameters more frequently, using only one training sample at a time instead of the entire dataset.\n",
        "\n",
        "## Working Principle of SGD\n",
        "\n",
        "The objective is to minimize a loss function $L(\\theta)$ with respect to model parameters $\\theta$. Unlike batch gradient descent which calculates the gradient over all samples, SGD uses a single data point $(x_i, y_i)$ to compute an approximate gradient:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta L_i(\\theta)\n",
        "$$\n",
        "\n",
        "where $\\eta$ is the learning rate and $\\nabla_\\theta L_i(\\theta)$ is the gradient of the loss computed using the $i$-th training example.\n",
        "\n",
        "## Advantages of SGD\n",
        "\n",
        "SGD performs frequent updates, which can lead to faster convergence especially on large datasets. The noise introduced by single-sample updates can help the algorithm escape shallow local minima or saddle points, enhancing the chances of finding a better minimum.\n",
        "\n",
        "## Mathematical Formulation\n",
        "\n",
        "Given a dataset of $N$ samples, SGD updates parameters at iteration $t$ as:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L_{i_t}(\\theta_t)\n",
        "$$\n",
        "\n",
        "where $i_t$ is the index of the randomly selected training sample at iteration $t$.\n",
        "\n",
        "The loss function often takes the form:\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\frac{1}{N} \\sum_{i=1}^N L_i(\\theta)\n",
        "$$\n",
        "\n",
        "but SGD approximates this by considering only one $L_{i_t}(\\theta)$ per iteration.\n",
        "\n",
        "## Convergence Properties\n",
        "\n",
        "Although SGD introduces variance in the gradient estimate, with an appropriate learning rate schedule (typically decreasing $\\eta$ over time), it converges to the optimal solution under certain convexity assumptions.\n",
        "\n",
        "## Practical Considerations\n",
        "\n",
        "- The choice of learning rate $\\eta$ significantly affects convergence speed and stability.\n",
        "- Shuffling the dataset before each epoch improves performance by reducing correlation between samples.\n",
        "- Mini-batch SGD generalizes this concept by using small batches instead of single samples.\n",
        "\n",
        "## Summary\n",
        "\n",
        "SGD is a foundational optimization method for training machine learning models, trading off exact gradient computation for faster, noisier updates. Its efficiency and effectiveness make it a default choice for many large-scale learning tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mini-Batch Gradient Descent in Machine Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Mini-Batch Gradient Descent is an optimization technique that combines the advantages of Batch Gradient Descent and Stochastic Gradient Descent (SGD). Instead of computing gradients over the entire dataset or a single data point, it calculates gradients using small subsets called mini-batches. This balances the computational efficiency and the stability of the updates.\n",
        "\n",
        "## Working Principle\n",
        "\n",
        "Given a dataset with $N$ samples, the dataset is divided into mini-batches of size $m$ where $m < N$. At each iteration, the model parameters $\\theta$ are updated using the average gradient computed from one mini-batch.\n",
        "\n",
        "The update rule is:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_\\theta L_i(\\theta)\n",
        "$$\n",
        "\n",
        "Here, $\\eta$ is the learning rate, and $L_i(\\theta)$ is the loss for the $i$-th sample in the mini-batch.\n",
        "\n",
        "## Mathematical Formulation\n",
        "\n",
        "The overall loss function over the dataset is:\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\frac{1}{N} \\sum_{i=1}^N L_i(\\theta)\n",
        "$$\n",
        "\n",
        "Mini-batch gradient descent approximates this loss gradient by:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta L(\\theta) \\approx \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_\\theta L_i(\\theta)\n",
        "$$\n",
        "\n",
        "where the summation is over samples in the current mini-batch.\n",
        "\n",
        "## Advantages of Mini-Batch Gradient Descent\n",
        "\n",
        "Mini-batch gradient descent offers faster convergence than batch gradient descent by updating parameters more frequently. It reduces the variance of parameter updates compared to SGD, which helps in smoother convergence. Mini-batches also enable efficient use of vectorized operations and parallelism on modern hardware like GPUs.\n",
        "\n",
        "## Practical Considerations\n",
        "\n",
        "- The mini-batch size $m$ is a hyperparameter. Typical values range from 32 to 256 depending on the problem and hardware.\n",
        "- Shuffling the dataset before creating mini-batches helps improve model generalization.\n",
        "- Learning rate $\\eta$ remains a crucial hyperparameter for stable and efficient training.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Mini-Batch Gradient Descent is a compromise between Batch Gradient Descent and SGD that combines efficiency and stability by updating model parameters using gradients computed over small batches of data, accelerating training and improving convergence behavior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RMSProp Optimizer in Deep Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "RMSProp (Root Mean Square Propagation) is an adaptive learning rate optimization algorithm designed to accelerate convergence and improve training stability in deep learning models. It adjusts the learning rate for each parameter individually by dividing the learning rate by a moving average of recent squared gradients.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Standard Gradient Descent uses a fixed learning rate, which can cause slow convergence or oscillations, especially in ravines or regions with varying gradient scales. RMSProp adapts the learning rate dynamically to mitigate these issues, making it well-suited for non-stationary objectives and online learning.\n",
        "\n",
        "## Algorithm Details\n",
        "\n",
        "At each iteration $t$, for parameter $\\theta_t$, RMSProp computes:\n",
        "\n",
        "1. The squared gradient moving average:\n",
        "\n",
        "$$\n",
        "E[g^2]_t = \\gamma E[g^2]_{t-1} + (1 - \\gamma) g_t^2\n",
        "$$\n",
        "\n",
        "where $g_t = \\nabla_{\\theta} L(\\theta_t)$ is the gradient at iteration $t$, and $\\gamma$ is the decay rate (commonly set around 0.9).\n",
        "\n",
        "2. The parameter update is then:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\, g_t\n",
        "$$\n",
        "\n",
        "Here, $\\eta$ is the base learning rate and $\\epsilon$ is a small constant (e.g., $10^{-8}$) added for numerical stability.\n",
        "\n",
        "## Explanation\n",
        "\n",
        "- The moving average $E[g^2]_t$ acts as a normalization factor, scaling the learning rate inversely proportional to the recent magnitude of gradients.\n",
        "- Parameters with consistently large gradients receive smaller effective learning rates, preventing drastic oscillations.\n",
        "- Parameters with small or infrequent gradients have relatively larger learning rates, enabling efficient updates.\n",
        "\n",
        "## Benefits of RMSProp\n",
        "\n",
        "RMSProp combines the benefits of Adagrad (which adapts learning rates) and momentum-based optimizers, while avoiding the rapid decay of learning rates seen in Adagrad. It is particularly effective in recurrent neural networks and online learning scenarios.\n",
        "\n",
        "## Summary\n",
        "\n",
        "RMSProp dynamically adjusts the learning rate by normalizing gradients with an exponentially decaying average of squared gradients. This improves convergence speed and stability, making it a popular choice in deep learning optimizers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Adam Optimizer\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Adam (Adaptive Moment Estimation) is an advanced optimization algorithm widely used in deep learning due to its efficient handling of sparse gradients and noisy problems. It combines ideas from Momentum and RMSProp optimizers, maintaining adaptive learning rates for each parameter with estimates of first and second moments of the gradients.\n",
        "\n",
        "## Core Concept\n",
        "\n",
        "Adam calculates adaptive learning rates by tracking:\n",
        "\n",
        "- The exponentially decaying average of past gradients (first moment, mean).\n",
        "- The exponentially decaying average of past squared gradients (second moment, variance).\n",
        "\n",
        "This allows Adam to adaptively adjust step sizes for each parameter, improving convergence speed and robustness.\n",
        "\n",
        "## Algorithm Formulation\n",
        "\n",
        "At iteration $t$, for parameter $\\theta_t$:\n",
        "\n",
        "1. Compute the gradient of the loss function $L$ with respect to $\\theta_t$:\n",
        "\n",
        "$$\n",
        "g_t = \\nabla_{\\theta} L(\\theta_t)\n",
        "$$\n",
        "\n",
        "2. Update biased first moment estimate (mean of gradients):\n",
        "\n",
        "$$\n",
        "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
        "$$\n",
        "\n",
        "3. Update biased second moment estimate (mean of squared gradients):\n",
        "\n",
        "$$\n",
        "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
        "$$\n",
        "\n",
        "Here, $\\beta_1$ and $\\beta_2$ are decay rates, typically $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$.\n",
        "\n",
        "4. Correct bias in moment estimates to counteract initialization at zero:\n",
        "\n",
        "$$\n",
        "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
        "$$\n",
        "\n",
        "5. Update parameters using the corrected moments:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\n",
        "$$\n",
        "\n",
        "where $\\eta$ is the learning rate and $\\epsilon$ is a small constant (e.g., $10^{-8}$) to prevent division by zero.\n",
        "\n",
        "## Explanation\n",
        "\n",
        "- The first moment $m_t$ captures the direction of the gradient (momentum).\n",
        "- The second moment $v_t$ captures the gradient magnitude (variance).\n",
        "- Bias correction compensates for initialization effects in early steps.\n",
        "- The parameter update normalizes the step size by the root of variance, stabilizing updates.\n",
        "\n",
        "## Advantages\n",
        "\n",
        "Adam provides:\n",
        "\n",
        "- Fast convergence due to momentum and adaptive learning rates.\n",
        "- Robustness to noisy or sparse gradients.\n",
        "- Minimal hyperparameter tuning requirements.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Adam is a powerful optimizer combining momentum and adaptive learning rate scaling through first and second moment estimation. It is highly effective and popular for training complex neural networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Convolutional Neural Networks (CNNs)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Digital Image Processing Basics\n",
        "\n",
        "## Introduction to Digital Image Processing\n",
        "\n",
        "Digital Image Processing (DIP) refers to the use of computer algorithms to perform image processing on digital images. It involves manipulating pixel values to enhance images, extract information, or prepare images for analysis.\n",
        "\n",
        "An image in DIP is represented as a two-dimensional function:\n",
        "\n",
        "$$\n",
        "f(x, y)\n",
        "$$\n",
        "\n",
        "where $x$ and $y$ denote spatial coordinates, and $f(x, y)$ denotes the intensity or gray level at that point.\n",
        "\n",
        "## Digital Image Representation\n",
        "\n",
        "A digital image consists of discrete pixels arranged in a grid. Each pixel value represents the intensity level, quantized into finite levels, typically ranging from 0 to 255 for an 8-bit grayscale image.\n",
        "\n",
        "- The total number of pixels is $M \\times N$, where $M$ is the height and $N$ is the width of the image.\n",
        "- The intensity values are stored as an integer matrix:\n",
        "\n",
        "$$\n",
        "I = \\{f(x,y) | x = 0, 1, ..., M-1; y = 0, 1, ..., N-1\\}\n",
        "$$\n",
        "\n",
        "## Basic Operations in Image Processing\n",
        "\n",
        "### Image Enhancement\n",
        "\n",
        "Enhancement improves the visual appearance or highlights features. Common operations include contrast adjustment, histogram equalization, and filtering.\n",
        "\n",
        "Histogram equalization redistributes pixel intensities to enhance contrast. Given the histogram $p(r_k)$ of intensity levels $r_k$, the transformation is:\n",
        "\n",
        "$$\n",
        "s_k = T(r_k) = \\sum_{j=0}^k p(r_j)\n",
        "$$\n",
        "\n",
        "where $s_k$ is the new intensity after equalization.\n",
        "\n",
        "### Image Restoration\n",
        "\n",
        "Restoration attempts to reconstruct or recover an image degraded by noise or distortions. This often uses inverse filtering or Wiener filtering.\n",
        "\n",
        "### Image Compression\n",
        "\n",
        "Compression reduces storage and transmission costs. It can be lossless or lossy.\n",
        "\n",
        "The compression ratio is defined as:\n",
        "\n",
        "$$\n",
        "\\text{Compression Ratio} = \\frac{\\text{Original Image Size}}{\\text{Compressed Image Size}}\n",
        "$$\n",
        "\n",
        "### Image Segmentation\n",
        "\n",
        "Segmentation partitions an image into meaningful regions, typically by detecting edges or boundaries.\n",
        "\n",
        "## Types of Images\n",
        "\n",
        "- **Binary images:** Only two intensity levels (0 and 1).\n",
        "- **Grayscale images:** Multiple intensity levels, usually 256 levels (8-bit).\n",
        "- **Color images:** Represented as three components (e.g., RGB), each with intensity values.\n",
        "\n",
        "## Spatial and Frequency Domains\n",
        "\n",
        "Image processing can be performed in:\n",
        "\n",
        "- **Spatial domain:** Direct manipulation of pixels. A spatial operation can be described as:\n",
        "\n",
        "$$\n",
        "g(x,y) = T[f(x,y)]\n",
        "$$\n",
        "\n",
        "where $T$ is an operator on pixels.\n",
        "\n",
        "- **Frequency domain:** Image is transformed using Fourier Transform to analyze frequency components. The 2D Discrete Fourier Transform (DFT) is:\n",
        "\n",
        "$$\n",
        "F(u,v) = \\sum_{x=0}^{M-1} \\sum_{y=0}^{N-1} f(x,y) e^{-j 2 \\pi \\left(\\frac{ux}{M} + \\frac{vy}{N}\\right)}\n",
        "$$\n",
        "\n",
        "where $F(u,v)$ represents the frequency component at coordinates $(u,v)$.\n",
        "\n",
        "## Noise in Images\n",
        "\n",
        "Common noise types:\n",
        "\n",
        "- **Gaussian noise:** Statistical noise with Gaussian distribution.\n",
        "- **Salt-and-pepper noise:** Random occurrences of black and white pixels.\n",
        "- **Speckle noise:** Multiplicative noise common in radar images.\n",
        "\n",
        "Noise reduction often involves filtering techniques such as mean, median, or Gaussian filters.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Digital Image Processing manipulates images via mathematical operations on pixel values to improve quality, extract features, or compress data. It spans various domains including spatial and frequency, and involves foundational operations like enhancement, restoration, segmentation, and compression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importance of Convolutional Neural Networks (CNN) in Machine Learning\n",
        "\n",
        "## Introduction to CNNs\n",
        "\n",
        "Convolutional Neural Networks (CNNs) are specialized neural networks designed primarily for processing grid-like data such as images. CNNs leverage spatial hierarchies by using convolutional layers that apply filters to input data, enabling efficient feature extraction and reducing the number of parameters compared to fully connected networks.\n",
        "\n",
        "## Key Concepts of CNN\n",
        "\n",
        "### Convolution Operation\n",
        "\n",
        "Convolution is the core operation in CNNs, defined for a 2D input image $I$ and a filter (kernel) $K$ as:\n",
        "\n",
        "$$\n",
        "S(i,j) = (I * K)(i,j) = \\sum_m \\sum_n I(i - m, j - n) \\cdot K(m, n)\n",
        "$$\n",
        "\n",
        "Here, $S$ is the feature map resulting from sliding the kernel $K$ over the input image $I$.\n",
        "\n",
        "### Feature Maps and Filters\n",
        "\n",
        "- Filters detect local patterns such as edges, textures, and shapes.\n",
        "- Multiple filters generate multiple feature maps, capturing diverse features.\n",
        "\n",
        "### Pooling Operation\n",
        "\n",
        "Pooling reduces the spatial size of feature maps to decrease computational load and control overfitting. Max pooling, for example, selects the maximum value in a sub-region:\n",
        "\n",
        "$$\n",
        "P(i,j) = \\max_{\\substack{m \\in R_i \\\\ n \\in R_j}} S(m,n)\n",
        "$$\n",
        "\n",
        "where $R_i$, $R_j$ define the pooling window.\n",
        "\n",
        "## Advantages of CNN over Traditional Neural Networks\n",
        "\n",
        "1. **Parameter Sharing**\n",
        "\n",
        "Unlike fully connected layers, convolutional layers share weights (filter coefficients) across spatial locations. This drastically reduces the number of parameters from $O(N^2)$ to $O(k^2)$ for an input of size $N \\times N$ and kernel size $k \\times k$.\n",
        "\n",
        "2. **Translation Invariance**\n",
        "\n",
        "CNNs inherently recognize features regardless of their spatial position in the input, thanks to convolution and pooling.\n",
        "\n",
        "3. **Hierarchical Feature Learning**\n",
        "\n",
        "Early layers learn simple features (e.g., edges), while deeper layers combine these into complex features (e.g., object parts).\n",
        "\n",
        "## CNN Architecture Overview\n",
        "\n",
        "A typical CNN consists of alternating convolutional and pooling layers, followed by fully connected layers. The overall operation of a CNN can be expressed as:\n",
        "\n",
        "$$\n",
        "\\hat{y} = f\\left(W^{(L)} \\cdot \\phi \\left( \\cdots \\phi \\left( W^{(2)} \\cdot \\phi \\left( W^{(1)} * X + b^{(1)} \\right) + b^{(2)} \\right) \\cdots \\right) + b^{(L)} \\right)\n",
        "$$\n",
        "\n",
        "where \n",
        "\n",
        "- $X$ is the input,\n",
        "- $W^{(l)}$ and $b^{(l)}$ are weights and biases at layer $l$,\n",
        "- $\\phi$ is a nonlinear activation function,\n",
        "- $*$ denotes convolution,\n",
        "- $\\hat{y}$ is the output prediction.\n",
        "\n",
        "## Applications of CNN\n",
        "\n",
        "CNNs have revolutionized areas such as:\n",
        "\n",
        "- Image classification\n",
        "- Object detection\n",
        "- Facial recognition\n",
        "- Medical image analysis\n",
        "- Video analysis\n",
        "- Natural language processing (via 1D convolutions)\n",
        "\n",
        "## Summary\n",
        "\n",
        "CNNs are powerful due to their ability to efficiently learn hierarchical spatial features through convolution and pooling. They reduce parameter complexity via weight sharing and provide translation invariance, making them superior for tasks involving visual data compared to traditional neural networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Strided Convolutions in Machine Learning\n",
        "\n",
        "## Overview of Strided Convolutions\n",
        "\n",
        "Strided convolution is a variation of the standard convolution operation where the filter (kernel) moves over the input with a step size greater than one, known as the stride. This technique reduces the spatial dimensions of the output feature map, effectively performing downsampling during convolution.\n",
        "\n",
        "## Definition of Stride\n",
        "\n",
        "If the stride is denoted by $s$, then the convolution filter moves $s$ pixels at each step along the width and height of the input. The stride controls how much the filter shifts during convolution.\n",
        "\n",
        "## Mathematical Expression of Output Size\n",
        "\n",
        "Given an input with spatial dimensions $W \\times H$, a filter (kernel) of size $K \\times K$, padding $P$, and stride $s$, the size of the output feature map $O$ is computed as:\n",
        "\n",
        "$$\n",
        "O = \\left\\lfloor \\frac{W - K + 2P}{s} \\right\\rfloor + 1\n",
        "$$\n",
        "\n",
        "The same applies for both width and height dimensions.\n",
        "\n",
        "## Effect of Strided Convolution\n",
        "\n",
        "1. **Downsampling**\n",
        "\n",
        "Strided convolutions reduce the spatial size of the output feature map by skipping positions in the input. This helps in reducing computational cost and the number of parameters in deeper layers.\n",
        "\n",
        "2. **Parameter Efficiency**\n",
        "\n",
        "By reducing the output size, strided convolution can replace pooling layers or be used alongside them to perform feature map size reduction.\n",
        "\n",
        "3. **Information Preservation**\n",
        "\n",
        "Compared to pooling, strided convolutions learn weights while downsampling, potentially capturing more meaningful features.\n",
        "\n",
        "## Formula for Convolution with Stride\n",
        "\n",
        "Let $I$ represent the input, $K$ the kernel, and $s$ the stride. Then the output at position $(i,j)$ is:\n",
        "\n",
        "$$\n",
        "S(i,j) = \\sum_{m=0}^{K-1} \\sum_{n=0}^{K-1} I(s \\cdot i + m, s \\cdot j + n) \\cdot K(m,n)\n",
        "$$\n",
        "\n",
        "Here, the input is sampled every $s$ steps, effectively reducing output resolution.\n",
        "\n",
        "## Comparison to Standard Convolution\n",
        "\n",
        "For stride $s=1$, the convolution processes every position in the input. For $s>1$, fewer positions are processed, resulting in smaller output dimensions.\n",
        "\n",
        "## Application and Usage\n",
        "\n",
        "Strided convolutions are widely used in convolutional neural networks (CNNs) for:\n",
        "\n",
        "- Efficient feature extraction at multiple scales.\n",
        "- Controlled reduction of feature map sizes without separate pooling layers.\n",
        "- Enabling deeper architectures with manageable computation.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Strided convolution extends standard convolution by moving the kernel in steps larger than one pixel, enabling learned downsampling and efficient spatial dimension reduction. The output size depends on stride, kernel size, input size, and padding, allowing flexibility in neural network design.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Padding in Convolutional Neural Networks (CNN)\n",
        "\n",
        "## Purpose of Padding\n",
        "\n",
        "Padding in CNNs is the technique of adding extra pixels around the border of the input image or feature map before applying convolution. The main reasons for padding are:\n",
        "\n",
        "- To control the spatial size of the output feature map.\n",
        "- To preserve important edge information during convolution.\n",
        "- To enable the kernel to properly cover border pixels.\n",
        "\n",
        "## Types of Padding\n",
        "\n",
        "### Valid Padding (No Padding)\n",
        "\n",
        "In valid padding, no extra pixels are added. The convolution kernel only slides within the original input boundaries. This causes the output size to shrink.\n",
        "\n",
        "### Same Padding (Zero Padding)\n",
        "\n",
        "Same padding adds zeros around the input so that the output spatial dimensions remain the same as the input (assuming stride = 1). It preserves spatial dimensions and avoids losing edge information.\n",
        "\n",
        "## Mathematical Formula for Output Size with Padding\n",
        "\n",
        "Given an input of size $W \\times H$, kernel size $K \\times K$, padding $P$, and stride $s$, the output dimension $O$ is computed as:\n",
        "\n",
        "$$\n",
        "O = \\left\\lfloor \\frac{W - K + 2P}{s} \\right\\rfloor + 1\n",
        "$$\n",
        "\n",
        "For same padding and stride $s=1$, to maintain output size equal to input size:\n",
        "\n",
        "$$\n",
        "P = \\frac{K - 1}{2}\n",
        "$$\n",
        "\n",
        "(usually $K$ is odd for symmetric padding)\n",
        "\n",
        "## Zero Padding Details\n",
        "\n",
        "Zero padding means adding pixels with value zero around the border of the input matrix. For example, padding of size $P=1$ adds a one-pixel-wide border of zeros around the input.\n",
        "\n",
        "## Impact of Padding on Feature Maps\n",
        "\n",
        "- **Without Padding:** Output shrinks, losing border information.\n",
        "- **With Padding:** Output size can be preserved, helping the network better learn features near edges.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Padding is an essential operation in CNNs that influences output size, feature representation near image borders, and overall network performance. Proper padding choice balances preserving spatial dimensions and computational efficiency.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convolution Layers in Neural Networks\n",
        "\n",
        "## Introduction to Convolution Layers\n",
        "\n",
        "Convolution layers are fundamental building blocks of Convolutional Neural Networks (CNNs). They automatically learn spatial hierarchies of features from input images or other grid-like data by applying convolution operations with learnable filters (kernels). These layers extract local patterns such as edges, textures, or shapes, which are essential for tasks like image recognition and classification.\n",
        "\n",
        "## Structure of a Convolution Layer\n",
        "\n",
        "A convolution layer consists of multiple filters, each represented by a kernel matrix $K$ of size $k \\times k$. These kernels slide over the input feature map $I$ and produce output feature maps by computing local weighted sums.\n",
        "\n",
        "## Mathematical Definition of Convolution Operation\n",
        "\n",
        "For an input feature map $I$ of spatial dimensions $W \\times H$ and a kernel $K$ of size $k \\times k$, the convolution output at position $(i,j)$ is given by:\n",
        "\n",
        "$$\n",
        "S(i,j) = \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} I(i+m, j+n) \\cdot K(m,n)\n",
        "$$\n",
        "\n",
        "This operation is performed across the entire input, producing a feature map $S$.\n",
        "\n",
        "## Multiple Filters and Depth\n",
        "\n",
        "Convolution layers typically have $F$ filters, each producing one output channel. For an input with $C$ channels, each filter has dimensions $k \\times k \\times C$. The convolution operation extends as:\n",
        "\n",
        "$$\n",
        "S_f(i,j) = \\sum_{c=0}^{C-1} \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} I_c(i+m, j+n) \\cdot K_{f,c}(m,n)\n",
        "$$\n",
        "\n",
        "for filter $f$, where $I_c$ is the input channel $c$, and $K_{f,c}$ is the kernel weights for filter $f$ and input channel $c$.\n",
        "\n",
        "## Padding and Stride in Convolution Layers\n",
        "\n",
        "To control output spatial size, convolution layers use:\n",
        "\n",
        "- **Padding** $P$: Number of pixels added around the input border to preserve spatial dimensions.\n",
        "- **Stride** $s$: Step size with which the kernel moves over the input.\n",
        "\n",
        "The output spatial dimension $O$ for width or height is:\n",
        "\n",
        "$$\n",
        "O = \\left\\lfloor \\frac{W - k + 2P}{s} \\right\\rfloor + 1\n",
        "$$\n",
        "\n",
        "## Parameters and Learnability\n",
        "\n",
        "Each convolution layer has trainable parameters consisting of weights in the kernels and optionally bias terms. The number of parameters per layer is:\n",
        "\n",
        "$$\n",
        "\\text{Params} = F \\times (k \\times k \\times C) + F\n",
        "$$\n",
        "\n",
        "where the $+F$ accounts for biases for each filter.\n",
        "\n",
        "## Role in Feature Extraction\n",
        "\n",
        "Convolution layers hierarchically learn to detect:\n",
        "\n",
        "- Low-level features (edges, corners) in early layers.\n",
        "- Mid-level features (textures, shapes) in intermediate layers.\n",
        "- High-level semantic features (objects, faces) in deeper layers.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Convolution layers apply learnable kernels across input data to extract spatial features while controlling output dimensions via padding and stride. They form the backbone of CNNs by enabling hierarchical and parameter-efficient feature learning for various machine learning tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pooling Layers in Convolutional Neural Networks\n",
        "\n",
        "## Overview of Pooling Layers\n",
        "\n",
        "Pooling layers are essential components in Convolutional Neural Networks (CNNs) used to progressively reduce the spatial dimensions of input feature maps. This downsampling reduces computational complexity, controls overfitting, and helps the network learn invariant features by summarizing regions.\n",
        "\n",
        "## Purpose and Benefits\n",
        "\n",
        "Pooling layers:\n",
        "\n",
        "- Reduce feature map size, decreasing the number of parameters and computation.\n",
        "- Provide translation invariance by summarizing spatial neighborhoods.\n",
        "- Help mitigate overfitting by abstracting feature representations.\n",
        "\n",
        "## Common Types of Pooling\n",
        "\n",
        "### Max Pooling\n",
        "\n",
        "Max pooling selects the maximum value from a defined window (pool size) within the feature map. For a window of size $p \\times p$, max pooling output at location $(i,j)$ is:\n",
        "\n",
        "$$\n",
        "P(i,j) = \\max_{0 \\leq m < p,\\, 0 \\leq n < p} S(i \\times s + m, j \\times s + n)\n",
        "$$\n",
        "\n",
        "where $S$ is the input feature map, and $s$ is the stride (step size).\n",
        "\n",
        "### Average Pooling\n",
        "\n",
        "Average pooling computes the average value within the pooling window:\n",
        "\n",
        "$$\n",
        "P(i,j) = \\frac{1}{p^2} \\sum_{m=0}^{p-1} \\sum_{n=0}^{p-1} S(i \\times s + m, j \\times s + n)\n",
        "$$\n",
        "\n",
        "## Spatial Dimension Calculation\n",
        "\n",
        "Given an input feature map size $W \\times H$, pool size $p$, stride $s$, and optional padding $P$, the output size $O$ for each spatial dimension is:\n",
        "\n",
        "$$\n",
        "O = \\left\\lfloor \\frac{W - p + 2P}{s} \\right\\rfloor + 1\n",
        "$$\n",
        "\n",
        "Pooling usually does not add padding, so $P=0$.\n",
        "\n",
        "## Effect on Feature Maps\n",
        "\n",
        "Pooling layers reduce the spatial resolution while preserving the number of channels. If the input has $C$ channels, the output also has $C$ channels, but smaller width and height.\n",
        "\n",
        "## Pooling Layer Characteristics\n",
        "\n",
        "- Pooling does not have trainable parameters; it is a fixed function.\n",
        "- It is typically applied after convolutional layers to gradually reduce spatial size.\n",
        "- Helps the network focus on dominant features and increases robustness to small translations.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Pooling layers perform downsampling by summarizing local neighborhoods in feature maps through max or average operations. They play a crucial role in reducing computational load, preventing overfitting, and providing spatial invariance in CNN architectures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fully Connected Layers in Deep Learning\n",
        "\n",
        "## Introduction to Fully Connected Layers\n",
        "\n",
        "A Fully Connected (FC) layer, also known as a dense layer, is a fundamental building block in deep neural networks. In an FC layer, every input neuron is connected to every output neuron. This layer performs a weighted sum of the inputs followed by an activation function, enabling the network to learn complex relationships.\n",
        "\n",
        "## Structure and Operation\n",
        "\n",
        "Given an input vector $x \\in \\mathbb{R}^n$, the fully connected layer computes the output vector $y \\in \\mathbb{R}^m$ as:\n",
        "\n",
        "$$\n",
        "y = f(Wx + b)\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "- $W$ is an $m \\times n$ weight matrix,\n",
        "- $b$ is a bias vector of dimension $m$,\n",
        "- $f(\\cdot)$ is the activation function applied element-wise.\n",
        "\n",
        "Each output neuron $y_j$ is calculated by:\n",
        "\n",
        "$$\n",
        "y_j = f\\left(\\sum_{i=1}^n W_{ji} x_i + b_j\\right)\n",
        "$$\n",
        "\n",
        "## Role in Neural Networks\n",
        "\n",
        "Fully connected layers aggregate features extracted by earlier layers (such as convolutional or pooling layers) and transform them into the final output space. They are often used in the last layers of networks for tasks like classification or regression.\n",
        "\n",
        "## Parameter Complexity\n",
        "\n",
        "The total number of parameters in a fully connected layer is:\n",
        "\n",
        "$$\n",
        "\\text{Parameters} = m \\times n + m\n",
        "$$\n",
        "\n",
        "which includes the weights and biases.\n",
        "\n",
        "## Advantages and Considerations\n",
        "\n",
        "- FC layers are capable of learning complex, non-linear combinations of features.\n",
        "- Due to dense connectivity, they require significant memory and computation for large input sizes.\n",
        "- They tend to overfit if not regularized properly, especially in deep networks.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Fully connected layers connect every input neuron to every output neuron using learned weights and biases. They perform linear transformations followed by non-linear activations, playing a critical role in mapping learned features to final predictions in deep learning models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch Normalization in Convolutional Neural Networks (CNNs)\n",
        "\n",
        "## Introduction to Batch Normalization\n",
        "\n",
        "Batch Normalization (BN) is a technique designed to improve the training of deep neural networks by normalizing the inputs of each layer. It reduces internal covariate shift, stabilizes learning, and accelerates convergence. BN is widely used in CNNs to improve performance and training stability.\n",
        "\n",
        "## Motivation for Batch Normalization\n",
        "\n",
        "During training, the distribution of inputs to each layer changes as parameters in previous layers update. This internal covariate shift slows down training because each layer must continuously adapt to new input distributions. Batch Normalization addresses this by normalizing layer inputs to maintain a stable distribution.\n",
        "\n",
        "## Batch Normalization Operation\n",
        "\n",
        "Given an input mini-batch of activations for a particular layer:\n",
        "\n",
        "$$\n",
        "\\mathcal{B} = \\{x_1, x_2, \\dots, x_m\\}\n",
        "$$\n",
        "\n",
        "where $m$ is the batch size and each $x_i$ is an activation vector (or scalar in convolutional layers), BN performs the following steps for each feature dimension:\n",
        "\n",
        "1. Compute the batch mean:\n",
        "\n",
        "$$\n",
        "\\mu_\\mathcal{B} = \\frac{1}{m} \\sum_{i=1}^m x_i\n",
        "$$\n",
        "\n",
        "2. Compute the batch variance:\n",
        "\n",
        "$$\n",
        "\\sigma_\\mathcal{B}^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_\\mathcal{B})^2\n",
        "$$\n",
        "\n",
        "3. Normalize each activation:\n",
        "\n",
        "$$\n",
        "\\hat{x}_i = \\frac{x_i - \\mu_\\mathcal{B}}{\\sqrt{\\sigma_\\mathcal{B}^2 + \\epsilon}}\n",
        "$$\n",
        "\n",
        "where $\\epsilon$ is a small constant added for numerical stability.\n",
        "\n",
        "4. Scale and shift the normalized value:\n",
        "\n",
        "$$\n",
        "y_i = \\gamma \\hat{x}_i + \\beta\n",
        "$$\n",
        "\n",
        "Here, $\\gamma$ and $\\beta$ are learnable parameters that allow the network to restore the representation power if needed.\n",
        "\n",
        "## Role of Parameters $\\gamma$ and $\\beta$\n",
        "\n",
        "- $\\gamma$ controls the scaling of normalized values.\n",
        "- $\\beta$ controls the shifting.\n",
        "- These parameters enable the network to learn the optimal scale and mean for the normalized activations.\n",
        "\n",
        "## Benefits of Batch Normalization\n",
        "\n",
        "- **Faster training:** By reducing internal covariate shift, networks converge faster.\n",
        "- **Higher learning rates:** Allows use of larger learning rates without divergence.\n",
        "- **Regularization effect:** Acts as a form of regularization, reducing the need for dropout.\n",
        "- **Stabilizes gradients:** Prevents vanishing/exploding gradients by normalizing layer inputs.\n",
        "\n",
        "## Batch Normalization in CNNs\n",
        "\n",
        "In CNNs, BN is applied over each feature map independently by computing statistics across the mini-batch and spatial dimensions. For a feature map activation $x_i^{(k)}$ (where $k$ indexes channels):\n",
        "\n",
        "$$\n",
        "\\mu_\\mathcal{B}^{(k)} = \\frac{1}{mHW} \\sum_{i=1}^m \\sum_{h=1}^H \\sum_{w=1}^W x_i^{(k)}(h,w)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_\\mathcal{B}^{(k)2} = \\frac{1}{mHW} \\sum_{i=1}^m \\sum_{h=1}^H \\sum_{w=1}^W \\left( x_i^{(k)}(h,w) - \\mu_\\mathcal{B}^{(k)} \\right)^2\n",
        "$$\n",
        "\n",
        "Normalization is then performed per channel.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Batch Normalization normalizes layer inputs within each mini-batch to stabilize training, accelerate convergence, and improve generalization. It introduces learnable scale and shift parameters to preserve representation ability, making it an essential technique in modern CNN architectures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Backpropagation in Convolutional Neural Networks (CNNs)\n",
        "\n",
        "## Introduction to Backpropagation in CNNs\n",
        "\n",
        "Backpropagation is a fundamental algorithm used to train neural networks by minimizing the loss function through gradient descent. In CNNs, backpropagation involves computing gradients of the loss with respect to filters (kernels), biases, and inputs, considering the convolutional and pooling operations.\n",
        "\n",
        "## Overview of the Backpropagation Process\n",
        "\n",
        "Backpropagation consists of two main phases:\n",
        "\n",
        "- **Forward pass:** Input data passes through convolutional, activation, and pooling layers to produce output and calculate the loss.\n",
        "\n",
        "- **Backward pass:** Gradients of the loss are propagated backward through the network to update weights and biases.\n",
        "\n",
        "The key is to compute gradients efficiently for convolution operations.\n",
        "\n",
        "## Notation and Setup\n",
        "\n",
        "Let:\n",
        "\n",
        "- $x$ be the input to a convolutional layer.\n",
        "- $w$ be the convolution kernel (filter).\n",
        "- $b$ be the bias term.\n",
        "- $y$ be the output feature map after convolution.\n",
        "- $L$ be the loss function.\n",
        "\n",
        "The forward convolution operation can be expressed as:\n",
        "\n",
        "$$\n",
        "y = x * w + b\n",
        "$$\n",
        "\n",
        "where $*$ denotes the convolution operation.\n",
        "\n",
        "## Calculating Gradients\n",
        "\n",
        "During backpropagation, we need:\n",
        "\n",
        "- Gradient of loss w.r.t weights: $\\frac{\\partial L}{\\partial w}$\n",
        "- Gradient of loss w.r.t biases: $\\frac{\\partial L}{\\partial b}$\n",
        "- Gradient of loss w.r.t inputs: $\\frac{\\partial L}{\\partial x}$\n",
        "\n",
        "Assuming the gradient of loss w.r.t output feature map $y$ is known as $\\delta y = \\frac{\\partial L}{\\partial y}$, we calculate:\n",
        "\n",
        "### Gradient w.r.t Weights\n",
        "\n",
        "The gradient of the loss w.r.t the filter weights $w$ is given by the convolution of input $x$ with the flipped gradient $\\delta y$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w} = x * \\delta y_{flipped}\n",
        "$$\n",
        "\n",
        "Here, $\\delta y_{flipped}$ is the gradient map flipped both horizontally and vertically due to convolution’s cross-correlation property.\n",
        "\n",
        "### Gradient w.r.t Biases\n",
        "\n",
        "Since bias is added linearly and shared across spatial positions, its gradient is the sum over all positions of $\\delta y$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = \\sum_{i,j} \\delta y_{i,j}\n",
        "$$\n",
        "\n",
        "### Gradient w.r.t Inputs\n",
        "\n",
        "The gradient w.r.t the input $x$ is computed by convolving the gradient $\\delta y$ with the filter $w$ rotated 180 degrees:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial x} = \\delta y * w_{rotated}\n",
        "$$\n",
        "\n",
        "This effectively backpropagates the error to the previous layer's inputs.\n",
        "\n",
        "## Handling Stride and Padding\n",
        "\n",
        "Gradients must account for stride ($s$) and padding ($p$) used during the forward pass:\n",
        "\n",
        "- When stride $s > 1$, the gradient $\\delta y$ is upsampled by inserting zeros between elements before convolution in the backward pass.\n",
        "\n",
        "- Padding influences the size of gradients to maintain consistency with input dimensions.\n",
        "\n",
        "## Backpropagation Through Pooling Layers\n",
        "\n",
        "For max pooling layers:\n",
        "\n",
        "- Gradients propagate only to the input positions that had the maximum value during the forward pass.\n",
        "\n",
        "For average pooling layers:\n",
        "\n",
        "- Gradients are distributed evenly across all inputs in the pooling window.\n",
        "\n",
        "## Summary of Backpropagation Steps in CNNs\n",
        "\n",
        "1. Compute $\\delta y = \\frac{\\partial L}{\\partial y}$ at the output layer.\n",
        "\n",
        "2. Calculate $\\frac{\\partial L}{\\partial w}$ by convolving input $x$ with $\\delta y$ flipped.\n",
        "\n",
        "3. Calculate $\\frac{\\partial L}{\\partial b}$ by summing all elements of $\\delta y$.\n",
        "\n",
        "4. Calculate $\\frac{\\partial L}{\\partial x}$ by convolving $\\delta y$ with rotated filter weights.\n",
        "\n",
        "5. Propagate $\\frac{\\partial L}{\\partial x}$ backward through preceding layers.\n",
        "\n",
        "Backpropagation in CNNs efficiently computes gradients by leveraging convolution properties, enabling effective end-to-end learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LeNet-5 Architecture in Deep Learning\n",
        "\n",
        "## Introduction to LeNet-5\n",
        "\n",
        "LeNet-5 is a pioneering convolutional neural network (CNN) designed by Yann LeCun for handwritten digit recognition (e.g., MNIST dataset). It introduced the concept of combining convolutional layers with subsampling (pooling) and fully connected layers to extract hierarchical features.\n",
        "\n",
        "LeNet-5 has 7 layers (excluding input), where each layer includes trainable parameters (weights and biases). The architecture uses tanh or sigmoid activation functions and is trained using backpropagation with gradient descent.\n",
        "\n",
        "## Input Layer\n",
        "\n",
        "The network accepts a grayscale image of size $32 \\times 32$ pixels.\n",
        "\n",
        "To handle smaller images like MNIST ($28 \\times 28$), zero-padding is applied.\n",
        "\n",
        "## Layer C1: First Convolutional Layer\n",
        "\n",
        "Applies 6 convolutional filters of size $5 \\times 5$ with stride 1.\n",
        "\n",
        "Input: $32 \\times 32$  \n",
        "Filter: $5 \\times 5$  \n",
        "Output: $28 \\times 28 \\times 6$\n",
        "\n",
        "Each feature map is computed as:\n",
        "\n",
        "$$\n",
        "C1_k = \\tanh(x * w_k + b_k)\n",
        "$$\n",
        "\n",
        "where $w_k$ and $b_k$ are the kernel and bias for the $k$-th map.\n",
        "\n",
        "## Layer S2: First Subsampling (Pooling) Layer\n",
        "\n",
        "Applies average pooling with a $2 \\times 2$ filter and stride 2.\n",
        "\n",
        "Input: $28 \\times 28 \\times 6$  \n",
        "Output: $14 \\times 14 \\times 6$\n",
        "\n",
        "Each element is computed as:\n",
        "\n",
        "$$\n",
        "S2_{i,j,k} = \\tanh\\left(\\frac{1}{4} \\sum x_{2i:2i+2, 2j:2j+2, k} + b_k\\right)\n",
        "$$\n",
        "\n",
        "## Layer C3: Second Convolutional Layer\n",
        "\n",
        "Uses 16 filters with varying connection schemes to the previous 6 maps.\n",
        "\n",
        "Filter: $5 \\times 5$  \n",
        "Input: $14 \\times 14 \\times 6$  \n",
        "Output: $10 \\times 10 \\times 16$\n",
        "\n",
        "This partial connectivity reduces parameters and enhances feature diversity.\n",
        "\n",
        "## Layer S4: Second Subsampling Layer\n",
        "\n",
        "Applies $2 \\times 2$ average pooling with stride 2.\n",
        "\n",
        "Input: $10 \\times 10 \\times 16$  \n",
        "Output: $5 \\times 5 \\times 16$\n",
        "\n",
        "The operation is similar to S2.\n",
        "\n",
        "## Layer C5: Fully Connected Convolutional Layer\n",
        "\n",
        "Although it's a convolutional layer, each unit is connected to all $5 \\times 5 \\times 16 = 400$ units from S4.\n",
        "\n",
        "Filter size: $5 \\times 5$  \n",
        "Input: $5 \\times 5 \\times 16$  \n",
        "Output: $1 \\times 1 \\times 120$  \n",
        "Flattened: $120$ features\n",
        "\n",
        "$$\n",
        "C5_i = \\tanh\\left(\\sum x * w_i + b_i\\right)\n",
        "$$\n",
        "\n",
        "## Layer F6: Fully Connected Layer\n",
        "\n",
        "A standard dense layer with 84 neurons.\n",
        "\n",
        "Each neuron:\n",
        "\n",
        "$$\n",
        "F6_j = \\tanh\\left(\\sum w_{ij} \\cdot C5_i + b_j\\right)\n",
        "$$\n",
        "\n",
        "## Output Layer\n",
        "\n",
        "10 neurons representing digit classes (0–9) using a softmax classifier:\n",
        "\n",
        "$$\n",
        "y_k = \\frac{e^{z_k}}{\\sum_{i=1}^{10} e^{z_i}}\n",
        "$$\n",
        "\n",
        "where $z_k$ is the input to the $k$-th output neuron.\n",
        "\n",
        "## Summary of Architecture\n",
        "\n",
        "Input: $32 \\times 32$  \n",
        "C1: $6$ maps of $28 \\times 28$  \n",
        "S2: $6$ maps of $14 \\times 14$  \n",
        "C3: $16$ maps of $10 \\times 10$  \n",
        "S4: $16$ maps of $5 \\times 5$  \n",
        "C5: $120$ units  \n",
        "F6: $84$ units  \n",
        "Output: $10$ classes\n",
        "\n",
        "LeNet-5 set the foundation for modern CNNs by introducing hierarchical feature extraction and local connectivity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to AlexNet in Deep Learning\n",
        "\n",
        "## Overview\n",
        "\n",
        "AlexNet is a deep convolutional neural network developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. It won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012 and significantly outperformed traditional methods. It popularized the use of GPUs for training deep networks and introduced key innovations like ReLU activation, dropout, and overlapping max pooling.\n",
        "\n",
        "AlexNet consists of 8 learned layers: 5 convolutional layers and 3 fully connected layers. It uses ReLU for non-linearity, softmax for classification, and dropout for regularization.\n",
        "\n",
        "## Input Layer\n",
        "\n",
        "Input to the network is an image of size $227 \\times 227 \\times 3$, obtained by resizing the original $224 \\times 224$ image and applying padding.\n",
        "\n",
        "## Layer 1: First Convolutional Layer\n",
        "\n",
        "Applies 96 filters of size $11 \\times 11 \\times 3$ with stride 4 and padding 0.\n",
        "\n",
        "Input: $227 \\times 227 \\times 3$  \n",
        "Filter: $11 \\times 11 \\times 3$  \n",
        "Stride: 4  \n",
        "Output: $55 \\times 55 \\times 96$\n",
        "\n",
        "Followed by ReLU and $3 \\times 3$ max pooling with stride 2, giving:\n",
        "\n",
        "Output after pooling: $27 \\times 27 \\times 96$\n",
        "\n",
        "## Layer 2: Second Convolutional Layer\n",
        "\n",
        "Applies 256 filters of size $5 \\times 5$ with stride 1 and padding 2.\n",
        "\n",
        "Input: $27 \\times 27 \\times 96$  \n",
        "Output: $27 \\times 27 \\times 256$\n",
        "\n",
        "Followed by ReLU and $3 \\times 3$ max pooling with stride 2:\n",
        "\n",
        "Output after pooling: $13 \\times 13 \\times 256$\n",
        "\n",
        "## Layer 3: Third Convolutional Layer\n",
        "\n",
        "Applies 384 filters of size $3 \\times 3$ with stride 1 and padding 1.\n",
        "\n",
        "Input: $13 \\times 13 \\times 256$  \n",
        "Output: $13 \\times 13 \\times 384$\n",
        "\n",
        "ReLU activation is applied.\n",
        "\n",
        "## Layer 4: Fourth Convolutional Layer\n",
        "\n",
        "Applies 384 filters of size $3 \\times 3$ with stride 1 and padding 1.\n",
        "\n",
        "Input: $13 \\times 13 \\times 384$  \n",
        "Output: $13 \\times 13 \\times 384$\n",
        "\n",
        "ReLU activation is applied.\n",
        "\n",
        "## Layer 5: Fifth Convolutional Layer\n",
        "\n",
        "Applies 256 filters of size $3 \\times 3$ with stride 1 and padding 1.\n",
        "\n",
        "Input: $13 \\times 13 \\times 384$  \n",
        "Output: $13 \\times 13 \\times 256$\n",
        "\n",
        "Followed by $3 \\times 3$ max pooling with stride 2:\n",
        "\n",
        "Output after pooling: $6 \\times 6 \\times 256$\n",
        "\n",
        "## Flattening Layer\n",
        "\n",
        "The output volume is flattened to a vector of size $6 \\times 6 \\times 256 = 9216$.\n",
        "\n",
        "## Fully Connected Layer 1 (FC6)\n",
        "\n",
        "Input: 9216  \n",
        "Output: 4096  \n",
        "Activation: ReLU  \n",
        "Dropout is applied with $p = 0.5$\n",
        "\n",
        "## Fully Connected Layer 2 (FC7)\n",
        "\n",
        "Input: 4096  \n",
        "Output: 4096  \n",
        "Activation: ReLU  \n",
        "Dropout is applied\n",
        "\n",
        "## Fully Connected Layer 3 (FC8)\n",
        "\n",
        "Input: 4096  \n",
        "Output: 1000 (for 1000 ImageNet classes)\n",
        "\n",
        "## Output Layer\n",
        "\n",
        "Softmax function is applied to produce class probabilities:\n",
        "\n",
        "$$\n",
        "P(y = j | x) = \\frac{e^{z_j}}{\\sum_{k=1}^{1000} e^{z_k}}\n",
        "$$\n",
        "\n",
        "where $z_j$ is the output of the final neuron for class $j$.\n",
        "\n",
        "## Key Features of AlexNet\n",
        "\n",
        "1. Uses ReLU: $f(x) = \\max(0, x)$  \n",
        "2. Overlapping max pooling to reduce spatial size  \n",
        "3. Dropout to prevent overfitting  \n",
        "4. Data augmentation (image translation, reflection)  \n",
        "5. Trained using stochastic gradient descent (SGD) with momentum:\n",
        "\n",
        "$$\n",
        "v_{t+1} = \\mu v_t - \\eta \\nabla L(w_t)  \n",
        "$$\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t + v_{t+1}\n",
        "$$\n",
        "\n",
        "where $\\mu$ is momentum and $\\eta$ is learning rate.\n",
        "\n",
        "## Summary of AlexNet Architecture\n",
        "\n",
        "Input: $227 \\times 227 \\times 3$  \n",
        "Conv1 + Pooling: $27 \\times 27 \\times 96$  \n",
        "Conv2 + Pooling: $13 \\times 13 \\times 256$  \n",
        "Conv3: $13 \\times 13 \\times 384$  \n",
        "Conv4: $13 \\times 13 \\times 384$  \n",
        "Conv5 + Pooling: $6 \\times 6 \\times 256$  \n",
        "FC6: 4096  \n",
        "FC7: 4096  \n",
        "FC8: 1000  \n",
        "Softmax: Probability scores for 1000 classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VGG-16 Convolutional Neural Network Architecture\n",
        "\n",
        "## Overview\n",
        "\n",
        "VGG-16 is a deep convolutional neural network architecture proposed by the Visual Geometry Group (VGG) at Oxford. It significantly improves accuracy by increasing the network depth using small $3 \\times 3$ convolutional filters throughout the architecture.\n",
        "\n",
        "VGG-16 contains 13 convolutional layers and 3 fully connected layers, making a total of 16 weight layers. It uses ReLU as the activation function and max pooling for downsampling. The model was trained on the ImageNet dataset and achieved excellent performance in classification tasks.\n",
        "\n",
        "## Input Layer\n",
        "\n",
        "The input image size is $224 \\times 224 \\times 3$.\n",
        "\n",
        "## Convolutional Blocks\n",
        "\n",
        "Each block contains multiple convolutional layers followed by a max pooling layer. The convolutional layers use filters of size $3 \\times 3$ with stride 1 and padding 1 to preserve spatial resolution.\n",
        "\n",
        "### Block 1\n",
        "\n",
        "2 convolutional layers:  \n",
        "$3 \\times 3$ filters, 64 channels  \n",
        "Input: $224 \\times 224 \\times 3$  \n",
        "Output: $224 \\times 224 \\times 64$  \n",
        "Followed by $2 \\times 2$ max pooling with stride 2:  \n",
        "Output: $112 \\times 112 \\times 64$\n",
        "\n",
        "### Block 2\n",
        "\n",
        "2 convolutional layers:  \n",
        "$3 \\times 3$ filters, 128 channels  \n",
        "Input: $112 \\times 112 \\times 64$  \n",
        "Output: $112 \\times 112 \\times 128$  \n",
        "Max pooling:  \n",
        "Output: $56 \\times 56 \\times 128$\n",
        "\n",
        "### Block 3\n",
        "\n",
        "3 convolutional layers:  \n",
        "$3 \\times 3$ filters, 256 channels  \n",
        "Input: $56 \\times 56 \\times 128$  \n",
        "Output: $56 \\times 56 \\times 256$  \n",
        "Max pooling:  \n",
        "Output: $28 \\times 28 \\times 256$\n",
        "\n",
        "### Block 4\n",
        "\n",
        "3 convolutional layers:  \n",
        "$3 \\times 3$ filters, 512 channels  \n",
        "Input: $28 \\times 28 \\times 256$  \n",
        "Output: $28 \\times 28 \\times 512$  \n",
        "Max pooling:  \n",
        "Output: $14 \\times 14 \\times 512$\n",
        "\n",
        "### Block 5\n",
        "\n",
        "3 convolutional layers:  \n",
        "$3 \\times 3$ filters, 512 channels  \n",
        "Input: $14 \\times 14 \\times 512$  \n",
        "Output: $14 \\times 14 \\times 512$  \n",
        "Max pooling:  \n",
        "Output: $7 \\times 7 \\times 512$\n",
        "\n",
        "## Fully Connected Layers\n",
        "\n",
        "The output of the last pooling layer is flattened to a 1D vector of size $7 \\times 7 \\times 512 = 25088$.\n",
        "\n",
        "### Fully Connected Layer 1\n",
        "\n",
        "Input: 25088  \n",
        "Output: 4096  \n",
        "Activation: ReLU\n",
        "\n",
        "### Fully Connected Layer 2\n",
        "\n",
        "Input: 4096  \n",
        "Output: 4096  \n",
        "Activation: ReLU\n",
        "\n",
        "### Fully Connected Layer 3\n",
        "\n",
        "Input: 4096  \n",
        "Output: 1000 (for classification into 1000 ImageNet classes)\n",
        "\n",
        "## Output Layer\n",
        "\n",
        "The final layer uses the softmax function to output class probabilities:\n",
        "\n",
        "$$\n",
        "P(y = j | x) = \\frac{e^{z_j}}{\\sum_{k=1}^{1000} e^{z_k}}\n",
        "$$\n",
        "\n",
        "where $z_j$ is the score for class $j$.\n",
        "\n",
        "## Activation Function\n",
        "\n",
        "All convolutional and fully connected layers use the ReLU activation:\n",
        "\n",
        "$$\n",
        "f(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "## Parameters\n",
        "\n",
        "The total number of parameters in VGG-16 is approximately 138 million. The depth and uniform structure of small convolutional filters enable better feature extraction with fewer parameters compared to larger kernels.\n",
        "\n",
        "## Summary\n",
        "\n",
        "VGG-16 architecture follows a simple and consistent pattern: multiple $3 \\times 3$ convolutions followed by max pooling, then fully connected layers for classification. Its key characteristics are depth, uniform filter size, and use of ReLU and max pooling to reduce spatial dimensions while increasing depth.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VGG-19 Network Architecture\n",
        "\n",
        "## Introduction\n",
        "\n",
        "VGGNet is a family of deep convolutional neural networks proposed by the Visual Geometry Group at Oxford. The most widely used variants are VGG-16 and VGG-19, where the numbers indicate the number of weight layers. The design focuses on simplicity by using small $3 \\times 3$ filters throughout the architecture.\n",
        "\n",
        "## Design Principles\n",
        "\n",
        "### Use of Small Filters\n",
        "\n",
        "All convolutional layers use $3 \\times 3$ filters with stride 1 and padding 1. This helps in capturing complex patterns using deep stacks of simple operations.\n",
        "\n",
        "A stack of two $3 \\times 3$ convolutions has the receptive field of a single $5 \\times 5$ convolution. A stack of three $3 \\times 3$ convolutions matches the receptive field of a $7 \\times 7$ convolution, but with fewer parameters and more non-linearities.\n",
        "\n",
        "### Layer Configuration\n",
        "\n",
        "The network contains a sequence of convolutional blocks followed by pooling. Each block increases the depth while reducing the spatial resolution.\n",
        "\n",
        "For VGG-16, the configuration is:\n",
        "\n",
        "$[2 \\times 64, 2 \\times 128, 3 \\times 256, 3 \\times 512, 3 \\times 512]$  \n",
        "where each entry denotes the number of convolutional layers and their depth.\n",
        "\n",
        "## Pooling Layers\n",
        "\n",
        "After each block of convolutions, a max pooling layer is applied using $2 \\times 2$ window with stride 2, reducing spatial resolution by half.\n",
        "\n",
        "## Fully Connected Layers\n",
        "\n",
        "After the convolutional and pooling layers, the output is flattened and passed through three fully connected layers.\n",
        "\n",
        "First FC layer: 4096 units  \n",
        "Second FC layer: 4096 units  \n",
        "Third FC layer: 1000 units (for ImageNet classification)\n",
        "\n",
        "## Activation Function\n",
        "\n",
        "Each layer uses ReLU as the activation function:\n",
        "\n",
        "$$\n",
        "f(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "ReLU introduces non-linearity, enabling the network to learn complex functions.\n",
        "\n",
        "## Output Layer\n",
        "\n",
        "The final layer uses softmax to produce class probabilities:\n",
        "\n",
        "$$\n",
        "P(y = j | x) = \\frac{e^{z_j}}{\\sum_{k=1}^{C} e^{z_k}}\n",
        "$$\n",
        "\n",
        "where $C$ is the number of classes (e.g., 1000 for ImageNet).\n",
        "\n",
        "## Parameter Efficiency\n",
        "\n",
        "Using small filters helps in reducing the number of parameters compared to large kernels. For example:\n",
        "\n",
        "A single $7 \\times 7$ convolution with $C$ channels has $49C^2$ parameters  \n",
        "Three $3 \\times 3$ convolutions have $27C^2$ parameters\n",
        "\n",
        "This reduction allows deeper architectures without increasing complexity.\n",
        "\n",
        "## VGG-19\n",
        "\n",
        "VGG-19 follows the same principle as VGG-16 but adds two more convolutional layers:\n",
        "\n",
        "$[2 \\times 64, 2 \\times 128, 4 \\times 256, 4 \\times 512, 4 \\times 512]$\n",
        "\n",
        "## Summary\n",
        "\n",
        "VGGNet emphasizes depth and simplicity, using uniform $3 \\times 3$ convolutions and $2 \\times 2$ pooling. It achieved excellent performance in classification tasks and remains a foundational architecture in computer vision. Its main trade-off is the high number of parameters (around 138 million for VGG-16), making it computationally intensive but effective.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GoogLeNet (Inception v1) CNN Architecture\n",
        "\n",
        "## Overview\n",
        "\n",
        "GoogLeNet, also known as Inception v1, is a deep convolutional neural network architecture that achieved top performance in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014. Unlike traditional CNNs, GoogLeNet employs a novel Inception module that allows the network to capture features at multiple scales efficiently.\n",
        "\n",
        "GoogLeNet is 22 layers deep (27 layers including pooling and softmax) and has significantly fewer parameters (~5 million) compared to other architectures like VGGNet (~138 million).\n",
        "\n",
        "## Design Motivation\n",
        "\n",
        "Increasing depth and width improves performance, but leads to overfitting and high computational cost. The Inception module addresses this by combining multiple kernel sizes in the same layer, enabling multi-scale feature extraction without dramatically increasing parameters.\n",
        "\n",
        "## Inception Module\n",
        "\n",
        "An Inception module applies the following in parallel:\n",
        "\n",
        "1. $1 \\times 1$ convolution  \n",
        "2. $3 \\times 3$ convolution preceded by $1 \\times 1$ convolution for dimensionality reduction  \n",
        "3. $5 \\times 5$ convolution preceded by $1 \\times 1$ convolution  \n",
        "4. $3 \\times 3$ max pooling followed by $1 \\times 1$ convolution  \n",
        "\n",
        "The outputs are concatenated along the channel dimension.\n",
        "\n",
        "If the input has $C$ channels, the $1 \\times 1$ convolution acts as a bottleneck layer to reduce dimensionality, helping control computational cost.\n",
        "\n",
        "## Dimensionality Reduction with $1 \\times 1$ Convolutions\n",
        "\n",
        "A $1 \\times 1$ convolution reduces depth while preserving spatial dimensions:\n",
        "\n",
        "For input volume size $H \\times W \\times C_{in}$, applying $N$ filters of size $1 \\times 1$ gives output $H \\times W \\times N$.\n",
        "\n",
        "This reduces parameters and introduces non-linearity when combined with ReLU:\n",
        "\n",
        "$$\n",
        "f(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "## GoogLeNet Architecture Details\n",
        "\n",
        "The architecture is structured as follows:\n",
        "\n",
        "### Input\n",
        "\n",
        "Input image size: $224 \\times 224 \\times 3$\n",
        "\n",
        "### Initial Layers\n",
        "\n",
        "1. $7 \\times 7$ convolution with stride 2, output: $112 \\times 112 \\times 64$  \n",
        "2. $3 \\times 3$ max pooling with stride 2  \n",
        "3. Two $3 \\times 3$ convolutions  \n",
        "4. $3 \\times 3$ max pooling\n",
        "\n",
        "### Inception Layers\n",
        "\n",
        "There are 9 Inception modules organized as:\n",
        "\n",
        "- 2 modules in Inception (3a, 3b)  \n",
        "- 2 modules in Inception (4a, 4b, 4c, 4d, 4e)  \n",
        "- 2 modules in Inception (5a, 5b)\n",
        "\n",
        "Each Inception layer increases depth while reducing spatial resolution via max pooling.\n",
        "\n",
        "### Auxiliary Classifiers\n",
        "\n",
        "To combat vanishing gradients and improve convergence, two auxiliary classifiers are added after Inception 4a and 4d. These are small networks with:\n",
        "\n",
        "- Average pooling  \n",
        "- $1 \\times 1$ convolution  \n",
        "- Fully connected layer  \n",
        "- Softmax\n",
        "\n",
        "They act as regularizers and are used only during training.\n",
        "\n",
        "### Final Layers\n",
        "\n",
        "1. Average pooling over spatial dimensions ($7 \\times 7$)  \n",
        "2. Dropout layer  \n",
        "3. Fully connected layer with 1000 outputs (for ImageNet)  \n",
        "4. Softmax activation\n",
        "\n",
        "## Softmax Formula\n",
        "\n",
        "The softmax function computes class probabilities:\n",
        "\n",
        "$$\n",
        "P(y = j | x) = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}}\n",
        "$$\n",
        "\n",
        "where $z_j$ is the output score for class $j$ and $K$ is the number of classes.\n",
        "\n",
        "## Parameter Efficiency\n",
        "\n",
        "By using $1 \\times 1$ convolutions before larger filters, GoogLeNet reduces parameter count. For example, a direct $5 \\times 5$ convolution is replaced with a $1 \\times 1$ convolution followed by $5 \\times 5$, significantly reducing parameters and computations.\n",
        "\n",
        "## Summary\n",
        "\n",
        "GoogLeNet introduces the Inception module to perform multi-scale processing in a single layer while maintaining computational efficiency. The use of $1 \\times 1$ convolutions enables deep and wide networks without excessive parameters. Auxiliary classifiers stabilize training and improve gradient flow, making GoogLeNet a milestone in CNN architecture design.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Residual Networks (ResNet) in Deep Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "As neural networks get deeper, they tend to suffer from vanishing gradients, making training difficult and resulting in higher training error even with additional layers. Residual Networks (ResNet) address this degradation problem by introducing shortcut connections that allow the model to learn residual functions instead of direct mappings.\n",
        "\n",
        "ResNet was introduced by Kaiming He et al. and won the ImageNet 2015 competition with significant performance improvements. It allows training of extremely deep networks with hundreds or even thousands of layers.\n",
        "\n",
        "## Core Idea: Learning Residuals\n",
        "\n",
        "Instead of directly learning a mapping $H(x)$, ResNet learns the residual function:\n",
        "\n",
        "$$\n",
        "F(x) = H(x) - x\n",
        "$$\n",
        "\n",
        "So the original function becomes:\n",
        "\n",
        "$$\n",
        "H(x) = F(x) + x\n",
        "$$\n",
        "\n",
        "This is implemented using a shortcut (skip) connection that adds the input $x$ directly to the output of a series of layers.\n",
        "\n",
        "## Residual Block\n",
        "\n",
        "A residual block typically includes two or three layers followed by a shortcut connection. For a simple two-layer block:\n",
        "\n",
        "Let the input be $x$ and the function represented by the two layers be $F(x, \\{W_i\\})$, where $W_i$ are the weights. The output of the residual block is:\n",
        "\n",
        "$$\n",
        "y = F(x, \\{W_i\\}) + x\n",
        "$$\n",
        "\n",
        "If dimensions of $F(x)$ and $x$ differ, a linear projection (e.g., using a $1 \\times 1$ convolution) may be applied:\n",
        "\n",
        "$$\n",
        "y = F(x, \\{W_i\\}) + W_s x\n",
        "$$\n",
        "\n",
        "where $W_s$ is the projection matrix.\n",
        "\n",
        "## Benefits of Residual Learning\n",
        "\n",
        "- **Eases optimization** by allowing identity mapping\n",
        "- **Improves gradient flow** in backpropagation\n",
        "- **Reduces training error** in very deep networks\n",
        "- **Allows deeper models** without degradation in performance\n",
        "\n",
        "## ResNet Architecture\n",
        "\n",
        "ResNet comes in multiple variants, such as ResNet-18, ResNet-34, ResNet-50, ResNet-101, and ResNet-152. The difference lies in the number of layers and whether they use basic or bottleneck residual blocks.\n",
        "\n",
        "### Basic Block\n",
        "\n",
        "Used in ResNet-18 and ResNet-34, with two convolutional layers and a skip connection:\n",
        "\n",
        "$$\n",
        "y = \\text{ReLU}(F(x, \\{W_1, W_2\\}) + x)\n",
        "$$\n",
        "\n",
        "### Bottleneck Block\n",
        "\n",
        "Used in ResNet-50 and deeper models. It includes three layers:\n",
        "\n",
        "1. $1 \\times 1$ convolution to reduce dimensions  \n",
        "2. $3 \\times 3$ convolution  \n",
        "3. $1 \\times 1$ convolution to restore dimensions\n",
        "\n",
        "The skip connection adds the original input to the output of these three layers.\n",
        "\n",
        "## Identity and Projection Shortcuts\n",
        "\n",
        "- **Identity shortcut** is used when input and output dimensions match  \n",
        "- **Projection shortcut** is used to match dimensions when they differ using a learnable transformation $W_s$\n",
        "\n",
        "## Forward and Backward Flow\n",
        "\n",
        "In backpropagation, the gradient of the loss $L$ with respect to input $x$ in a residual block is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\left( \\frac{\\partial F}{\\partial x} + 1 \\right)\n",
        "$$\n",
        "\n",
        "The term $+1$ improves gradient flow, reducing the risk of vanishing gradients.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "ResNet revolutionized deep learning by making it feasible to train very deep networks without performance degradation. The key innovation is the residual connection, enabling identity mappings and easier optimization. As a result, ResNet architectures are widely used in computer vision tasks and serve as the backbone for many state-of-the-art models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MobileNetV2 Architecture in Computer Vision\n",
        "\n",
        "## Introduction\n",
        "\n",
        "MobileNetV2 is an efficient convolutional neural network architecture designed for mobile and embedded vision applications. It builds on the success of MobileNetV1 by introducing **inverted residuals** and **linear bottlenecks**, which reduce computational cost while maintaining performance.\n",
        "\n",
        "## Key Innovations\n",
        "\n",
        "### Depthwise Separable Convolution\n",
        "\n",
        "Instead of using standard convolution, MobileNetV2 uses depthwise separable convolution to reduce computation.\n",
        "\n",
        "Given:\n",
        "- Input feature map of size $H \\times W \\times D_{in}$\n",
        "- Kernel size $K \\times K$\n",
        "- Output depth $D_{out}$\n",
        "\n",
        "Standard convolution requires:\n",
        "\n",
        "$$\n",
        "K \\times K \\times D_{in} \\times D_{out} \\times H \\times W\n",
        "$$\n",
        "\n",
        "Depthwise separable convolution factorizes it into:\n",
        "\n",
        "1. **Depthwise convolution**:\n",
        "   $K \\times K \\times D_{in} \\times H \\times W$\n",
        "2. **Pointwise convolution**:\n",
        "   $1 \\times 1 \\times D_{in} \\times D_{out} \\times H \\times W$\n",
        "\n",
        "This reduces computation by approximately:\n",
        "\n",
        "$$\n",
        "\\frac{1}{D_{out}} + \\frac{1}{K^2}\n",
        "$$\n",
        "\n",
        "### Linear Bottlenecks\n",
        "\n",
        "In traditional architectures, non-linearities like ReLU are used throughout. In MobileNetV2, after expanding the input to a higher-dimensional space, a linear layer (without non-linearity) projects it back to a lower-dimensional output to preserve information.\n",
        "\n",
        "### Inverted Residuals\n",
        "\n",
        "MobileNetV2 inverts the traditional residual structure. Instead of compressing and then expanding, it **expands** to a higher-dimensional space, processes features, then **compresses** back.\n",
        "\n",
        "Let:\n",
        "- Input be $x$\n",
        "- Expansion layer: $t \\cdot D_{in}$ where $t > 1$ is expansion factor\n",
        "- Output: $D_{out}$\n",
        "\n",
        "The block flow is:\n",
        "1. $1 \\times 1$ convolution: expand to $t \\cdot D_{in}$\n",
        "2. $3 \\times 3$ depthwise convolution\n",
        "3. $1 \\times 1$ linear projection to $D_{out}$\n",
        "\n",
        "The residual connection is used only when the input and output dimensions are the same.\n",
        "\n",
        "## Block Structure\n",
        "\n",
        "Each bottleneck residual block consists of:\n",
        "- Expansion layer: $1 \\times 1$ convolution with ReLU6\n",
        "- Depthwise convolution: $3 \\times 3$ with stride $s$\n",
        "- Projection layer: $1 \\times 1$ linear convolution\n",
        "\n",
        "If stride $s = 1$ and input/output dimensions match, the input is added to the output.\n",
        "\n",
        "## Network Structure\n",
        "\n",
        "The MobileNetV2 architecture includes:\n",
        "\n",
        "- Initial $3 \\times 3$ convolution layer with stride 2\n",
        "- Series of inverted residual blocks with varying expansion factors and strides\n",
        "- Final $1 \\times 1$ convolution\n",
        "- Global average pooling\n",
        "- Fully connected softmax layer\n",
        "\n",
        "## Output Shapes (Example)\n",
        "\n",
        "Input: $224 \\times 224 \\times 3$\n",
        "\n",
        "- After initial conv: $112 \\times 112 \\times 32$\n",
        "- After first block: $112 \\times 112 \\times 16$\n",
        "- Output after all blocks: $7 \\times 7 \\times 1280$\n",
        "- After global average pooling: $1 \\times 1 \\times 1280$\n",
        "- Final classification layer: $1 \\times 1 \\times 1000$ (for ImageNet)\n",
        "\n",
        "## Advantages\n",
        "\n",
        "- Highly efficient on mobile and edge devices\n",
        "- Lower number of parameters and FLOPs\n",
        "- Maintains high accuracy with fewer resources\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "MobileNetV2 optimizes network design for resource-constrained environments using depthwise separable convolutions, inverted residuals, and linear bottlenecks. These innovations allow it to achieve excellent trade-offs between speed and accuracy, making it a popular choice for mobile computer vision tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Recurrent Neural Networks (RNNs)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vanishing and Exploding Gradients in Deep Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "As neural networks grow deeper, training them becomes more difficult due to two major problems: vanishing and exploding gradients. These problems occur during backpropagation when gradients are propagated backward through the layers.\n",
        "\n",
        "## Gradient Flow in Backpropagation\n",
        "\n",
        "During training, weight updates depend on gradients computed via the chain rule:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial \\mathcal{L}}{\\partial a_n} \\cdot \\frac{\\partial a_n}{\\partial a_{n-1}} \\cdot \\dots \\cdot \\frac{\\partial a_1}{\\partial w}\n",
        "$$\n",
        "\n",
        "If each derivative $\\frac{\\partial a_i}{\\partial a_{i-1}}$ is small (or large), their product can become very small (vanishing) or very large (exploding).\n",
        "\n",
        "## Vanishing Gradient Problem\n",
        "\n",
        "When gradients become very small, weights update very slowly or not at all. This typically happens when using activation functions like sigmoid or tanh, whose derivatives are less than 1.\n",
        "\n",
        "Example for sigmoid:\n",
        "\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1 + e^{-x}}, \\quad \\sigma'(x) = \\sigma(x)(1 - \\sigma(x)) \\leq 0.25\n",
        "$$\n",
        "\n",
        "For a network with $n$ layers, the gradient magnitude shrinks as:\n",
        "\n",
        "$$\n",
        "\\left(\\prod_{i=1}^n \\sigma'(x_i)\\right) \\rightarrow 0 \\quad \\text{as } n \\rightarrow \\infty\n",
        "$$\n",
        "\n",
        "This leads to the network being unable to learn deep patterns, especially in earlier layers.\n",
        "\n",
        "## Exploding Gradient Problem\n",
        "\n",
        "When the derivatives are large (e.g., due to large weights or ReLU variants without clipping), gradients can grow exponentially:\n",
        "\n",
        "$$\n",
        "\\left(\\prod_{i=1}^n \\partial a_i / \\partial a_{i-1}\\right) \\rightarrow \\infty\n",
        "$$\n",
        "\n",
        "This causes large weight updates, instability, and possible overflow in numerical computations.\n",
        "\n",
        "## Effects on Training\n",
        "\n",
        "- **Vanishing gradients** cause the network to stop learning in earlier layers, leading to underfitting.\n",
        "- **Exploding gradients** lead to unstable weights and divergent loss, resulting in training failure.\n",
        "\n",
        "## Mitigation Strategies\n",
        "\n",
        "### Weight Initialization\n",
        "\n",
        "Proper initialization like Xavier or He initialization ensures variance of activations remains stable across layers.\n",
        "\n",
        "For tanh (Xavier):\n",
        "\n",
        "$$\n",
        "W \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}}\\right)\n",
        "$$\n",
        "\n",
        "For ReLU (He):\n",
        "\n",
        "$$\n",
        "W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in}}}\\right)\n",
        "$$\n",
        "\n",
        "### Activation Function Choice\n",
        "\n",
        "Using ReLU or variants (Leaky ReLU, ELU) helps maintain stronger gradients because ReLU derivative is either 0 or 1.\n",
        "\n",
        "### Gradient Clipping\n",
        "\n",
        "To prevent exploding gradients, clip gradients within a fixed range:\n",
        "\n",
        "$$\n",
        "g \\leftarrow \\frac{\\tau}{\\max(\\tau, \\|g\\|)} \\cdot g\n",
        "$$\n",
        "\n",
        "Where $\\tau$ is a threshold.\n",
        "\n",
        "### Batch Normalization\n",
        "\n",
        "By normalizing activations within each batch, batch normalization prevents them from growing or shrinking uncontrollably, stabilizing training.\n",
        "\n",
        "### Residual Connections\n",
        "\n",
        "In architectures like ResNet, residual connections help preserve gradients by providing shortcut paths that skip one or more layers:\n",
        "\n",
        "$$\n",
        "y = F(x) + x\n",
        "$$\n",
        "\n",
        "This helps gradients flow directly to earlier layers.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Vanishing and exploding gradients are critical issues in training deep networks. Using better initialization, activation functions, normalization techniques, and architectural improvements helps maintain stable gradient flow, enabling efficient training of deep neural networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feedforward Neural Networks vs Recurrent Neural Networks\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Neural networks can be broadly classified into two main types: Feedforward Neural Networks (FNNs) and Recurrent Neural Networks (RNNs). They differ in structure, data flow, and applicability to different types of tasks, particularly static vs sequential data.\n",
        "\n",
        "## Feedforward Neural Networks (FNNs)\n",
        "\n",
        "### Structure\n",
        "\n",
        "FNNs are acyclic networks where data moves only in one direction, from input to output through hidden layers. There is no feedback or looping mechanism.\n",
        "\n",
        "### Forward Propagation\n",
        "\n",
        "Given input $x$, the output at each layer is:\n",
        "\n",
        "$$\n",
        "a^{(l)} = f(W^{(l)} a^{(l-1)} + b^{(l)})\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $a^{(l)}$ is the activation of layer $l$\n",
        "- $W^{(l)}$ is the weight matrix\n",
        "- $b^{(l)}$ is the bias\n",
        "- $f$ is an activation function like ReLU or sigmoid\n",
        "\n",
        "### Application\n",
        "\n",
        "FNNs are best suited for tasks with fixed-size inputs and outputs, such as image classification, tabular data prediction, and regression.\n",
        "\n",
        "## Recurrent Neural Networks (RNNs)\n",
        "\n",
        "### Structure\n",
        "\n",
        "RNNs contain loops, allowing outputs from previous time steps to influence the current input processing. They are designed for sequential or time-dependent data.\n",
        "\n",
        "### Recurrence Equation\n",
        "\n",
        "For an input sequence $x = (x_1, x_2, ..., x_T)$, the hidden state $h_t$ at time $t$ is:\n",
        "\n",
        "$$\n",
        "h_t = f(W_h h_{t-1} + W_x x_t + b)\n",
        "$$\n",
        "\n",
        "The output $y_t$ is typically:\n",
        "\n",
        "$$\n",
        "y_t = g(W_y h_t + c)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $W_h$ is the hidden state weight\n",
        "- $W_x$ is the input weight\n",
        "- $W_y$ is the output weight\n",
        "- $b, c$ are biases\n",
        "- $f, g$ are activation functions\n",
        "\n",
        "### Application\n",
        "\n",
        "RNNs are ideal for tasks involving sequences, such as:\n",
        "- Language modeling\n",
        "- Time series forecasting\n",
        "- Speech recognition\n",
        "- Machine translation\n",
        "\n",
        "## Key Differences\n",
        "\n",
        "### Memory\n",
        "\n",
        "- FNNs do not retain information from previous inputs.\n",
        "- RNNs maintain a hidden state that acts as memory, allowing context retention across time steps.\n",
        "\n",
        "### Input and Output Shape\n",
        "\n",
        "- FNNs: fixed-length input and output\n",
        "- RNNs: flexible input and output (one-to-one, one-to-many, many-to-one, many-to-many)\n",
        "\n",
        "### Training Complexity\n",
        "\n",
        "- FNNs are simpler to train and less prone to gradient issues.\n",
        "- RNNs require Backpropagation Through Time (BPTT), which can lead to vanishing or exploding gradients over long sequences.\n",
        "\n",
        "### Computational Cost\n",
        "\n",
        "- FNNs process inputs in parallel.\n",
        "- RNNs process inputs sequentially, which increases computation time and limits parallelism.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Feedforward Neural Networks are suitable for non-sequential problems with static inputs, while Recurrent Neural Networks are essential for modeling time-dependent or sequential data. Understanding the differences allows selecting the right architecture for a given machine learning task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Backpropagation Through Time (BPTT)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Backpropagation Through Time (BPTT) is the extension of standard backpropagation used to train Recurrent Neural Networks (RNNs). Since RNNs deal with sequential data and maintain a hidden state, their gradients must be propagated through time steps, not just layers.\n",
        "\n",
        "## Unrolling the RNN\n",
        "\n",
        "For a given sequence $x = (x_1, x_2, ..., x_T)$, the RNN is unrolled into $T$ copies of itself, one per time step. This transforms the temporal problem into a feedforward-like structure across time.\n",
        "\n",
        "## RNN Forward Pass\n",
        "\n",
        "The hidden state at time $t$ is:\n",
        "\n",
        "$$\n",
        "h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b_h)\n",
        "$$\n",
        "\n",
        "The output at time $t$ is:\n",
        "\n",
        "$$\n",
        "y_t = g(W_{hy} h_t + b_y)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $W_{hh}$ is the recurrent weight\n",
        "- $W_{xh}$ is the input weight\n",
        "- $W_{hy}$ is the output weight\n",
        "- $f$, $g$ are activation functions\n",
        "\n",
        "## Loss Function\n",
        "\n",
        "Assuming the total loss over the sequence is:\n",
        "\n",
        "$$\n",
        "L = \\sum_{t=1}^{T} L_t\n",
        "$$\n",
        "\n",
        "Each $L_t$ is the loss at time step $t$, such as cross-entropy or mean squared error.\n",
        "\n",
        "## Gradient Computation\n",
        "\n",
        "To update the weights, we compute gradients of $L$ with respect to the parameters. For example, the gradient of loss with respect to the hidden state $h_t$ involves contributions from both the current time step and future time steps due to the recurrence:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial h_t} = \\sum_{k=t}^{T} \\frac{\\partial L_k}{\\partial h_t}\n",
        "$$\n",
        "\n",
        "The key idea is to recursively compute gradients from the last time step back to the first:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W_{hh}} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_{hh}}\n",
        "$$\n",
        "\n",
        "The partial derivative of $h_t$ with respect to $W_{hh}$ depends on previous hidden states and propagates backward through time.\n",
        "\n",
        "## Challenges in BPTT\n",
        "\n",
        "### Vanishing Gradient\n",
        "\n",
        "As the gradient is propagated through many time steps, it may diminish exponentially:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial h_t}{\\partial h_{t-k}} \\propto (W_{hh})^k\n",
        "$$\n",
        "\n",
        "If $W_{hh}$ has small eigenvalues, gradients shrink rapidly.\n",
        "\n",
        "### Exploding Gradient\n",
        "\n",
        "If $W_{hh}$ has large eigenvalues, gradients grow uncontrollably.\n",
        "\n",
        "## Truncated BPTT\n",
        "\n",
        "To avoid computational burden and instability, BPTT is often truncated. Instead of backpropagating through the entire sequence, we limit it to a window of $k$ steps. This reduces time complexity and mitigates gradient instability.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Backpropagation Through Time enables learning in RNNs by unfolding them across time and applying gradient descent. It captures temporal dependencies, but suffers from vanishing and exploding gradients over long sequences. Techniques like truncated BPTT and gradient clipping are used to improve training stability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Types of Recurrent Neural Networks (RNNs)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Recurrent Neural Networks (RNNs) are specialized neural networks for processing sequential data by maintaining a hidden state that captures information from previous inputs. Different RNN architectures improve on the basic RNN to handle challenges like vanishing gradients and long-term dependencies.\n",
        "\n",
        "## Basic RNN\n",
        "\n",
        "The basic RNN computes hidden states recursively over time as:\n",
        "\n",
        "$$\n",
        "h_t = \\phi(W_{xh} x_t + W_{hh} h_{t-1} + b_h)\n",
        "$$\n",
        "\n",
        "The output at time $t$ is:\n",
        "\n",
        "$$\n",
        "y_t = \\psi(W_{hy} h_t + b_y)\n",
        "$$\n",
        "\n",
        "Where $\\phi$ and $\\psi$ are activation functions, commonly $\\tanh$ or ReLU for $\\phi$, and softmax or linear for $\\psi$.\n",
        "\n",
        "### Limitation\n",
        "\n",
        "Basic RNNs struggle with learning long-term dependencies due to vanishing or exploding gradients during backpropagation through time.\n",
        "\n",
        "## Long Short-Term Memory (LSTM)\n",
        "\n",
        "LSTMs introduce gated units to control information flow and mitigate the vanishing gradient problem.\n",
        "\n",
        "The LSTM cell includes:\n",
        "\n",
        "- Forget gate:\n",
        "\n",
        "$$\n",
        "f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
        "$$\n",
        "\n",
        "- Input gate:\n",
        "\n",
        "$$\n",
        "i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
        "$$\n",
        "\n",
        "- Candidate cell state:\n",
        "\n",
        "$$\n",
        "\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
        "$$\n",
        "\n",
        "- Cell state update:\n",
        "\n",
        "$$\n",
        "C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t\n",
        "$$\n",
        "\n",
        "- Output gate:\n",
        "\n",
        "$$\n",
        "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
        "$$\n",
        "\n",
        "- Hidden state:\n",
        "\n",
        "$$\n",
        "h_t = o_t \\odot \\tanh(C_t)\n",
        "$$\n",
        "\n",
        "Here, $\\sigma$ is the sigmoid activation, $\\odot$ denotes element-wise multiplication.\n",
        "\n",
        "## Gated Recurrent Unit (GRU)\n",
        "\n",
        "GRUs simplify LSTM by combining forget and input gates into a single update gate.\n",
        "\n",
        "- Update gate:\n",
        "\n",
        "$$\n",
        "z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
        "$$\n",
        "\n",
        "- Reset gate:\n",
        "\n",
        "$$\n",
        "r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
        "$$\n",
        "\n",
        "- Candidate hidden state:\n",
        "\n",
        "$$\n",
        "\\tilde{h}_t = \\tanh(W \\cdot [r_t \\odot h_{t-1}, x_t] + b)\n",
        "$$\n",
        "\n",
        "- Final hidden state:\n",
        "\n",
        "$$\n",
        "h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
        "$$\n",
        "\n",
        "GRUs offer comparable performance to LSTMs with fewer parameters and simpler structure.\n",
        "\n",
        "## Bidirectional RNN (BiRNN)\n",
        "\n",
        "Bidirectional RNNs process sequences in both forward and backward directions to capture past and future context.\n",
        "\n",
        "Hidden states are computed as:\n",
        "\n",
        "$$\n",
        "\\overrightarrow{h_t} = \\phi(W_{xh} x_t + W_{hh} \\overrightarrow{h}_{t-1} + b_h)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\overleftarrow{h_t} = \\phi(W_{xh} x_t + W_{hh} \\overleftarrow{h}_{t+1} + b_h)\n",
        "$$\n",
        "\n",
        "The output combines both directions:\n",
        "\n",
        "$$\n",
        "y_t = \\psi(W_{hy} [\\overrightarrow{h_t}; \\overleftarrow{h_t}] + b_y)\n",
        "$$\n",
        "\n",
        "Where $[\\cdot ; \\cdot]$ denotes concatenation.\n",
        "\n",
        "## Deep RNNs\n",
        "\n",
        "Deep RNNs stack multiple RNN layers, where the output of one layer serves as input to the next, enabling learning of hierarchical temporal features.\n",
        "\n",
        "For layer $l$:\n",
        "\n",
        "$$\n",
        "h_t^{(l)} = \\phi(W_{xh}^{(l)} h_t^{(l-1)} + W_{hh}^{(l)} h_{t-1}^{(l)} + b_h^{(l)})\n",
        "$$\n",
        "\n",
        "With $h_t^{(0)} = x_t$.\n",
        "\n",
        "## Summary\n",
        "\n",
        "RNN variants like LSTM, GRU, Bidirectional, and Deep RNNs improve the ability to learn complex temporal patterns by addressing vanishing gradients, capturing bidirectional context, and increasing representational capacity. Their choice depends on the task complexity, sequence length, and computational constraints.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bidirectional Recurrent Neural Networks (BiRNN)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Bidirectional Recurrent Neural Networks (BiRNN) extend standard RNNs by processing data sequences in both forward and backward directions. This structure allows the model to have access to past (previous timesteps) and future (upcoming timesteps) context simultaneously, improving performance on sequence tasks where full context is beneficial.\n",
        "\n",
        "## Standard RNN Recap\n",
        "\n",
        "In a typical RNN, the hidden state at time $t$ is computed based on the current input $x_t$ and the previous hidden state $h_{t-1}$:\n",
        "\n",
        "$$\n",
        "h_t = \\phi(W_{xh} x_t + W_{hh} h_{t-1} + b_h)\n",
        "$$\n",
        "\n",
        "The output at time $t$ is:\n",
        "\n",
        "$$\n",
        "y_t = \\psi(W_{hy} h_t + b_y)\n",
        "$$\n",
        "\n",
        "Here, $\\phi$ and $\\psi$ denote activation functions, commonly $\\tanh$ or ReLU for $\\phi$, and softmax or linear for $\\psi$.\n",
        "\n",
        "## BiRNN Architecture\n",
        "\n",
        "BiRNNs employ two separate hidden layers: one processes the sequence forward in time, the other processes it backward.\n",
        "\n",
        "- Forward hidden state:\n",
        "\n",
        "$$\n",
        "\\overrightarrow{h_t} = \\phi(W_{xh}^{\\rightarrow} x_t + W_{hh}^{\\rightarrow} \\overrightarrow{h}_{t-1} + b_h^{\\rightarrow})\n",
        "$$\n",
        "\n",
        "- Backward hidden state:\n",
        "\n",
        "$$\n",
        "\\overleftarrow{h_t} = \\phi(W_{xh}^{\\leftarrow} x_t + W_{hh}^{\\leftarrow} \\overleftarrow{h}_{t+1} + b_h^{\\leftarrow})\n",
        "$$\n",
        "\n",
        "The final output $y_t$ combines both hidden states, typically by concatenation or addition:\n",
        "\n",
        "$$\n",
        "y_t = \\psi(W_{hy} [\\overrightarrow{h_t}; \\overleftarrow{h_t}] + b_y)\n",
        "$$\n",
        "\n",
        "Here, $[\\cdot;\\cdot]$ denotes concatenation.\n",
        "\n",
        "## Advantages of BiRNN\n",
        "\n",
        "By having access to both past and future information at every timestep, BiRNNs are more effective in tasks such as speech recognition, natural language processing, and time series prediction where context from both directions improves accuracy.\n",
        "\n",
        "## Training Considerations\n",
        "\n",
        "Training BiRNNs uses Backpropagation Through Time (BPTT) in both forward and backward passes. This requires storing the full sequence during training and may increase computational complexity compared to unidirectional RNNs.\n",
        "\n",
        "## Summary\n",
        "\n",
        "BiRNNs enhance traditional RNNs by processing sequences bidirectionally, combining forward and backward hidden states to leverage full temporal context. This leads to improved modeling of sequential data where both past and future inputs contribute to the output at each timestep.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bidirectional Long Short-Term Memory (BiLSTM) in NLP\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Bidirectional Long Short-Term Memory (BiLSTM) networks improve upon traditional LSTMs by processing sequences in both forward and backward directions. This enables the model to capture past and future context simultaneously, which is especially valuable in Natural Language Processing (NLP) tasks where understanding both previous and subsequent words enhances meaning extraction.\n",
        "\n",
        "## Recap of LSTM\n",
        "\n",
        "An LSTM unit addresses the vanishing gradient problem in RNNs by maintaining a memory cell $c_t$ controlled by input, forget, and output gates. The LSTM cell at time $t$ is defined as follows:\n",
        "\n",
        "Input gate:\n",
        "\n",
        "$$\n",
        "i_t = \\sigma(W_{xi} x_t + W_{hi} h_{t-1} + b_i)\n",
        "$$\n",
        "\n",
        "Forget gate:\n",
        "\n",
        "$$\n",
        "f_t = \\sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f)\n",
        "$$\n",
        "\n",
        "Output gate:\n",
        "\n",
        "$$\n",
        "o_t = \\sigma(W_{xo} x_t + W_{ho} h_{t-1} + b_o)\n",
        "$$\n",
        "\n",
        "Candidate cell state:\n",
        "\n",
        "$$\n",
        "\\tilde{c}_t = \\tanh(W_{xc} x_t + W_{hc} h_{t-1} + b_c)\n",
        "$$\n",
        "\n",
        "Cell state update:\n",
        "\n",
        "$$\n",
        "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
        "$$\n",
        "\n",
        "Hidden state:\n",
        "\n",
        "$$\n",
        "h_t = o_t \\odot \\tanh(c_t)\n",
        "$$\n",
        "\n",
        "Here, $\\sigma$ is the sigmoid activation, $\\tanh$ is the hyperbolic tangent, and $\\odot$ represents element-wise multiplication.\n",
        "\n",
        "## BiLSTM Architecture\n",
        "\n",
        "BiLSTM consists of two LSTM layers:\n",
        "\n",
        "- The forward LSTM processes the sequence from $t=1$ to $t=T$ producing forward hidden states $\\overrightarrow{h_t}$.\n",
        "  \n",
        "- The backward LSTM processes the sequence from $t=T$ to $t=1$ producing backward hidden states $\\overleftarrow{h_t}$.\n",
        "\n",
        "The combined output at time $t$ is formed by concatenating the forward and backward hidden states:\n",
        "\n",
        "$$\n",
        "h_t = [\\overrightarrow{h_t}; \\overleftarrow{h_t}]\n",
        "$$\n",
        "\n",
        "This combined hidden representation $h_t$ is then used for further processing, such as classification or sequence tagging.\n",
        "\n",
        "## Benefits in NLP\n",
        "\n",
        "Language understanding depends heavily on context, where both preceding and succeeding words influence meaning. BiLSTMs exploit this by incorporating information from both directions, improving tasks like:\n",
        "\n",
        "- Part-of-speech tagging\n",
        "- Named entity recognition\n",
        "- Machine translation\n",
        "- Sentiment analysis\n",
        "\n",
        "## Training\n",
        "\n",
        "Training BiLSTMs uses Backpropagation Through Time (BPTT) in both forward and backward passes. The model learns parameters for both directions simultaneously, optimizing to minimize loss over the combined hidden states.\n",
        "\n",
        "## Summary\n",
        "\n",
        "BiLSTM networks extend LSTMs by adding backward processing of input sequences, enabling richer context capture for each timestep. This dual-directional approach makes BiLSTMs highly effective for complex NLP tasks requiring full-sequence understanding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gated Recurrent Unit (GRU) Networks\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Gated Recurrent Units (GRUs) are a type of recurrent neural network (RNN) architecture designed to capture temporal dependencies in sequence data while addressing the vanishing gradient problem. GRUs simplify the Long Short-Term Memory (LSTM) structure by combining the forget and input gates into a single update gate, resulting in fewer parameters and faster training without significantly sacrificing performance.\n",
        "\n",
        "## GRU Architecture\n",
        "\n",
        "A GRU cell uses two gates: the update gate $z_t$ and the reset gate $r_t$. These gates control the flow of information through the hidden state to retain long-term dependencies and decide what information to discard or update.\n",
        "\n",
        "### Update Gate\n",
        "\n",
        "The update gate $z_t$ determines how much of the previous hidden state $h_{t-1}$ should be carried forward to the current hidden state $h_t$:\n",
        "\n",
        "$$\n",
        "z_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z)\n",
        "$$\n",
        "\n",
        "### Reset Gate\n",
        "\n",
        "The reset gate $r_t$ decides how much of the past information to forget:\n",
        "\n",
        "$$\n",
        "r_t = \\sigma(W_r x_t + U_r h_{t-1} + b_r)\n",
        "$$\n",
        "\n",
        "### Candidate Activation\n",
        "\n",
        "The candidate hidden state $\\tilde{h}_t$ combines the current input $x_t$ and the reset-modified previous hidden state:\n",
        "\n",
        "$$\n",
        "\\tilde{h}_t = \\tanh(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h)\n",
        "$$\n",
        "\n",
        "### Final Hidden State\n",
        "\n",
        "The new hidden state $h_t$ is a linear interpolation between the previous hidden state and the candidate hidden state, controlled by the update gate:\n",
        "\n",
        "$$\n",
        "h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
        "$$\n",
        "\n",
        "Here, $\\sigma$ denotes the sigmoid function, $\\tanh$ is the hyperbolic tangent function, and $\\odot$ represents element-wise multiplication.\n",
        "\n",
        "## Advantages of GRU\n",
        "\n",
        "- **Fewer parameters:** Compared to LSTMs, GRUs have a simpler structure with fewer gates, which leads to faster training and less computational cost.\n",
        "  \n",
        "- **Effective learning:** GRUs can capture long-term dependencies efficiently and often perform on par with LSTMs in many tasks.\n",
        "  \n",
        "- **Less prone to vanishing gradients:** The gating mechanisms enable gradients to flow effectively through time steps.\n",
        "\n",
        "## Applications\n",
        "\n",
        "GRUs are widely used in sequence modeling tasks such as:\n",
        "\n",
        "- Language modeling\n",
        "- Speech recognition\n",
        "- Machine translation\n",
        "- Time series forecasting\n",
        "\n",
        "## Summary\n",
        "\n",
        "GRU networks provide a streamlined alternative to LSTMs, utilizing two gates—update and reset—to manage the memory and forget mechanisms. Their simpler architecture facilitates efficient learning of temporal dependencies in sequential data with fewer computational resources.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Generative Models in Deep Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generative Adversarial Network (GAN)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Generative Adversarial Networks (GANs) are a class of deep learning models used for generative tasks. Introduced by Ian Goodfellow in 2014, GANs consist of two neural networks—the Generator and the Discriminator—that compete in a game-theoretic framework. This adversarial process enables the Generator to produce data samples indistinguishable from real data.\n",
        "\n",
        "## GAN Architecture\n",
        "\n",
        "GAN comprises two components:\n",
        "\n",
        "### Generator ($G$)\n",
        "\n",
        "The Generator creates synthetic data samples from random noise input $z$, aiming to mimic the real data distribution $p_{\\text{data}}$.\n",
        "\n",
        "$$\n",
        "G: z \\rightarrow x_{\\text{fake}}\n",
        "$$\n",
        "\n",
        "where $z$ is drawn from a prior distribution $p_z(z)$, commonly uniform or Gaussian noise.\n",
        "\n",
        "### Discriminator ($D$)\n",
        "\n",
        "The Discriminator is a binary classifier that receives either real data samples $x$ or fake samples $x_{\\text{fake}}$ from the Generator. It outputs a probability indicating whether the input is real or generated.\n",
        "\n",
        "$$\n",
        "D: x \\rightarrow [0,1]\n",
        "$$\n",
        "\n",
        "## Objective Function\n",
        "\n",
        "GANs are trained through a minimax game where the Generator tries to fool the Discriminator, and the Discriminator tries to correctly distinguish real from fake data. The value function $V(G, D)$ is:\n",
        "\n",
        "$$\n",
        "\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z} [\\log (1 - D(G(z)))]\n",
        "$$\n",
        "\n",
        "- The Discriminator maximizes the probability of correctly classifying real and fake samples.\n",
        "- The Generator minimizes $\\log(1 - D(G(z)))$, trying to maximize the Discriminator's error.\n",
        "\n",
        "## Training Process\n",
        "\n",
        "Training involves alternating optimization steps:\n",
        "\n",
        "1. Update $D$ to maximize the ability to distinguish real vs. fake samples.\n",
        "2. Update $G$ to minimize the Discriminator’s success by producing more realistic samples.\n",
        "\n",
        "Convergence is reached when $G$ generates data indistinguishable from real samples, and $D$ outputs 0.5 for all inputs.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- **Adversarial loss:** The opposing objectives of $G$ and $D$ drive mutual improvement.\n",
        "- **Mode collapse:** A challenge where $G$ produces limited diversity; techniques like feature matching help mitigate this.\n",
        "- **Jensen-Shannon divergence:** The GAN training objective implicitly minimizes this divergence between real and generated data distributions.\n",
        "\n",
        "## Summary\n",
        "\n",
        "GANs leverage adversarial training between a Generator and a Discriminator to learn complex data distributions and generate realistic synthetic samples. Their powerful framework has applications in image synthesis, data augmentation, style transfer, and more.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Autoencoders\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Autoencoders are unsupervised neural networks designed for learning efficient data encodings. They aim to compress input data into a lower-dimensional representation (encoding) and then reconstruct the original input from this representation (decoding). This process helps in tasks such as dimensionality reduction, denoising, and anomaly detection.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "An autoencoder consists of two main parts:\n",
        "\n",
        "### Encoder\n",
        "\n",
        "The encoder maps the input data $x \\in \\mathbb{R}^n$ to a latent representation $h \\in \\mathbb{R}^m$ where $m < n$:\n",
        "\n",
        "$$\n",
        "h = f_{\\theta}(x)\n",
        "$$\n",
        "\n",
        "Here, $f_{\\theta}$ is typically a neural network parameterized by weights $\\theta$.\n",
        "\n",
        "### Decoder\n",
        "\n",
        "The decoder reconstructs the input from the latent code $h$:\n",
        "\n",
        "$$\n",
        "\\hat{x} = g_{\\phi}(h)\n",
        "$$\n",
        "\n",
        "where $g_{\\phi}$ is another neural network parameterized by $\\phi$ producing the reconstruction $\\hat{x}$.\n",
        "\n",
        "## Objective Function\n",
        "\n",
        "The goal is to minimize the reconstruction error between the input $x$ and its reconstruction $\\hat{x}$. The most common loss function is the Mean Squared Error (MSE):\n",
        "\n",
        "$$\n",
        "L(x, \\hat{x}) = \\| x - \\hat{x} \\|^2 = \\sum_{i=1}^n (x_i - \\hat{x}_i)^2\n",
        "$$\n",
        "\n",
        "The network parameters $(\\theta, \\phi)$ are optimized to minimize $L$ over the training data.\n",
        "\n",
        "## Variants of Autoencoders\n",
        "\n",
        "- **Undercomplete Autoencoder:** The latent space dimension $m$ is smaller than the input dimension $n$, enforcing a compressed representation.\n",
        "  \n",
        "- **Denoising Autoencoder:** Trained to reconstruct clean inputs from corrupted versions, improving robustness.\n",
        "\n",
        "- **Sparse Autoencoder:** Adds a sparsity constraint on $h$ to encourage representations with few active neurons.\n",
        "\n",
        "- **Variational Autoencoder (VAE):** Learns a probabilistic latent space by approximating the data distribution with a latent variable model.\n",
        "\n",
        "## Training Process\n",
        "\n",
        "1. Pass input $x$ through encoder to get $h = f_{\\theta}(x)$.\n",
        "2. Reconstruct $\\hat{x} = g_{\\phi}(h)$ via decoder.\n",
        "3. Compute loss $L(x, \\hat{x})$.\n",
        "4. Backpropagate gradients and update parameters $(\\theta, \\phi)$ to minimize reconstruction error.\n",
        "\n",
        "## Applications\n",
        "\n",
        "Autoencoders are widely used for feature extraction, dimensionality reduction, image denoising, anomaly detection, and data generation (with variants like VAEs).\n",
        "\n",
        "## Summary\n",
        "\n",
        "Autoencoders learn compressed representations of data by training a neural network to reconstruct the input. Minimizing reconstruction loss enables discovering meaningful latent features useful for various machine learning tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Restricted Boltzmann Machine (RBM)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "A Restricted Boltzmann Machine (RBM) is a generative stochastic neural network that can learn a probability distribution over its input data. RBMs are composed of two layers: a visible layer representing the observed data and a hidden layer that captures dependencies between visible units. The \"restricted\" part means there are no connections between nodes within the same layer, only between visible and hidden layers.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "RBM consists of:\n",
        "\n",
        "- **Visible units** $v = (v_1, v_2, \\dots, v_n)$ representing the input data.\n",
        "- **Hidden units** $h = (h_1, h_2, \\dots, h_m)$ capturing latent features.\n",
        "\n",
        "Each visible unit is connected to every hidden unit, but no visible-visible or hidden-hidden connections exist.\n",
        "\n",
        "## Energy Function\n",
        "\n",
        "RBM defines an energy function $E(v,h)$ that measures the compatibility between visible vector $v$ and hidden vector $h$:\n",
        "\n",
        "$$\n",
        "E(v,h) = - \\sum_{i=1}^n \\sum_{j=1}^m v_i w_{ij} h_j - \\sum_{i=1}^n b_i v_i - \\sum_{j=1}^m c_j h_j\n",
        "$$\n",
        "\n",
        "Here:\n",
        "\n",
        "- $w_{ij}$ is the weight between visible unit $i$ and hidden unit $j$,\n",
        "- $b_i$ and $c_j$ are biases for visible and hidden units respectively.\n",
        "\n",
        "## Probability Distribution\n",
        "\n",
        "The joint probability distribution over $(v,h)$ is given by the Boltzmann distribution:\n",
        "\n",
        "$$\n",
        "P(v,h) = \\frac{1}{Z} e^{-E(v,h)}\n",
        "$$\n",
        "\n",
        "where the partition function $Z$ normalizes the distribution:\n",
        "\n",
        "$$\n",
        "Z = \\sum_{v,h} e^{-E(v,h)}\n",
        "$$\n",
        "\n",
        "The marginal probability of visible vector $v$ is:\n",
        "\n",
        "$$\n",
        "P(v) = \\frac{1}{Z} \\sum_h e^{-E(v,h)}\n",
        "$$\n",
        "\n",
        "## Activation Probabilities\n",
        "\n",
        "Because of the bipartite structure, the hidden units are conditionally independent given visible units, and vice versa.\n",
        "\n",
        "The probability that hidden unit $h_j$ is activated given $v$ is:\n",
        "\n",
        "$$\n",
        "P(h_j = 1 | v) = \\sigma \\left( \\sum_{i=1}^n w_{ij} v_i + c_j \\right)\n",
        "$$\n",
        "\n",
        "Similarly, the probability that visible unit $v_i$ is activated given $h$ is:\n",
        "\n",
        "$$\n",
        "P(v_i = 1 | h) = \\sigma \\left( \\sum_{j=1}^m w_{ij} h_j + b_i \\right)\n",
        "$$\n",
        "\n",
        "Here, $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid function.\n",
        "\n",
        "## Training Objective\n",
        "\n",
        "The goal is to maximize the likelihood of the training data, equivalently minimizing the negative log-likelihood:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = - \\sum_v \\log P(v)\n",
        "$$\n",
        "\n",
        "The gradient of the log-likelihood with respect to the weights is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\log P(v)}{\\partial w_{ij}} = \\langle v_i h_j \\rangle_{\\text{data}} - \\langle v_i h_j \\rangle_{\\text{model}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $\\langle \\cdot \\rangle_{\\text{data}}$ denotes expectation over the data distribution,\n",
        "- $\\langle \\cdot \\rangle_{\\text{model}}$ denotes expectation over the model's distribution.\n",
        "\n",
        "## Contrastive Divergence Algorithm\n",
        "\n",
        "Computing $\\langle \\cdot \\rangle_{\\text{model}}$ is intractable due to $Z$. Contrastive Divergence (CD) approximates this by:\n",
        "\n",
        "1. Sampling $h$ given observed $v$.\n",
        "2. Reconstructing $\\tilde{v}$ given $h$.\n",
        "3. Resampling $\\tilde{h}$ given $\\tilde{v}$.\n",
        "4. Updating weights using:\n",
        "\n",
        "$$\n",
        "\\Delta w_{ij} = \\eta \\left( v_i h_j - \\tilde{v}_i \\tilde{h}_j \\right)\n",
        "$$\n",
        "\n",
        "where $\\eta$ is the learning rate.\n",
        "\n",
        "## Summary\n",
        "\n",
        "RBMs model complex data distributions via a bipartite network with stochastic binary units. Training involves adjusting weights to minimize energy for data configurations while maximizing it for reconstructed samples, enabling feature learning useful in pretraining deep networks and dimensionality reduction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Convolutional Generative Adversarial Networks (DCGAN)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Deep Convolutional Generative Adversarial Networks (DCGANs) combine the power of Convolutional Neural Networks (CNNs) with Generative Adversarial Networks (GANs) to generate high-quality images. DCGANs leverage convolutional layers for both the generator and discriminator, enabling better feature extraction and generation compared to fully connected GANs.\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "DCGAN consists of two competing neural networks:\n",
        "\n",
        "- **Generator ($G$)**: Generates fake images from random noise vectors.\n",
        "- **Discriminator ($D$)**: Classifies images as real or fake.\n",
        "\n",
        "The two networks play a minimax game defined by the value function:\n",
        "\n",
        "$$\n",
        "\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z} [\\log (1 - D(G(z)))]\n",
        "$$\n",
        "\n",
        "Here:\n",
        "\n",
        "- $x$ represents real images sampled from data distribution $p_{\\text{data}}$.\n",
        "- $z$ represents noise vectors sampled from noise prior $p_z$ (often uniform or Gaussian).\n",
        "- $D(x)$ outputs the probability that $x$ is real.\n",
        "\n",
        "## Generator Network\n",
        "\n",
        "The generator learns a mapping from the latent noise space $z \\in \\mathbb{R}^d$ to the image space. It uses transposed convolutional layers (also known as deconvolutions) to upsample the noise vector into an image.\n",
        "\n",
        "The generator architecture typically follows:\n",
        "\n",
        "1. Dense layer reshaping $z$ into a low-resolution feature map.\n",
        "2. Several transposed convolutional layers that progressively increase spatial resolution.\n",
        "3. Batch Normalization to stabilize training.\n",
        "4. Activation functions, usually ReLU for intermediate layers and Tanh for the output to normalize pixel values between $-1$ and $1$.\n",
        "\n",
        "Mathematically, generator output can be expressed as:\n",
        "\n",
        "$$\n",
        "G(z; \\theta_g) = \\text{Tanh}(\\text{ConvTranspose}(...(\\text{ReLU}(\\text{BatchNorm}(\\text{ConvTranspose}(z)))))\n",
        "$$\n",
        "\n",
        "where $\\theta_g$ are the generator parameters.\n",
        "\n",
        "## Discriminator Network\n",
        "\n",
        "The discriminator is a CNN that outputs a scalar probability indicating if the input image is real or fake. It consists of:\n",
        "\n",
        "1. Convolutional layers that downsample the input image.\n",
        "2. Leaky ReLU activations to allow small gradients for negative inputs.\n",
        "3. Batch Normalization (except for the first layer) to stabilize training.\n",
        "4. A final sigmoid activation to produce a probability.\n",
        "\n",
        "Discriminator function:\n",
        "\n",
        "$$\n",
        "D(x; \\theta_d) = \\sigma(f(x; \\theta_d))\n",
        "$$\n",
        "\n",
        "where $f$ represents the convolutional feature extraction, $\\sigma$ is the sigmoid function, and $\\theta_d$ are discriminator parameters.\n",
        "\n",
        "## Training Objective\n",
        "\n",
        "The discriminator aims to maximize the likelihood of correctly classifying real and fake images:\n",
        "\n",
        "$$\n",
        "\\max_{\\theta_d} \\mathbb{E}_{x \\sim p_{\\text{data}}} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z} [\\log (1 - D(G(z)))]\n",
        "$$\n",
        "\n",
        "The generator tries to fool the discriminator by minimizing:\n",
        "\n",
        "$$\n",
        "\\min_{\\theta_g} \\mathbb{E}_{z \\sim p_z} [\\log (1 - D(G(z)))]\n",
        "$$\n",
        "\n",
        "Alternatively, to avoid vanishing gradients, the generator can maximize:\n",
        "\n",
        "$$\n",
        "\\max_{\\theta_g} \\mathbb{E}_{z \\sim p_z} [\\log D(G(z))]\n",
        "$$\n",
        "\n",
        "## Loss Functions\n",
        "\n",
        "- **Discriminator loss**:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_D = - \\mathbb{E}_{x \\sim p_{\\text{data}}} [\\log D(x)] - \\mathbb{E}_{z \\sim p_z} [\\log (1 - D(G(z)))]\n",
        "$$\n",
        "\n",
        "- **Generator loss**:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_G = - \\mathbb{E}_{z \\sim p_z} [\\log D(G(z))]\n",
        "$$\n",
        "\n",
        "## Key Techniques for Stability\n",
        "\n",
        "- Use of **Batch Normalization** in both networks to reduce internal covariate shift.\n",
        "- Leaky ReLU activations in discriminator to avoid dying ReLUs.\n",
        "- Removing fully connected layers to improve training stability.\n",
        "- Using the **Tanh** activation in the generator output layer to scale images in $[-1, 1]$.\n",
        "- Careful initialization of weights to zero-centered normal distribution.\n",
        "\n",
        "## Summary\n",
        "\n",
        "DCGANs improve image generation quality by integrating convolutional architectures within the GAN framework. The generator learns to create realistic images by upsampling noise through transposed convolutions, while the discriminator uses convolutional filters to detect fake images. The adversarial training objective aligns their goals, allowing DCGANs to generate complex, high-dimensional data distributions effectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conditional Generative Adversarial Networks (cGAN)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Conditional Generative Adversarial Networks (cGANs) extend the original GAN framework by conditioning both the generator and discriminator on auxiliary information, such as class labels or data from other modalities. This conditioning guides the generation process, enabling the creation of targeted or controlled outputs.\n",
        "\n",
        "## Core Idea\n",
        "\n",
        "In a standard GAN, the generator creates samples from random noise $z$ alone. In a cGAN, the generator and discriminator receive an additional input $y$ representing the condition (e.g., class label, attributes):\n",
        "\n",
        "- Generator input: $(z, y)$\n",
        "- Discriminator input: $(x, y)$, where $x$ is a real or generated sample.\n",
        "\n",
        "This conditioning allows the model to learn a conditional distribution $p(x|y)$ rather than the unconditional $p(x)$.\n",
        "\n",
        "## Objective Function\n",
        "\n",
        "The value function for cGAN modifies the original GAN objective to incorporate conditioning:\n",
        "\n",
        "$$\n",
        "\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D(x|y)] + \\mathbb{E}_{z \\sim p_z}[\\log (1 - D(G(z|y)|y))]\n",
        "$$\n",
        "\n",
        "Here,\n",
        "\n",
        "- $D(x|y)$ denotes the discriminator's probability that $x$ is real given condition $y$.\n",
        "- $G(z|y)$ generates a sample conditioned on $y$.\n",
        "- $z$ is noise vector sampled from prior $p_z$.\n",
        "\n",
        "## Generator Architecture\n",
        "\n",
        "The generator learns a mapping from the noise vector and condition to the data space:\n",
        "\n",
        "$$\n",
        "G: (z, y) \\to x\n",
        "$$\n",
        "\n",
        "Typically, the condition $y$ is embedded and concatenated with the noise vector $z$ before being passed through the generator network.\n",
        "\n",
        "The generator's goal is to produce samples indistinguishable from real data conditioned on $y$:\n",
        "\n",
        "$$\n",
        "G(z|y) = \\text{GeneratorNetwork}([z; y])\n",
        "$$\n",
        "\n",
        "where $[z; y]$ denotes concatenation.\n",
        "\n",
        "## Discriminator Architecture\n",
        "\n",
        "The discriminator receives both the data sample and the condition:\n",
        "\n",
        "$$\n",
        "D: (x, y) \\to [0,1]\n",
        "$$\n",
        "\n",
        "It learns to distinguish between real and fake samples, considering the condition $y$. The discriminator often concatenates an embedding of $y$ with intermediate features or directly with the input $x$.\n",
        "\n",
        "## Training Dynamics\n",
        "\n",
        "Training proceeds similarly to vanilla GANs but with conditioned inputs. The discriminator tries to maximize the likelihood of correctly classifying real and fake samples with the condition, while the generator tries to fool the discriminator with condition-aware fake samples.\n",
        "\n",
        "The discriminator loss is:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_D = -\\mathbb{E}_{x,y \\sim p_{\\text{data}}} [\\log D(x|y)] - \\mathbb{E}_{z,y \\sim p_z, p_y} [\\log (1 - D(G(z|y)|y))]\n",
        "$$\n",
        "\n",
        "The generator loss is:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_G = - \\mathbb{E}_{z,y \\sim p_z, p_y} [\\log D(G(z|y)|y)]\n",
        "$$\n",
        "\n",
        "## Advantages of cGANs\n",
        "\n",
        "- Controlled generation: Enables generating samples corresponding to specific classes or attributes.\n",
        "- Improved training stability by narrowing the target distribution.\n",
        "- Applicability to tasks like image-to-image translation, super-resolution, and text-to-image synthesis.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Conditional GANs extend GANs by incorporating auxiliary information $y$ into both generator and discriminator, enabling the learning of conditional data distributions $p(x|y)$. The objective function adapts to condition the adversarial game on $y$, guiding generation and improving model controllability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cycle Generative Adversarial Network (CycleGAN)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "CycleGAN is a type of Generative Adversarial Network designed for unpaired image-to-image translation tasks. Unlike traditional GANs that require paired datasets, CycleGAN learns to translate images from one domain $X$ to another domain $Y$ without direct correspondence between samples. This enables tasks like style transfer where paired data is unavailable.\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "CycleGAN consists of two generators and two discriminators:\n",
        "\n",
        "- Generators: \n",
        "  - $G: X \\to Y$ (translates images from domain $X$ to domain $Y$)\n",
        "  - $F: Y \\to X$ (translates images from domain $Y$ to domain $X$)\n",
        "  \n",
        "- Discriminators:\n",
        "  - $D_Y$ distinguishes real $Y$ images from generated $G(X)$ images.\n",
        "  - $D_X$ distinguishes real $X$ images from generated $F(Y)$ images.\n",
        "\n",
        "## Objective Functions\n",
        "\n",
        "### Adversarial Loss\n",
        "\n",
        "Each generator competes against its corresponding discriminator, trying to produce outputs indistinguishable from real images in the target domain.\n",
        "\n",
        "For generator $G$ and discriminator $D_Y$:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{GAN}(G, D_Y, X, Y) = \\mathbb{E}_{y \\sim p_{data}(y)}[\\log D_Y(y)] + \\mathbb{E}_{x \\sim p_{data}(x)}[\\log (1 - D_Y(G(x)))]\n",
        "$$\n",
        "\n",
        "Similarly for generator $F$ and discriminator $D_X$:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{GAN}(F, D_X, Y, X) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D_X(x)] + \\mathbb{E}_{y \\sim p_{data}(y)}[\\log (1 - D_X(F(y)))]\n",
        "$$\n",
        "\n",
        "### Cycle Consistency Loss\n",
        "\n",
        "To ensure the mappings $G$ and $F$ are consistent inverses, CycleGAN introduces cycle consistency:\n",
        "\n",
        "- Forward cycle: $x \\to G(x) \\to F(G(x)) \\approx x$\n",
        "- Backward cycle: $y \\to F(y) \\to G(F(y)) \\approx y$\n",
        "\n",
        "The cycle consistency loss enforces these constraints by minimizing the difference between original and reconstructed images:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{cyc}(G, F) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\|F(G(x)) - x\\|_1] + \\mathbb{E}_{y \\sim p_{data}(y)}[\\|G(F(y)) - y\\|_1]\n",
        "$$\n",
        "\n",
        "Here, the $L_1$ norm encourages the reconstructions to be close to the originals.\n",
        "\n",
        "### Identity Loss (Optional)\n",
        "\n",
        "To preserve color composition and prevent unnecessary changes when input images are already in the target style, an identity mapping loss can be applied:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{idt}(G, F) = \\mathbb{E}_{y \\sim p_{data}(y)}[\\|G(y) - y\\|_1] + \\mathbb{E}_{x \\sim p_{data}(x)}[\\|F(x) - x\\|_1]\n",
        "$$\n",
        "\n",
        "## Full Objective\n",
        "\n",
        "The complete loss function is a weighted sum of adversarial, cycle consistency, and optionally identity losses:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(G, F, D_X, D_Y) = \\mathcal{L}_{GAN}(G, D_Y, X, Y) + \\mathcal{L}_{GAN}(F, D_X, Y, X) + \\lambda \\mathcal{L}_{cyc}(G, F) + \\gamma \\mathcal{L}_{idt}(G, F)\n",
        "$$\n",
        "\n",
        "where $\\lambda$ and $\\gamma$ are hyperparameters controlling the importance of cycle consistency and identity losses.\n",
        "\n",
        "## Training Process\n",
        "\n",
        "CycleGAN trains generators $G$ and $F$ jointly with discriminators $D_X$ and $D_Y$ to minimize the full objective. The cycle consistency constraint ensures that the learned mappings are meaningful and avoid arbitrary changes, enabling realistic image translations without paired datasets.\n",
        "\n",
        "## Summary\n",
        "\n",
        "CycleGAN enables unpaired image-to-image translation by combining adversarial training with cycle consistency losses. The dual generators and discriminators learn forward and backward mappings between domains, constrained to be near-inverses, which ensures plausible and consistent transformations without requiring paired training examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Super Resolution Generative Adversarial Network (SRGAN)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Super Resolution GAN (SRGAN) is a deep learning model designed to generate high-resolution images from low-resolution inputs, addressing the problem of image super-resolution. It leverages adversarial training to produce photo-realistic details beyond simple pixel-wise upscaling.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Traditional super-resolution methods minimize pixel-wise losses such as Mean Squared Error (MSE), which often result in overly smooth and blurry images lacking fine details. SRGAN introduces a perceptual loss framework to better capture high-frequency details and texture, making images visually more convincing.\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "SRGAN consists of two main components:\n",
        "\n",
        "- Generator $G$: Upsamples a low-resolution image $I_{LR}$ to a high-resolution image $I_{SR} = G(I_{LR})$.\n",
        "\n",
        "- Discriminator $D$: Distinguishes between real high-resolution images $I_{HR}$ and generated images $I_{SR}$.\n",
        "\n",
        "The goal is for $G$ to produce images that $D$ cannot distinguish from real high-resolution images.\n",
        "\n",
        "## Loss Functions\n",
        "\n",
        "### Content Loss\n",
        "\n",
        "Instead of pixel-wise MSE, SRGAN uses a perceptual loss based on differences in high-level feature representations extracted by a pre-trained network (usually VGG).\n",
        "\n",
        "The content loss compares features of generated and ground truth images at a chosen layer $\\phi_j$ of the VGG network:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{content}(G) = \\mathbb{E}_{I_{HR}, I_{SR}} \\left[ \\| \\phi_j(I_{HR}) - \\phi_j(I_{SR}) \\|_2^2 \\right]\n",
        "$$\n",
        "\n",
        "This encourages the generator to produce images that are perceptually similar to the real high-resolution images.\n",
        "\n",
        "### Adversarial Loss\n",
        "\n",
        "The adversarial loss encourages $G$ to produce images that are indistinguishable from real images by the discriminator:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{GAN}(G, D) = \\mathbb{E}_{I_{LR}} \\left[ \\log(1 - D(G(I_{LR}))) \\right]\n",
        "$$\n",
        "\n",
        "The discriminator loss is:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{D} = -\\mathbb{E}_{I_{HR}}[\\log D(I_{HR})] - \\mathbb{E}_{I_{LR}}[\\log (1 - D(G(I_{LR})))]\n",
        "$$\n",
        "\n",
        "### Total Loss\n",
        "\n",
        "The total loss for the generator combines content and adversarial losses with a weighting parameter $\\lambda$:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{G} = \\mathcal{L}_{content} + \\lambda \\mathcal{L}_{GAN}\n",
        "$$\n",
        "\n",
        "Typically, $\\lambda$ controls the trade-off between perceptual quality and faithfulness to the content.\n",
        "\n",
        "## Network Details\n",
        "\n",
        "### Generator\n",
        "\n",
        "The generator uses deep residual blocks and upsampling layers to learn the mapping from $I_{LR}$ to $I_{SR}$. Residual connections help in stabilizing training and improving convergence.\n",
        "\n",
        "### Discriminator\n",
        "\n",
        "The discriminator is a deep convolutional network that outputs the probability that an input image is real or generated, trained to maximize classification accuracy between real high-resolution and generated images.\n",
        "\n",
        "## Training Procedure\n",
        "\n",
        "1. The generator produces super-resolved images from low-resolution inputs.\n",
        "\n",
        "2. The discriminator evaluates these images against real high-resolution images.\n",
        "\n",
        "3. Generator parameters are updated to minimize the total loss $\\mathcal{L}_G$, aiming to fool the discriminator and match perceptual features.\n",
        "\n",
        "4. Discriminator parameters are updated to maximize its ability to distinguish real from generated images.\n",
        "\n",
        "## Summary\n",
        "\n",
        "SRGAN advances super-resolution by combining perceptual content loss with adversarial loss, allowing generation of high-resolution images with finer texture and visually pleasing details, surpassing classical pixel-wise optimization methods.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wasserstein Generative Adversarial Networks (WGANs): Convergence and Optimization\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Wasserstein GAN (WGAN) is an improved variant of the traditional GAN designed to address training instability and mode collapse issues by using a different distance metric — the Wasserstein distance (Earth Mover's Distance) — instead of the Jensen-Shannon divergence.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Traditional GANs optimize the Jensen-Shannon divergence between the real data distribution $P_r$ and the generated data distribution $P_g$. However, this divergence can saturate and cause unstable training when $P_r$ and $P_g$ have non-overlapping supports.\n",
        "\n",
        "WGAN proposes to optimize the Wasserstein distance $W(P_r, P_g)$, which provides a smoother and more meaningful loss landscape for training.\n",
        "\n",
        "## Wasserstein Distance\n",
        "\n",
        "The Wasserstein-1 distance between two distributions $P_r$ and $P_g$ over a metric space $\\mathcal{X}$ is defined as:\n",
        "\n",
        "$$\n",
        "W(P_r, P_g) = \\inf_{\\gamma \\in \\Pi(P_r, P_g)} \\mathbb{E}_{(x, y) \\sim \\gamma} [ \\| x - y \\| ]\n",
        "$$\n",
        "\n",
        "where $\\Pi(P_r, P_g)$ is the set of all joint distributions with marginals $P_r$ and $P_g$. Intuitively, it represents the minimal \"cost\" of transporting mass to transform $P_r$ into $P_g$.\n",
        "\n",
        "## Kantorovich-Rubinstein Duality\n",
        "\n",
        "Direct computation of $W(P_r, P_g)$ is intractable. Using Kantorovich-Rubinstein duality, it can be rewritten as:\n",
        "\n",
        "$$\n",
        "W(P_r, P_g) = \\sup_{\\|f\\|_L \\leq 1} \\mathbb{E}_{x \\sim P_r}[f(x)] - \\mathbb{E}_{x \\sim P_g}[f(x)]\n",
        "$$\n",
        "\n",
        "where the supremum is taken over all 1-Lipschitz functions $f : \\mathcal{X} \\to \\mathbb{R}$.\n",
        "\n",
        "In WGAN, the critic function $f$ (replacing the discriminator) is trained to approximate this supremum under the Lipschitz constraint.\n",
        "\n",
        "## WGAN Objective\n",
        "\n",
        "The WGAN training optimizes the following objective:\n",
        "\n",
        "$$\n",
        "\\min_{G} \\max_{f \\in \\mathcal{F}} \\mathbb{E}_{x \\sim P_r}[f(x)] - \\mathbb{E}_{z \\sim P_z}[f(G(z))]\n",
        "$$\n",
        "\n",
        "where $\\mathcal{F}$ is the set of 1-Lipschitz functions, $G$ is the generator, and $z$ is the input noise sampled from a prior $P_z$.\n",
        "\n",
        "## Enforcing the Lipschitz Constraint\n",
        "\n",
        "Since enforcing the 1-Lipschitz constraint exactly is difficult, WGAN originally enforces it via weight clipping:\n",
        "\n",
        "$$\n",
        "w \\in [-c, c] \\quad \\text{for some constant } c\n",
        "$$\n",
        "\n",
        "This constrains the weights $w$ of the critic network to a compact space, ensuring the Lipschitz continuity approximately.\n",
        "\n",
        "## Improved WGAN: Gradient Penalty\n",
        "\n",
        "Weight clipping can cause optimization difficulties. An improved approach (WGAN-GP) replaces weight clipping with a gradient penalty, enforcing the Lipschitz constraint by penalizing the norm of the critic's gradient w.r.t. its input:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{GP} = \\lambda \\mathbb{E}_{\\hat{x} \\sim P_{\\hat{x}}} \\left[ \\left( \\| \\nabla_{\\hat{x}} f(\\hat{x}) \\|_2 - 1 \\right)^2 \\right]\n",
        "$$\n",
        "\n",
        "where $\\hat{x}$ is sampled uniformly along straight lines between real and generated samples, and $\\lambda$ controls the penalty strength.\n",
        "\n",
        "The critic loss becomes:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_D = \\mathbb{E}_{x \\sim P_g}[f(x)] - \\mathbb{E}_{x \\sim P_r}[f(x)] + \\mathcal{L}_{GP}\n",
        "$$\n",
        "\n",
        "## Training Procedure\n",
        "\n",
        "1. Update critic $f$ to maximize the difference $\\mathbb{E}_{x \\sim P_r}[f(x)] - \\mathbb{E}_{x \\sim P_g}[f(x)]$ while enforcing Lipschitz constraint.\n",
        "\n",
        "2. Update generator $G$ to minimize $\\mathbb{E}_{x \\sim P_g}[f(x)]$ to produce samples that maximize critic's output.\n",
        "\n",
        "3. Repeat iteratively until convergence.\n",
        "\n",
        "## Advantages of WGAN\n",
        "\n",
        "- Provides a meaningful loss that correlates with sample quality.\n",
        "\n",
        "- Mitigates mode collapse by improving gradient quality.\n",
        "\n",
        "- Stabilizes training dynamics due to smooth loss function.\n",
        "\n",
        "## Summary\n",
        "\n",
        "WGAN reformulates GAN training by minimizing the Wasserstein distance between data and model distributions, resulting in better convergence properties. Key to its success is enforcing the 1-Lipschitz condition on the critic, either via weight clipping or gradient penalty, enabling stable and meaningful training of generative models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# StyleGAN: Style Generative Adversarial Networks\n",
        "\n",
        "## Introduction\n",
        "\n",
        "StyleGAN is an advanced GAN architecture designed to improve the quality and controllability of generated images by introducing style-based generator architecture. It separates high-level attributes (like pose and identity) from stochastic variation (like freckles or hair), enabling intuitive control over image synthesis.\n",
        "\n",
        "## Motivation and Key Innovations\n",
        "\n",
        "Traditional GAN generators map a latent vector $z$ directly to an image, leading to entangled and uncontrollable image features. StyleGAN introduces a style-based generator that disentangles these features by using an intermediate latent space $W$ and adaptive instance normalization (AdaIN) layers, allowing finer control over the generated image’s attributes.\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "StyleGAN's generator consists of three key components:\n",
        "\n",
        "1. **Mapping Network**: Transforms the input latent vector $z$ sampled from a distribution $P_z$ (typically Gaussian) into an intermediate latent vector $w$ in space $W$:\n",
        "\n",
        "$$\n",
        "w = f(z), \\quad z \\sim P_z, \\quad w \\in W\n",
        "$$\n",
        "\n",
        "where $f$ is an 8-layer fully connected network that learns to disentangle latent factors.\n",
        "\n",
        "2. **Adaptive Instance Normalization (AdaIN)**: Instead of feeding $w$ directly, it modulates feature maps at each convolutional layer with style parameters derived from $w$. For an input feature map $x_i$:\n",
        "\n",
        "$$\n",
        "\\text{AdaIN}(x_i, y) = y_{s,i} \\frac{x_i - \\mu(x_i)}{\\sigma(x_i)} + y_{b,i}\n",
        "$$\n",
        "\n",
        "where $\\mu(x_i)$ and $\\sigma(x_i)$ are mean and standard deviation of feature $x_i$ across spatial dimensions, and $y_s$, $y_b$ are scale and bias parameters learned from $w$ via affine transforms.\n",
        "\n",
        "3. **Noise Injection**: Random noise is added explicitly to intermediate layers to generate stochastic details such as hair strands or skin pores, allowing fine-grained variation without changing high-level style.\n",
        "\n",
        "## Generator Workflow\n",
        "\n",
        "- Input $z$ is mapped to $w = f(z)$.\n",
        "\n",
        "- For each convolution layer $l$, style parameters $(y_s^l, y_b^l)$ are computed from $w$.\n",
        "\n",
        "- The layer’s feature maps are normalized and modulated via AdaIN using $(y_s^l, y_b^l)$.\n",
        "\n",
        "- Noise $n^l$ with learnable scaling factors is added to features for stochastic variation.\n",
        "\n",
        "This architecture disentangles \"style\" (controlled by $w$) from stochastic variation (noise), enabling control over global and local image features.\n",
        "\n",
        "## Mathematical Formulation of AdaIN Modulation\n",
        "\n",
        "Given a feature map $x_i$ at layer $l$, AdaIN operates as:\n",
        "\n",
        "$$\n",
        "\\text{AdaIN}(x_i, y^l) = y_s^l \\cdot \\frac{x_i - \\mu(x_i)}{\\sigma(x_i)} + y_b^l\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\mu(x_i) = \\frac{1}{HW} \\sum_{h=1}^H \\sum_{w=1}^W x_i(h,w), \\quad \\sigma(x_i) = \\sqrt{\\frac{1}{HW} \\sum_{h=1}^H \\sum_{w=1}^W (x_i(h,w) - \\mu(x_i))^2 + \\epsilon}\n",
        "$$\n",
        "\n",
        "$H$ and $W$ are spatial dimensions, and $\\epsilon$ is a small constant for numerical stability.\n",
        "\n",
        "The style parameters $y_s^l, y_b^l$ are affine transformations of $w$:\n",
        "\n",
        "$$\n",
        "(y_s^l, y_b^l) = A^l(w)\n",
        "$$\n",
        "\n",
        "where $A^l$ is a learned linear layer per convolution layer.\n",
        "\n",
        "## Loss Functions and Training\n",
        "\n",
        "StyleGAN employs standard GAN losses with a discriminator trained to distinguish real from generated images, and a generator trained to fool the discriminator. Additionally, it benefits from progressive growing techniques and regularization to stabilize training.\n",
        "\n",
        "The adversarial loss for generator $G$ and discriminator $D$ can be summarized as:\n",
        "\n",
        "$$\n",
        "\\min_G \\max_D \\mathbb{E}_{x \\sim P_r}[\\log D(x)] + \\mathbb{E}_{z \\sim P_z}[\\log(1 - D(G(z)))]\n",
        "$$\n",
        "\n",
        "## Advantages of StyleGAN\n",
        "\n",
        "- **Disentangled Latent Space**: The intermediate latent space $W$ allows meaningful manipulation of image features.\n",
        "\n",
        "- **Fine-grained Control**: Styles can be applied at different resolutions to control coarse to fine details.\n",
        "\n",
        "- **Stochastic Variation**: Explicit noise injection enables realistic stochastic details without affecting global structure.\n",
        "\n",
        "- **High-Quality Images**: Achieves state-of-the-art results in image synthesis with improved visual fidelity and variety.\n",
        "\n",
        "## Summary\n",
        "\n",
        "StyleGAN advances GANs by introducing an intermediate latent space and adaptive instance normalization, enabling disentangled and controllable image synthesis. It modulates styles at each convolutional layer and injects noise to generate realistic, high-quality images with fine control over appearance and variation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sparse Autoencoders in Deep Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Sparse Autoencoders are a variant of autoencoders designed to learn efficient and meaningful representations by imposing a sparsity constraint on the hidden units. Unlike standard autoencoders that aim to reconstruct inputs, sparse autoencoders encourage most neurons in the hidden layer to remain inactive (close to zero), forcing the model to learn a compressed and more interpretable feature representation.\n",
        "\n",
        "## Autoencoder Recap\n",
        "\n",
        "An autoencoder consists of two parts: an encoder that maps input $x$ to a hidden representation $h$, and a decoder that reconstructs $x$ from $h$.\n",
        "\n",
        "$$\n",
        "h = f(W^{(1)} x + b^{(1)}), \\quad \\hat{x} = g(W^{(2)} h + b^{(2)})\n",
        "$$\n",
        "\n",
        "where $f$ and $g$ are activation functions, $W^{(1)}, W^{(2)}$ are weight matrices, and $b^{(1)}, b^{(2)}$ are biases.\n",
        "\n",
        "The training objective is to minimize the reconstruction error:\n",
        "\n",
        "$$\n",
        "J_{\\text{recon}} = \\frac{1}{m} \\sum_{i=1}^m \\| x^{(i)} - \\hat{x}^{(i)} \\|^2\n",
        "$$\n",
        "\n",
        "for a dataset with $m$ samples.\n",
        "\n",
        "## Sparsity Constraint\n",
        "\n",
        "Sparse Autoencoders impose an additional penalty to enforce sparsity on the average activation of hidden units. Let $a_j^{(i)}$ denote the activation of hidden neuron $j$ on input $x^{(i)}$. The average activation of neuron $j$ over the training set is:\n",
        "\n",
        "$$\n",
        "\\hat{\\rho}_j = \\frac{1}{m} \\sum_{i=1}^m a_j^{(i)}\n",
        "$$\n",
        "\n",
        "The sparsity constraint enforces:\n",
        "\n",
        "$$\n",
        "\\hat{\\rho}_j \\approx \\rho\n",
        "$$\n",
        "\n",
        "where $\\rho$ is a small sparsity parameter (e.g., 0.05), indicating that neuron $j$ should be active only about 5% of the time.\n",
        "\n",
        "## Sparsity Penalty Function\n",
        "\n",
        "To encourage $\\hat{\\rho}_j$ to be close to $\\rho$, a penalty based on the Kullback-Leibler (KL) divergence is used:\n",
        "\n",
        "$$\n",
        "\\text{KL}(\\rho \\parallel \\hat{\\rho}_j) = \\rho \\log \\frac{\\rho}{\\hat{\\rho}_j} + (1-\\rho) \\log \\frac{1-\\rho}{1-\\hat{\\rho}_j}\n",
        "$$\n",
        "\n",
        "This penalty is zero when $\\hat{\\rho}_j = \\rho$ and increases as $\\hat{\\rho}_j$ deviates from $\\rho$.\n",
        "\n",
        "## Overall Cost Function\n",
        "\n",
        "The total loss function to minimize is a combination of reconstruction loss, weight decay regularization, and the sparsity penalty:\n",
        "\n",
        "$$\n",
        "J = J_{\\text{recon}} + \\lambda \\sum_l \\sum_{i,j} (W_{ij}^{(l)})^2 + \\beta \\sum_{j=1}^{s} \\text{KL}(\\rho \\parallel \\hat{\\rho}_j)\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\lambda$ controls weight decay regularization to prevent overfitting.\n",
        "\n",
        "- $\\beta$ controls the weight of the sparsity penalty.\n",
        "\n",
        "- $s$ is the number of hidden units.\n",
        "\n",
        "## Training Process\n",
        "\n",
        "The training minimizes $J$ using gradient descent or variants, adjusting weights and biases such that:\n",
        "\n",
        "- The network reconstructs input well.\n",
        "\n",
        "- The hidden units remain mostly inactive.\n",
        "\n",
        "This forces the network to discover a compressed and sparse representation of the input data.\n",
        "\n",
        "## Benefits of Sparse Autoencoders\n",
        "\n",
        "- **Feature Extraction**: Learns meaningful and interpretable features by enforcing sparsity.\n",
        "\n",
        "- **Avoids Trivial Solutions**: Sparsity prevents the network from simply copying inputs.\n",
        "\n",
        "- **Useful for High-Dimensional Data**: Effective in reducing dimensionality with minimal loss.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Sparse Autoencoders enhance traditional autoencoders by adding a sparsity constraint on hidden activations, enforced through a KL-divergence penalty. This encourages a compact and efficient representation where most neurons are inactive, improving feature learning and generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Denoising Autoencoders in Machine Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Denoising Autoencoders (DAEs) are a type of autoencoder designed to learn robust feature representations by reconstructing clean inputs from corrupted versions. Unlike standard autoencoders that reconstruct the input as is, DAEs are trained to remove noise or corruption, thereby learning features that capture the true underlying structure of the data.\n",
        "\n",
        "## Autoencoder Framework Recap\n",
        "\n",
        "An autoencoder consists of an encoder function $f_\\theta$ and a decoder function $g_\\theta$ with parameters $\\theta$:\n",
        "\n",
        "$$\n",
        "h = f_\\theta(\\tilde{x}) = \\sigma(W^{(1)} \\tilde{x} + b^{(1)}), \\quad \\hat{x} = g_\\theta(h) = \\sigma(W^{(2)} h + b^{(2)})\n",
        "$$\n",
        "\n",
        "where $\\tilde{x}$ is the input (in DAE, a corrupted version of $x$), $h$ is the latent representation, and $\\hat{x}$ is the reconstruction.\n",
        "\n",
        "## Corrupted Input\n",
        "\n",
        "The input $x$ is first corrupted to obtain $\\tilde{x}$ through a stochastic corruption process $q_D(\\tilde{x}|x)$, such as adding Gaussian noise or masking some inputs:\n",
        "\n",
        "$$\n",
        "\\tilde{x} \\sim q_D(\\tilde{x}|x)\n",
        "$$\n",
        "\n",
        "This forces the autoencoder to recover the original $x$ from its noisy version $\\tilde{x}$.\n",
        "\n",
        "## Objective Function\n",
        "\n",
        "The goal is to minimize the expected reconstruction loss over the data distribution $p_{\\text{data}}(x)$ and the corruption process:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\mathbb{E}_{\\tilde{x} \\sim q_D(\\tilde{x}|x)} \\left[ L \\big( x, g_\\theta(f_\\theta(\\tilde{x})) \\big) \\right]\n",
        "$$\n",
        "\n",
        "where $L$ is a loss function, often mean squared error:\n",
        "\n",
        "$$\n",
        "L(x, \\hat{x}) = \\| x - \\hat{x} \\|^2\n",
        "$$\n",
        "\n",
        "## Training Mechanism\n",
        "\n",
        "By minimizing $J(\\theta)$, the denoising autoencoder learns parameters $\\theta$ that can effectively remove noise from corrupted inputs, thus extracting features resilient to perturbations.\n",
        "\n",
        "## Benefits of Denoising Autoencoders\n",
        "\n",
        "- **Robust Feature Learning**: By reconstructing clean inputs from noisy data, DAEs learn stable and robust representations.\n",
        "\n",
        "- **Regularization Effect**: Corrupting inputs acts as a regularizer preventing overfitting.\n",
        "\n",
        "- **Improved Generalization**: Models trained with DAEs tend to generalize better on unseen data.\n",
        "\n",
        "## Mathematical Intuition\n",
        "\n",
        "The corruption and reconstruction forces the network to approximate the data manifold and learn a function $g_\\theta(f_\\theta(\\cdot))$ that projects corrupted samples back onto the manifold of clean data.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Denoising Autoencoders improve upon standard autoencoders by introducing input corruption during training and optimizing reconstruction of the original clean input. This leads to more robust and meaningful feature representations, useful for downstream tasks and improving model generalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Undercomplete Autoencoder\n",
        "\n",
        "## Introduction\n",
        "\n",
        "An undercomplete autoencoder is a type of autoencoder where the dimensionality of the latent representation (code) $h$ is strictly less than that of the input $x$. This dimensionality constraint forces the model to learn a compressed, efficient representation of the input data.\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "Given an input vector $x \\in \\mathbb{R}^d$, the encoder maps it to a latent vector $h \\in \\mathbb{R}^p$ where $p < d$:\n",
        "\n",
        "$$\n",
        "h = f_\\theta(x) = \\sigma(W^{(1)} x + b^{(1)})\n",
        "$$\n",
        "\n",
        "The decoder reconstructs the input from the latent code:\n",
        "\n",
        "$$\n",
        "\\hat{x} = g_\\theta(h) = \\sigma(W^{(2)} h + b^{(2)})\n",
        "$$\n",
        "\n",
        "where $W^{(1)}$ and $W^{(2)}$ are weight matrices, $b^{(1)}$ and $b^{(2)}$ are biases, and $\\sigma$ is a non-linear activation function.\n",
        "\n",
        "## Objective Function\n",
        "\n",
        "The training goal is to minimize the reconstruction loss, often Mean Squared Error (MSE):\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\| x^{(i)} - \\hat{x}^{(i)} \\|^2 = \\frac{1}{N} \\sum_{i=1}^N \\| x^{(i)} - g_\\theta(f_\\theta(x^{(i)})) \\|^2\n",
        "$$\n",
        "\n",
        "where $N$ is the number of training samples.\n",
        "\n",
        "## Importance of Dimensionality Constraint\n",
        "\n",
        "By restricting $p < d$, the network cannot simply learn the identity function. Instead, it must discover salient features that effectively encode the input information in fewer dimensions, enabling dimensionality reduction and feature extraction.\n",
        "\n",
        "## Comparison to Overcomplete Autoencoders\n",
        "\n",
        "Unlike overcomplete autoencoders where $p \\geq d$ and regularization techniques are needed to prevent trivial identity mapping, undercomplete autoencoders rely on the bottleneck to enforce meaningful compression inherently.\n",
        "\n",
        "## Applications\n",
        "\n",
        "Undercomplete autoencoders are widely used for:\n",
        "\n",
        "- Dimensionality reduction (similar to PCA but nonlinear).\n",
        "\n",
        "- Feature extraction for downstream tasks.\n",
        "\n",
        "- Data denoising and anomaly detection by measuring reconstruction error.\n",
        "\n",
        "## Summary\n",
        "\n",
        "The undercomplete autoencoder compresses input data into a lower-dimensional latent space and reconstructs it, learning compact and informative representations by minimizing reconstruction loss under a dimensionality bottleneck.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Contractive Autoencoder (CAE)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "A Contractive Autoencoder (CAE) is a variant of the traditional autoencoder designed to learn robust and invariant feature representations by explicitly penalizing the sensitivity of the encoded representation to small input perturbations. This encourages the model to learn features that are stable under slight variations of the input data.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "Like a standard autoencoder, CAE consists of an encoder and a decoder:\n",
        "\n",
        "The encoder maps input $x \\in \\mathbb{R}^d$ to a latent representation $h \\in \\mathbb{R}^p$:\n",
        "\n",
        "$$\n",
        "h = f_\\theta(x) = \\sigma(Wx + b)\n",
        "$$\n",
        "\n",
        "The decoder reconstructs the input from $h$:\n",
        "\n",
        "$$\n",
        "\\hat{x} = g_\\theta(h) = \\sigma'(W' h + b')\n",
        "$$\n",
        "\n",
        "where $W, W'$ are weight matrices, $b, b'$ biases, and $\\sigma, \\sigma'$ are activation functions.\n",
        "\n",
        "## Objective Function with Contractive Penalty\n",
        "\n",
        "CAE extends the reconstruction loss with a contractive regularization term that penalizes the Frobenius norm of the Jacobian of the encoder activations with respect to the input:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\| x^{(i)} - \\hat{x}^{(i)} \\|^2 + \\lambda \\sum_{i=1}^N \\| \\nabla_x f_\\theta(x^{(i)}) \\|_F^2\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- The first term is the Mean Squared Error (MSE) reconstruction loss.\n",
        "\n",
        "- The second term enforces robustness by penalizing the sensitivity of $h$ to changes in $x$.\n",
        "\n",
        "- $\\lambda > 0$ is a hyperparameter controlling the strength of the contractive penalty.\n",
        "\n",
        "- $\\| \\cdot \\|_F$ denotes the Frobenius norm.\n",
        "\n",
        "## Jacobian Calculation for the Contractive Term\n",
        "\n",
        "Assuming the encoder activation function is element-wise and differentiable, e.g., sigmoid $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, the Jacobian matrix $J$ for input $x$ is:\n",
        "\n",
        "$$\n",
        "J = \\frac{\\partial h}{\\partial x} = \\text{diag} \\left( \\sigma'(Wx + b) \\right) W\n",
        "$$\n",
        "\n",
        "where $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$ is the derivative of the sigmoid.\n",
        "\n",
        "The squared Frobenius norm of the Jacobian is:\n",
        "\n",
        "$$\n",
        "\\| J \\|_F^2 = \\sum_{j=1}^p \\sum_{k=1}^d \\left( \\frac{\\partial h_j}{\\partial x_k} \\right)^2\n",
        "$$\n",
        "\n",
        "This term measures how much the representation $h$ changes with small perturbations in $x$.\n",
        "\n",
        "## Intuition and Benefits\n",
        "\n",
        "By minimizing the Jacobian norm, CAE encourages the encoder to map nearby input points to similar latent representations, enhancing invariance and robustness to noise and input variability. This leads to smoother feature representations that generalize better.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Contractive Autoencoders augment standard autoencoders with a penalty on the sensitivity of encoded features to input changes, formulated via the Jacobian norm of the encoder. This leads to robust, stable feature learning by solving:\n",
        "\n",
        "$$\n",
        "\\min_\\theta \\frac{1}{N} \\sum_{i=1}^N \\| x^{(i)} - g_\\theta(f_\\theta(x^{(i)})) \\|^2 + \\lambda \\sum_{i=1}^N \\| \\nabla_x f_\\theta(x^{(i)}) \\|_F^2\n",
        "$$\n",
        "\n",
        "where the trade-off between reconstruction and contraction is controlled by $\\lambda$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convolutional Autoencoder (CAE)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "A Convolutional Autoencoder (CAE) is a type of autoencoder specially designed for processing image data. It replaces fully connected layers with convolutional layers, enabling the model to efficiently capture spatial hierarchies and local patterns in images.\n",
        "\n",
        "The CAE consists of two parts: an encoder that compresses the input image into a latent representation, and a decoder that reconstructs the image from this compressed form.\n",
        "\n",
        "## Encoder Architecture\n",
        "\n",
        "The encoder applies a series of convolutional operations followed by non-linear activations and optional downsampling (e.g., via strides or pooling). Formally, for input image $X$, the $l^{th}$ convolutional layer output $H^{(l)}$ is computed as:\n",
        "\n",
        "$$\n",
        "H^{(l)} = \\sigma \\left( W^{(l)} * H^{(l-1)} + b^{(l)} \\right)\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $*$ denotes convolution,\n",
        "\n",
        "- $W^{(l)}$ is the set of convolutional filters,\n",
        "\n",
        "- $b^{(l)}$ is the bias term,\n",
        "\n",
        "- $\\sigma$ is a non-linear activation function (commonly ReLU),\n",
        "\n",
        "- $H^{(0)} = X$ is the input image.\n",
        "\n",
        "The output of the final encoder layer is the latent representation $Z$.\n",
        "\n",
        "## Decoder Architecture\n",
        "\n",
        "The decoder reconstructs the input from the latent space by applying transposed convolution (also called deconvolution) layers, which perform upsampling. The $m^{th}$ decoder layer output $\\hat{H}^{(m)}$ is:\n",
        "\n",
        "$$\n",
        "\\hat{H}^{(m)} = \\sigma' \\left( W'^{(m)} \\star \\hat{H}^{(m-1)} + b'^{(m)} \\right)\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\star$ denotes the transposed convolution,\n",
        "\n",
        "- $W'^{(m)}$, $b'^{(m)}$ are the decoder filters and biases,\n",
        "\n",
        "- $\\sigma'$ is the activation function (often ReLU except for the last layer which may use sigmoid or linear),\n",
        "\n",
        "- $\\hat{H}^{(0)} = Z$ is the latent representation.\n",
        "\n",
        "The output $\\hat{X} = \\hat{H}^{(M)}$ is the reconstructed image.\n",
        "\n",
        "## Objective Function\n",
        "\n",
        "The network is trained to minimize the reconstruction error between input $X$ and reconstructed output $\\hat{X}$. The most common loss function is the Mean Squared Error (MSE):\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\| X^{(i)} - \\hat{X}^{(i)} \\|^2\n",
        "$$\n",
        "\n",
        "where $N$ is the number of training samples, and $\\theta$ represents all trainable parameters in encoder and decoder.\n",
        "\n",
        "## Advantages of Convolutional Autoencoders\n",
        "\n",
        "By using convolutional layers, CAEs effectively exploit the spatial structure of images, requiring fewer parameters than fully connected autoencoders. This improves training efficiency and quality of learned features.\n",
        "\n",
        "## Summary of Key Formulas\n",
        "\n",
        "- Encoder layer output:\n",
        "\n",
        "$$\n",
        "H^{(l)} = \\sigma(W^{(l)} * H^{(l-1)} + b^{(l)})\n",
        "$$\n",
        "\n",
        "- Decoder layer output:\n",
        "\n",
        "$$\n",
        "\\hat{H}^{(m)} = \\sigma'(W'^{(m)} \\star \\hat{H}^{(m-1)} + b'^{(m)})\n",
        "$$\n",
        "\n",
        "- Loss function (MSE):\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\| X^{(i)} - \\hat{X}^{(i)} \\|^2\n",
        "$$\n",
        "\n",
        "This design allows CAEs to learn compact, meaningful image representations useful for denoising, compression, and feature extraction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Variational Autoencoders (VAEs)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Variational Autoencoders (VAEs) are generative models that learn a probabilistic latent representation of data. Unlike traditional autoencoders that encode an input to a fixed vector, VAEs encode inputs as distributions over the latent space, enabling generation of new samples by sampling from this latent distribution.\n",
        "\n",
        "VAEs combine principles from deep learning and variational Bayesian inference to learn a continuous and smooth latent space, which facilitates data generation and interpolation.\n",
        "\n",
        "## VAE Architecture\n",
        "\n",
        "VAEs consist of an encoder network, a decoder network, and a latent variable model.\n",
        "\n",
        "Given input data $x$, the encoder learns to approximate the posterior distribution over latent variables $z$:\n",
        "\n",
        "$$\n",
        "q_\\phi(z|x)\n",
        "$$\n",
        "\n",
        "parameterized by $\\phi$ (encoder parameters). Instead of encoding $x$ into a deterministic vector, it outputs parameters of a probability distribution (typically Gaussian): mean $\\mu_\\phi(x)$ and standard deviation $\\sigma_\\phi(x)$.\n",
        "\n",
        "The decoder defines the likelihood of reconstructing $x$ from latent variable $z$:\n",
        "\n",
        "$$\n",
        "p_\\theta(x|z)\n",
        "$$\n",
        "\n",
        "parameterized by $\\theta$ (decoder parameters).\n",
        "\n",
        "## Objective Function: Evidence Lower Bound (ELBO)\n",
        "\n",
        "The training goal is to maximize the marginal likelihood $p_\\theta(x)$, which is generally intractable. Instead, VAEs maximize the Evidence Lower Bound (ELBO), which lower-bounds the log-likelihood:\n",
        "\n",
        "$$\n",
        "\\log p_\\theta(x) \\geq \\mathbb{E}_{q_\\phi(z|x)} \\big[ \\log p_\\theta(x|z) \\big] - D_{KL}\\big( q_\\phi(z|x) \\| p(z) \\big)\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "- $p(z)$ is the prior over latent variables (usually standard normal $ \\mathcal{N}(0,I)$),\n",
        "\n",
        "- $D_{KL}$ is the Kullback–Leibler divergence measuring the difference between the approximate posterior $q_\\phi(z|x)$ and the prior $p(z)$.\n",
        "\n",
        "The ELBO consists of two terms:\n",
        "\n",
        "1. **Reconstruction loss:** $\\mathbb{E}_{q_\\phi(z|x)} [ \\log p_\\theta(x|z) ]$ encourages the decoder to reconstruct $x$ accurately from sampled $z$.\n",
        "\n",
        "2. **Regularization term:** $D_{KL}( q_\\phi(z|x) \\| p(z) )$ forces the approximate posterior to stay close to the prior, ensuring smoothness and continuity in the latent space.\n",
        "\n",
        "## Reparameterization Trick\n",
        "\n",
        "Sampling from $q_\\phi(z|x)$ inside the network is non-differentiable, which blocks backpropagation. The reparameterization trick solves this by expressing $z$ as:\n",
        "\n",
        "$$\n",
        "z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
        "$$\n",
        "\n",
        "Here, $\\epsilon$ is a noise vector sampled from a standard normal distribution. This formulation allows gradients to propagate through $\\mu_\\phi$ and $\\sigma_\\phi$ during training.\n",
        "\n",
        "## Model Training\n",
        "\n",
        "The total loss to minimize (negative ELBO) is:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta, \\phi; x) = - \\mathbb{E}_{q_\\phi(z|x)} \\big[ \\log p_\\theta(x|z) \\big] + D_{KL}\\big( q_\\phi(z|x) \\| p(z) \\big)\n",
        "$$\n",
        "\n",
        "In practice:\n",
        "\n",
        "- The reconstruction loss is often implemented as Mean Squared Error (MSE) or Binary Cross-Entropy depending on data type.\n",
        "\n",
        "- The KL divergence for Gaussian distributions can be computed in closed form as:\n",
        "\n",
        "$$\n",
        "D_{KL} = \\frac{1}{2} \\sum_{j=1}^d \\left( \\mu_j^2 + \\sigma_j^2 - \\log \\sigma_j^2 - 1 \\right)\n",
        "$$\n",
        "\n",
        "where $d$ is the dimensionality of latent variable $z$.\n",
        "\n",
        "## Summary of Key Formulas\n",
        "\n",
        "- Encoder outputs: $\\mu_\\phi(x)$ and $\\sigma_\\phi(x)$ representing $q_\\phi(z|x) = \\mathcal{N}(\\mu_\\phi(x), \\mathrm{diag}(\\sigma_\\phi^2(x)))$.\n",
        "\n",
        "- Reparameterization trick:\n",
        "\n",
        "$$\n",
        "z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,I)\n",
        "$$\n",
        "\n",
        "- ELBO maximization objective:\n",
        "\n",
        "$$\n",
        "\\log p_\\theta(x) \\geq \\mathbb{E}_{q_\\phi(z|x)} [ \\log p_\\theta(x|z) ] - D_{KL}( q_\\phi(z|x) \\| p(z) )\n",
        "$$\n",
        "\n",
        "- Loss function (to minimize):\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta, \\phi; x) = - \\mathbb{E}_{q_\\phi(z|x)} [ \\log p_\\theta(x|z) ] + D_{KL}( q_\\phi(z|x) \\| p(z) )\n",
        "$$\n",
        "\n",
        "- KL divergence for Gaussian:\n",
        "\n",
        "$$\n",
        "D_{KL} = \\frac{1}{2} \\sum_{j=1}^d \\left( \\mu_j^2 + \\sigma_j^2 - \\log \\sigma_j^2 - 1 \\right)\n",
        "$$\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Variational Autoencoders provide a powerful framework to learn probabilistic latent representations and generate realistic data by sampling from latent distributions. The combination of reconstruction and KL divergence loss enforces both fidelity and smooth latent structure, enabling diverse applications in generative modeling, anomaly detection, and representation learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Deep Reinforcement Learning (DRL)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A Beginner’s Guide to Deep Reinforcement Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Deep Reinforcement Learning (Deep RL) combines Reinforcement Learning (RL) and Deep Learning to solve decision-making problems in high-dimensional environments. While RL is focused on learning optimal actions based on rewards from an environment, Deep RL uses neural networks to approximate policies and value functions in complex, unstructured input spaces like images or sensor data.\n",
        "\n",
        "## Core Concepts of Reinforcement Learning\n",
        "\n",
        "In RL, an agent interacts with an environment modeled as a Markov Decision Process (MDP). An MDP is defined by:\n",
        "\n",
        "- A set of states $S$\n",
        "- A set of actions $A$\n",
        "- A transition function $P(s'|s, a)$\n",
        "- A reward function $R(s, a)$\n",
        "- A discount factor $\\gamma \\in [0, 1]$\n",
        "\n",
        "At each timestep $t$, the agent:\n",
        "\n",
        "1. Observes state $s_t$\n",
        "2. Chooses an action $a_t$\n",
        "3. Receives a reward $r_t$\n",
        "4. Transitions to a new state $s_{t+1}$\n",
        "\n",
        "The goal is to learn a policy $\\pi(a|s)$ that maximizes the expected return:\n",
        "\n",
        "$$\n",
        "G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}\n",
        "$$\n",
        "\n",
        "## Value Function and Policy\n",
        "\n",
        "The **state-value function** $V^\\pi(s)$ represents the expected return starting from state $s$ under policy $\\pi$:\n",
        "\n",
        "$$\n",
        "V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} \\big| s_t = s \\right]\n",
        "$$\n",
        "\n",
        "The **action-value function** $Q^\\pi(s, a)$ gives the expected return starting from state $s$, taking action $a$, and following policy $\\pi$:\n",
        "\n",
        "$$\n",
        "Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} \\big| s_t = s, a_t = a \\right]\n",
        "$$\n",
        "\n",
        "The **optimal value functions** are:\n",
        "\n",
        "$$\n",
        "V^*(s) = \\max_\\pi V^\\pi(s)\n",
        "$$\n",
        "\n",
        "$$\n",
        "Q^*(s, a) = \\max_\\pi Q^\\pi(s, a)\n",
        "$$\n",
        "\n",
        "The **Bellman optimality equation** for $Q^*$ is:\n",
        "\n",
        "$$\n",
        "Q^*(s, a) = \\mathbb{E}_{s'} \\left[ r + \\gamma \\max_{a'} Q^*(s', a') \\right]\n",
        "$$\n",
        "\n",
        "## Deep Q-Network (DQN)\n",
        "\n",
        "DQN is a Deep RL algorithm where a neural network approximates the action-value function $Q(s, a; \\theta)$.\n",
        "\n",
        "The loss function for training the Q-network is:\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\mathbb{E}_{s, a, r, s'} \\left[ \\left( y - Q(s, a; \\theta) \\right)^2 \\right]\n",
        "$$\n",
        "\n",
        "where the target $y$ is:\n",
        "\n",
        "$$\n",
        "y = r + \\gamma \\max_{a'} Q(s', a'; \\theta^{-})\n",
        "$$\n",
        "\n",
        "$\\theta$ are the parameters of the current Q-network, and $\\theta^{-}$ are the parameters of a target network (updated periodically for stability).\n",
        "\n",
        "## Exploration vs Exploitation\n",
        "\n",
        "A balance is required between:\n",
        "\n",
        "- **Exploration**: trying new actions to discover their rewards\n",
        "- **Exploitation**: using current knowledge to maximize rewards\n",
        "\n",
        "A common approach is the $\\epsilon$-greedy strategy:\n",
        "\n",
        "- With probability $\\epsilon$, select a random action\n",
        "- With probability $1 - \\epsilon$, select $a = \\arg\\max_a Q(s, a)$\n",
        "\n",
        "## Policy-Based Methods\n",
        "\n",
        "Policy-based methods directly optimize the policy $\\pi_\\theta(a|s)$ by maximizing the expected return:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\sum_{t} \\gamma^t r_t \\right]\n",
        "$$\n",
        "\n",
        "The policy gradient is given by:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) Q^\\pi(s, a) \\right]\n",
        "$$\n",
        "\n",
        "This forms the basis of **REINFORCE** and other policy gradient methods.\n",
        "\n",
        "## Actor-Critic Methods\n",
        "\n",
        "Actor-Critic methods combine value-based and policy-based approaches. The **actor** updates the policy using gradients, while the **critic** estimates the value function to guide the actor.\n",
        "\n",
        "The update rule is:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) = \\mathbb{E} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) A(s, a) \\right]\n",
        "$$\n",
        "\n",
        "where $A(s, a) = Q(s, a) - V(s)$ is the advantage function.\n",
        "\n",
        "## Applications of Deep RL\n",
        "\n",
        "Deep RL has been successfully applied to:\n",
        "\n",
        "- Games (Atari, Go, Chess)\n",
        "- Robotics and control\n",
        "- Recommender systems\n",
        "- Self-driving cars\n",
        "- Resource allocation and scheduling\n",
        "\n",
        "## Summary\n",
        "\n",
        "- Deep RL uses neural networks to approximate policies and value functions\n",
        "- The goal is to learn a policy that maximizes cumulative rewards\n",
        "- Q-learning and Policy Gradient methods are core techniques\n",
        "- Actor-Critic combines strengths of both worlds\n",
        "- Deep RL is powerful for high-dimensional and sequential decision tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# What is Reinforcement Learning?\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Reinforcement Learning (RL) is a machine learning paradigm where an agent learns to make decisions by interacting with an environment. The agent aims to learn a policy that maximizes cumulative rewards over time through trial and error.\n",
        "\n",
        "## Core Components\n",
        "\n",
        "An RL system is modeled as a Markov Decision Process (MDP), defined by:\n",
        "\n",
        "- A set of states $S$\n",
        "- A set of actions $A$\n",
        "- A transition probability function $P(s'|s, a)$\n",
        "- A reward function $R(s, a)$\n",
        "- A discount factor $\\gamma \\in [0, 1]$\n",
        "\n",
        "At each timestep $t$:\n",
        "\n",
        "- The agent observes state $s_t$\n",
        "- Takes an action $a_t$\n",
        "- Receives a reward $r_t$\n",
        "- Transitions to a new state $s_{t+1}$\n",
        "\n",
        "The goal is to learn a policy $\\pi(a|s)$ that maximizes the expected cumulative reward, known as the return:\n",
        "\n",
        "$$\n",
        "G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}\n",
        "$$\n",
        "\n",
        "## Value Functions\n",
        "\n",
        "The **state-value function** $V^\\pi(s)$ measures the expected return from state $s$ following policy $\\pi$:\n",
        "\n",
        "$$\n",
        "V^\\pi(s) = \\mathbb{E}_\\pi \\left[ G_t \\big| s_t = s \\right]\n",
        "$$\n",
        "\n",
        "The **action-value function** $Q^\\pi(s, a)$ gives the expected return from state $s$, taking action $a$, and following policy $\\pi$:\n",
        "\n",
        "$$\n",
        "Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\big| s_t = s, a_t = a \\right]\n",
        "$$\n",
        "\n",
        "## Optimal Policy and Value\n",
        "\n",
        "The optimal state-value and action-value functions are:\n",
        "\n",
        "$$\n",
        "V^*(s) = \\max_\\pi V^\\pi(s)\n",
        "$$\n",
        "\n",
        "$$\n",
        "Q^*(s, a) = \\max_\\pi Q^\\pi(s, a)\n",
        "$$\n",
        "\n",
        "The optimal policy $\\pi^*$ is the one that achieves these maximums.\n",
        "\n",
        "## Bellman Equations\n",
        "\n",
        "The **Bellman Expectation Equation** for $V^\\pi(s)$:\n",
        "\n",
        "$$\n",
        "V^\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s'} P(s'|s, a) \\left[ R(s, a) + \\gamma V^\\pi(s') \\right]\n",
        "$$\n",
        "\n",
        "The **Bellman Optimality Equation** for $V^*(s)$:\n",
        "\n",
        "$$\n",
        "V^*(s) = \\max_a \\sum_{s'} P(s'|s, a) \\left[ R(s, a) + \\gamma V^*(s') \\right]\n",
        "$$\n",
        "\n",
        "## Categories of Reinforcement Learning\n",
        "\n",
        "### Model-Based vs Model-Free\n",
        "\n",
        "- **Model-Based RL** learns the transition and reward functions, then plans using them.\n",
        "- **Model-Free RL** learns the value function or policy directly from experience.\n",
        "\n",
        "### Value-Based vs Policy-Based\n",
        "\n",
        "- **Value-Based methods** learn $V(s)$ or $Q(s, a)$ to derive a policy.\n",
        "- **Policy-Based methods** directly learn the policy $\\pi(a|s)$.\n",
        "- **Actor-Critic methods** combine both approaches.\n",
        "\n",
        "## Exploration vs Exploitation\n",
        "\n",
        "The agent must balance:\n",
        "\n",
        "- **Exploration**: trying new actions to discover rewards\n",
        "- **Exploitation**: using known information to maximize rewards\n",
        "\n",
        "A common strategy is $\\epsilon$-greedy, where the agent explores with probability $\\epsilon$ and exploits with probability $1 - \\epsilon$.\n",
        "\n",
        "## Applications of Reinforcement Learning\n",
        "\n",
        "RL is used in areas such as:\n",
        "\n",
        "- Game playing (e.g., AlphaGo)\n",
        "- Robotics and control systems\n",
        "- Autonomous vehicles\n",
        "- Finance and trading strategies\n",
        "- Resource optimization and scheduling\n",
        "\n",
        "## Summary\n",
        "\n",
        "Reinforcement Learning is a goal-driven learning approach where an agent learns from interaction and feedback. It is built on MDPs and aims to maximize long-term rewards through policies learned from value estimates or directly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Markov Decision Process (MDP)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "A Markov Decision Process (MDP) is a formal framework for modeling decision-making in environments where outcomes are partly random and partly under the control of a decision-maker. MDPs are widely used in reinforcement learning.\n",
        "\n",
        "## Components of an MDP\n",
        "\n",
        "An MDP is defined by a 5-tuple $(S, A, P, R, \\gamma)$:\n",
        "\n",
        "- $S$: A finite set of states\n",
        "- $A$: A finite set of actions\n",
        "- $P(s'|s,a)$: Transition probability of reaching state $s'$ from state $s$ after taking action $a$\n",
        "- $R(s, a)$: Expected immediate reward received after taking action $a$ in state $s$\n",
        "- $\\gamma \\in [0,1]$: Discount factor that reduces the value of future rewards\n",
        "\n",
        "## Markov Property\n",
        "\n",
        "An MDP satisfies the **Markov property**, meaning that the future is conditionally independent of the past given the present state:\n",
        "\n",
        "$$\n",
        "P(s_{t+1} | s_t, a_t) = P(s_{t+1} | s_1, a_1, ..., s_t, a_t)\n",
        "$$\n",
        "\n",
        "This implies that the next state depends only on the current state and action.\n",
        "\n",
        "## Policy\n",
        "\n",
        "A **policy** $\\pi$ is a mapping from states to probabilities of selecting each possible action:\n",
        "\n",
        "$$\n",
        "\\pi(a|s) = P(a_t = a | s_t = s)\n",
        "$$\n",
        "\n",
        "A **deterministic policy** selects actions with certainty: $\\pi(s) = a$.\n",
        "\n",
        "## Reward Function\n",
        "\n",
        "The reward function $R(s, a)$ defines the expected immediate reward after taking action $a$ in state $s$. Alternatively, it may depend on the transition:\n",
        "\n",
        "$$\n",
        "R(s, a, s') = \\mathbb{E}[r_{t+1} | s_t = s, a_t = a, s_{t+1} = s']\n",
        "$$\n",
        "\n",
        "## Value Function\n",
        "\n",
        "The **state-value function** under policy $\\pi$ is:\n",
        "\n",
        "$$\n",
        "V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r_{t+1} \\big| s_0 = s \\right]\n",
        "$$\n",
        "\n",
        "The **action-value function** under policy $\\pi$ is:\n",
        "\n",
        "$$\n",
        "Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r_{t+1} \\big| s_0 = s, a_0 = a \\right]\n",
        "$$\n",
        "\n",
        "## Bellman Equations\n",
        "\n",
        "The Bellman equation for the state-value function under policy $\\pi$ is:\n",
        "\n",
        "$$\n",
        "V^\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s'} P(s'|s, a) [R(s, a) + \\gamma V^\\pi(s')]\n",
        "$$\n",
        "\n",
        "For the optimal value function $V^*(s)$:\n",
        "\n",
        "$$\n",
        "V^*(s) = \\max_{a} \\sum_{s'} P(s'|s, a) [R(s, a) + \\gamma V^*(s')]\n",
        "$$\n",
        "\n",
        "## Optimal Policy\n",
        "\n",
        "An optimal policy $\\pi^*$ achieves the maximum expected return from every state:\n",
        "\n",
        "$$\n",
        "\\pi^*(s) = \\arg\\max_a Q^*(s, a)\n",
        "$$\n",
        "\n",
        "Where $Q^*(s, a)$ satisfies:\n",
        "\n",
        "$$\n",
        "Q^*(s, a) = \\sum_{s'} P(s'|s, a) [R(s, a) + \\gamma \\max_{a'} Q^*(s', a')]\n",
        "$$\n",
        "\n",
        "## Summary\n",
        "\n",
        "An MDP provides a mathematical framework for modeling environments in reinforcement learning. It supports the formulation of policies, value functions, and optimal decision-making by exploiting the Markov property and expected rewards through recursive Bellman equations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function Approximation in Reinforcement Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In reinforcement learning (RL), **function approximation** is used when the state or action space is too large or continuous to represent the value function or policy as a table. Instead, we use parameterized functions to approximate these mappings.\n",
        "\n",
        "Function approximation helps generalize the learning across similar states or actions, which improves scalability and efficiency.\n",
        "\n",
        "## Types of Function Approximation\n",
        "\n",
        "### Value Function Approximation\n",
        "\n",
        "The goal is to approximate the state-value function $V(s)$ or action-value function $Q(s, a)$ using a parameterized function:\n",
        "\n",
        "$$\n",
        "\\hat{V}(s, \\mathbf{w}) \\approx V^\\pi(s)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{Q}(s, a, \\mathbf{w}) \\approx Q^\\pi(s, a)\n",
        "$$\n",
        "\n",
        "where $\\mathbf{w}$ is a vector of learnable parameters.\n",
        "\n",
        "### Policy Function Approximation\n",
        "\n",
        "In policy-based methods, the policy $\\pi(a|s)$ is also parameterized:\n",
        "\n",
        "$$\n",
        "\\pi(a|s, \\boldsymbol{\\theta}) \\approx \\pi^*(a|s)\n",
        "$$\n",
        "\n",
        "where $\\boldsymbol{\\theta}$ are the parameters to optimize.\n",
        "\n",
        "## Linear Function Approximation\n",
        "\n",
        "A linear function approximation of the value function is given by:\n",
        "\n",
        "$$\n",
        "\\hat{V}(s, \\mathbf{w}) = \\mathbf{w}^T \\mathbf{x}(s)\n",
        "$$\n",
        "\n",
        "Here, $\\mathbf{x}(s)$ is a feature vector for state $s$, and $\\mathbf{w}$ is the weight vector.\n",
        "\n",
        "## Non-Linear Function Approximation\n",
        "\n",
        "Neural networks are used as non-linear function approximators. For example, a neural network can approximate $Q(s, a)$ in Deep Q-Learning.\n",
        "\n",
        "$$\n",
        "\\hat{Q}(s, a; \\mathbf{w}) = \\text{NN}(s, a; \\mathbf{w})\n",
        "$$\n",
        "\n",
        "where NN denotes a neural network with weights $\\mathbf{w}$.\n",
        "\n",
        "## Objective Function\n",
        "\n",
        "The common objective is to minimize the mean squared error between the true and predicted values:\n",
        "\n",
        "$$\n",
        "J(\\mathbf{w}) = \\mathbb{E} \\left[ \\left( v^\\pi(s) - \\hat{v}(s, \\mathbf{w}) \\right)^2 \\right]\n",
        "$$\n",
        "\n",
        "Gradient descent is used to update the weights:\n",
        "\n",
        "$$\n",
        "\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left( v^\\pi(s) - \\hat{v}(s, \\mathbf{w}) \\right) \\nabla_{\\mathbf{w}} \\hat{v}(s, \\mathbf{w})\n",
        "$$\n",
        "\n",
        "## Bootstrapping Targets\n",
        "\n",
        "If the true value $v^\\pi(s)$ is unknown, we use bootstrapping techniques like Temporal Difference (TD) learning to estimate it:\n",
        "\n",
        "$$\n",
        "v^\\pi(s) \\approx r + \\gamma \\hat{v}(s', \\mathbf{w})\n",
        "$$\n",
        "\n",
        "This gives the update rule:\n",
        "\n",
        "$$\n",
        "\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left( r + \\gamma \\hat{v}(s', \\mathbf{w}) - \\hat{v}(s, \\mathbf{w}) \\right) \\nabla_{\\mathbf{w}} \\hat{v}(s, \\mathbf{w})\n",
        "$$\n",
        "\n",
        "## Benefits of Function Approximation\n",
        "\n",
        "- Reduces memory requirements\n",
        "- Enables generalization to unseen states\n",
        "- Handles continuous and large discrete spaces\n",
        "\n",
        "## Challenges\n",
        "\n",
        "- May introduce approximation error\n",
        "- Convergence is not guaranteed with non-linear functions\n",
        "- Requires careful tuning of features or architecture\n",
        "\n",
        "## Summary\n",
        "\n",
        "Function approximation allows reinforcement learning to scale to complex environments by estimating value functions and policies with parameterized functions. Linear models offer simplicity, while neural networks provide powerful non-linear mappings used in deep reinforcement learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Q-Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Deep Q-Learning is a value-based reinforcement learning algorithm that combines Q-Learning with deep neural networks. It approximates the optimal action-value function $Q^*(s, a)$ using a deep neural network instead of a Q-table, enabling it to handle high-dimensional and continuous state spaces.\n",
        "\n",
        "## Traditional Q-Learning Recap\n",
        "\n",
        "In standard Q-Learning, the agent updates the Q-value using the Bellman equation:\n",
        "\n",
        "$$\n",
        "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $s$ is the current state\n",
        "- $a$ is the current action\n",
        "- $r$ is the reward received\n",
        "- $s'$ is the next state\n",
        "- $\\gamma$ is the discount factor\n",
        "- $\\alpha$ is the learning rate\n",
        "\n",
        "## Motivation for Deep Q-Learning\n",
        "\n",
        "Traditional Q-Learning becomes infeasible when the state-action space is large. Deep Q-Learning uses a neural network to approximate $Q(s, a; \\theta)$, where $\\theta$ are the parameters (weights) of the network.\n",
        "\n",
        "## Deep Q-Network (DQN)\n",
        "\n",
        "A Deep Q-Network (DQN) is trained to minimize the mean squared error loss:\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\mathbb{E}_{(s, a, r, s')} \\left[ \\left( y - Q(s, a; \\theta) \\right)^2 \\right]\n",
        "$$\n",
        "\n",
        "where the target $y$ is given by:\n",
        "\n",
        "$$\n",
        "y = r + \\gamma \\max_{a'} Q(s', a'; \\theta^{-})\n",
        "$$\n",
        "\n",
        "Here, $\\theta^{-}$ represents the parameters of a **target network**, which is periodically updated to stabilize learning.\n",
        "\n",
        "## Experience Replay\n",
        "\n",
        "To improve training stability, Deep Q-Learning uses **experience replay**:\n",
        "- Transitions $(s, a, r, s')$ are stored in a replay buffer.\n",
        "- Mini-batches are sampled randomly to break correlation between sequential experiences.\n",
        "\n",
        "## Target Network\n",
        "\n",
        "A separate target network with parameters $\\theta^{-}$ is used to compute the target Q-value $y$. This network is updated with the main network’s weights periodically:\n",
        "\n",
        "$$\n",
        "\\theta^{-} \\leftarrow \\theta\n",
        "$$\n",
        "\n",
        "This prevents rapid oscillations in target values and improves convergence.\n",
        "\n",
        "## Algorithm Overview\n",
        "\n",
        "1. Initialize replay memory and Q-network with random weights $\\theta$.\n",
        "2. Initialize target network with weights $\\theta^{-} = \\theta$.\n",
        "3. For each step:\n",
        "   - Select action $a$ using an $\\epsilon$-greedy policy.\n",
        "   - Execute action, observe reward $r$ and next state $s'$.\n",
        "   - Store transition $(s, a, r, s')$ in replay buffer.\n",
        "   - Sample random mini-batch from buffer.\n",
        "   - Compute target $y = r + \\gamma \\max_{a'} Q(s', a'; \\theta^{-})$.\n",
        "   - Update $\\theta$ using gradient descent on loss $L(\\theta)$.\n",
        "   - Periodically update target network.\n",
        "\n",
        "## $\\epsilon$-Greedy Policy\n",
        "\n",
        "To balance exploration and exploitation, an $\\epsilon$-greedy policy is used:\n",
        "\n",
        "$$\n",
        "\\text{Choose random action with probability } \\epsilon\n",
        "$$\n",
        "$$\n",
        "\\text{Choose } \\arg\\max_a Q(s, a; \\theta) \\text{ with probability } 1 - \\epsilon\n",
        "$$\n",
        "\n",
        "$\\epsilon$ is often annealed from 1 to a small minimum value over time.\n",
        "\n",
        "## Loss Function and Gradient Descent\n",
        "\n",
        "The gradient of the loss function is used to update the weights:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta} L(\\theta) = \\left( y - Q(s, a; \\theta) \\right) \\nabla_{\\theta} Q(s, a; \\theta)\n",
        "$$\n",
        "\n",
        "The network is trained using stochastic gradient descent (SGD) or Adam.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Deep Q-Learning scales Q-Learning to large or continuous state spaces by using a neural network to approximate Q-values. Key innovations include:\n",
        "- Deep Q-Network for function approximation\n",
        "- Experience replay to break correlation\n",
        "- Target network to stabilize training\n",
        "- $\\epsilon$-greedy policy for exploration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# REINFORCE Algorithm\n",
        "\n",
        "## Introduction\n",
        "\n",
        "REINFORCE is a Monte Carlo policy gradient method used in reinforcement learning. It directly optimizes the policy by estimating gradients of expected return with respect to the policy parameters. Unlike value-based methods, REINFORCE works well in environments with large or continuous action spaces.\n",
        "\n",
        "## Objective of Policy Optimization\n",
        "\n",
        "Let $\\pi_\\theta(a|s)$ be the parameterized policy. The goal is to maximize the expected cumulative reward:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ R(\\tau) \\right]\n",
        "$$\n",
        "\n",
        "where $\\tau$ is a trajectory sampled from the policy and $R(\\tau)$ is the total return from the trajectory.\n",
        "\n",
        "## Gradient of the Objective\n",
        "\n",
        "The gradient of the expected return can be written as:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot R \\right]\n",
        "$$\n",
        "\n",
        "This is known as the likelihood ratio trick. It allows us to compute gradients without knowing the gradient of the environment dynamics.\n",
        "\n",
        "## Return Estimation\n",
        "\n",
        "In episodic settings, the return $R_t$ at time step $t$ is computed as the sum of future discounted rewards:\n",
        "\n",
        "$$\n",
        "R_t = \\sum_{k=0}^{T-t} \\gamma^k r_{t+k}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\gamma$ is the discount factor\n",
        "- $T$ is the terminal time step\n",
        "\n",
        "## REINFORCE Algorithm Steps\n",
        "\n",
        "1. Initialize policy parameters $\\theta$\n",
        "2. For each episode:\n",
        "   - Generate a trajectory $\\tau = (s_0, a_0, r_0, \\dots, s_T, a_T, r_T)$\n",
        "   - For each time step $t$:\n",
        "     - Compute return $R_t$\n",
        "     - Compute gradient $\\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot R_t$\n",
        "   - Update parameters using gradient ascent:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)\n",
        "$$\n",
        "\n",
        "## Baseline for Variance Reduction\n",
        "\n",
        "To reduce the variance of the gradient estimate, a baseline $b(s)$ is subtracted from the return:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot (R - b(s)) \\right]\n",
        "$$\n",
        "\n",
        "A common choice for $b(s)$ is the value function $V^\\pi(s)$.\n",
        "\n",
        "## Advantages\n",
        "\n",
        "- Works with continuous and stochastic policies\n",
        "- Easy to implement and understand\n",
        "- Compatible with deep neural networks as function approximators\n",
        "\n",
        "## Disadvantages\n",
        "\n",
        "- High variance in gradient estimates\n",
        "- Requires full episodes to compute returns\n",
        "- Often slower to converge than actor-critic methods\n",
        "\n",
        "## Summary\n",
        "\n",
        "REINFORCE is a fundamental policy gradient method that estimates the gradient of expected return using the log-probability of actions and observed rewards. Though simple, it forms the basis for more advanced algorithms such as Actor-Critic and PPO.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Actor-Critic Algorithm in Reinforcement Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The Actor-Critic algorithm combines the advantages of value-based and policy-based methods in reinforcement learning. It consists of two main components:\n",
        "\n",
        "- Actor: updates the policy parameters in the direction suggested by the Critic\n",
        "- Critic: estimates the value function to evaluate the chosen actions\n",
        "\n",
        "This structure enables efficient learning with reduced variance and improved stability.\n",
        "\n",
        "## Objective\n",
        "\n",
        "Let $\\pi_\\theta(a|s)$ be a stochastic policy and $V_w(s)$ the value function. The objective is to maximize the expected return:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\right]\n",
        "$$\n",
        "\n",
        "## Policy Gradient with Critic\n",
        "\n",
        "The gradient of the policy's objective function using the Critic is:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot A^{\\pi}(s_t, a_t) \\right]\n",
        "$$\n",
        "\n",
        "Here, $A^{\\pi}(s, a)$ is the advantage function, defined as:\n",
        "\n",
        "$$\n",
        "A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s)\n",
        "$$\n",
        "\n",
        "If the true advantage is not available, it can be estimated using the temporal-difference (TD) error:\n",
        "\n",
        "$$\n",
        "\\delta_t = r_t + \\gamma V_w(s_{t+1}) - V_w(s_t)\n",
        "$$\n",
        "\n",
        "This TD error serves as a proxy for the advantage function.\n",
        "\n",
        "## Actor Update Rule\n",
        "\n",
        "The Actor updates the policy parameters using:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\n",
        "$$\n",
        "\n",
        "## Critic Update Rule\n",
        "\n",
        "The Critic updates the value function parameters $w$ by minimizing the squared TD error:\n",
        "\n",
        "$$\n",
        "w \\leftarrow w - \\beta \\nabla_w \\left( \\delta_t^2 \\right)\n",
        "$$\n",
        "\n",
        "## Training Steps\n",
        "\n",
        "1. Initialize parameters $\\theta$ for the Actor and $w$ for the Critic\n",
        "2. Observe current state $s_t$\n",
        "3. Choose action $a_t \\sim \\pi_\\theta(a_t|s_t)$\n",
        "4. Receive reward $r_t$ and next state $s_{t+1}$\n",
        "5. Compute TD error:\n",
        "\n",
        "$$\n",
        "\\delta_t = r_t + \\gamma V_w(s_{t+1}) - V_w(s_t)\n",
        "$$\n",
        "\n",
        "6. Update Critic: minimize $\\delta_t^2$\n",
        "7. Update Actor: apply policy gradient using $\\delta_t$\n",
        "\n",
        "## Advantages\n",
        "\n",
        "- Combines benefits of policy gradients and value function estimation\n",
        "- Lower variance than REINFORCE due to use of TD learning\n",
        "- Works in continuous action spaces\n",
        "\n",
        "## Disadvantages\n",
        "\n",
        "- More complex to implement due to dual components\n",
        "- Potential instability from interaction between Actor and Critic\n",
        "- Sensitive to hyperparameters\n",
        "\n",
        "## Summary\n",
        "\n",
        "The Actor-Critic framework introduces a Critic to estimate value functions and guide policy updates. It improves learning efficiency and convergence stability compared to pure policy gradient methods, and is foundational for advanced algorithms like A2C, A3C, and PPO.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Proximal Policy Optimization (PPO)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Proximal Policy Optimization (PPO) is a policy gradient method in reinforcement learning that improves stability and reliability of training by restricting policy updates. It uses a surrogate objective to avoid large, destabilizing updates.\n",
        "\n",
        "PPO is widely used in deep reinforcement learning due to its simplicity, efficiency, and good empirical performance.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Standard policy gradient methods like REINFORCE and Actor-Critic suffer from high variance and instability. PPO addresses these by introducing a mechanism to limit how much the policy is updated during training, preventing performance collapse.\n",
        "\n",
        "## Surrogate Objective\n",
        "\n",
        "Let the old policy be $\\pi_{\\theta_{\\text{old}}}$ and the current policy be $\\pi_\\theta$. The probability ratio is:\n",
        "\n",
        "$$\n",
        "r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}\n",
        "$$\n",
        "\n",
        "The surrogate objective without clipping is:\n",
        "\n",
        "$$\n",
        "L^{\\text{PG}}(\\theta) = \\mathbb{E}_t \\left[ r_t(\\theta) \\cdot A_t \\right]\n",
        "$$\n",
        "\n",
        "where $A_t$ is the advantage estimate at time $t$.\n",
        "\n",
        "## Clipped Surrogate Objective\n",
        "\n",
        "To prevent $r_t(\\theta)$ from diverging too far from 1 (i.e., large policy shifts), PPO introduces a clipped version of the objective:\n",
        "\n",
        "$$\n",
        "L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\left[ \\min\\left( r_t(\\theta) A_t,\\ \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t \\right) \\right]\n",
        "$$\n",
        "\n",
        "The clipping parameter $\\epsilon$ is typically a small number like 0.1 or 0.2. This ensures that the update stays within a safe region.\n",
        "\n",
        "## Value Function and Entropy Bonus\n",
        "\n",
        "The full PPO objective also includes a value function loss and an entropy bonus to encourage exploration:\n",
        "\n",
        "$$\n",
        "L^{\\text{PPO}}(\\theta) = \\mathbb{E}_t \\left[ L^{\\text{CLIP}}(\\theta) - c_1 \\cdot (V_\\theta(s_t) - V_t)^2 + c_2 \\cdot \\mathcal{H}[\\pi_\\theta](s_t) \\right]\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $V_\\theta(s_t)$ is the predicted value function\n",
        "- $V_t$ is the estimated return\n",
        "- $\\mathcal{H}[\\pi_\\theta]$ is the entropy of the policy\n",
        "- $c_1$, $c_2$ are coefficients for the value loss and entropy bonus\n",
        "\n",
        "## Algorithm Overview\n",
        "\n",
        "1. Collect trajectories using current policy $\\pi_\\theta$\n",
        "2. Estimate advantages $A_t$ and returns $V_t$\n",
        "3. Compute clipped surrogate loss $L^{\\text{CLIP}}(\\theta)$\n",
        "4. Perform multiple epochs of minibatch updates on $\\theta$\n",
        "5. Repeat\n",
        "\n",
        "## Advantages\n",
        "\n",
        "- Stable training due to bounded updates\n",
        "- Simpler implementation than Trust Region Policy Optimization (TRPO)\n",
        "- Good empirical performance on many continuous control tasks\n",
        "\n",
        "## Summary\n",
        "\n",
        "Proximal Policy Optimization optimizes a clipped surrogate objective to keep policy updates close to the previous policy. This approach balances exploration and stability, and avoids performance degradation from overly aggressive updates. PPO is a foundational algorithm in modern policy-based reinforcement learning.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
