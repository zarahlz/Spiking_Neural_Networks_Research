{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1OVRxxmKdYu"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://statics.maktabkhooneh.org/front/images/maktabkhooneh/logo.png\" width=\"120\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <font size=\"8\"><b>Computational Neuroscience Summary</b></font><br>\n",
        "  <font size=\"5\"><i>Compact Theories, Core Mechanisms, and Mathematical Foundations for Accelerated Understanding</i></font><br><br>\n",
        "  <font size=\"4\"><b>Author: Zahra Helalizadeh</b></font>\n",
        "</p>\n",
        "\n",
        "<blockquote>\n",
        "  This notebook provides a structured and high-efficiency overview of the Maktabkhooneh Computational Neuroscience course.  \n",
        "  It highlights core ideas, essential neuron models, biological insights, and key mathematical frameworks.\n",
        "</blockquote>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYHyKQ7hdTxS"
      },
      "source": [
        "# **Table of Content**\n",
        "\n",
        "## Introduction and Foundations\n",
        "\n",
        "1. **Introduction to Computational Neuroscience**  \n",
        "2. **What is Intelligence?**  \n",
        "3. **Introduction to Neuroscience**  \n",
        "4. **Biological Neural Systems**  \n",
        "5. **Neuron and Its Function**  \n",
        "6. **Synapse and Its Function**  \n",
        "7. **Introduction to Modeling and Its Levels**  \n",
        "\n",
        "## Neuron Modeling and Dynamics\n",
        "\n",
        "8. **Neuronal Dynamics**  \n",
        "9. **Neuron Modeling**  \n",
        "10. **Passive Neuronal Membrane**  \n",
        "11. **Leaky Integrate-and-Fire (LIF) Neuron Model**  \n",
        "12. **Nonlinear LIF Neuron Model**  \n",
        "13. **Adaptive LIF Neuron Model**  \n",
        "14. **Hodgkin-Huxley Neuron Model**  \n",
        "\n",
        "## Neural Populations and Networks\n",
        "\n",
        "15. **Neural Populations**  \n",
        "16. **Balanced Neural Network**  \n",
        "17. **Decision-Making Processes in the Brain**  \n",
        "18. **Neural Encoding and Decoding**  \n",
        "19. **Spiking Neural Networks (SNNs)**  \n",
        "20. **Learning and Plasticity**  \n",
        "\n",
        "## Learning Patterns and Spiking Neurons\n",
        "\n",
        "21. **Unsupervised Learning and Its Rules**  \n",
        "22. **Reinforcement Learning ND Its Rules**  \n",
        "23. **Polychronous Neuron Groups**  \n",
        "24. **Visual System in the Brain**  \n",
        "25. **Retina and Lateral Geniculate Nucleus (LGN)**  \n",
        "26. **Visual Cortex Function**  \n",
        "27. **Modeling Levels in Primary Visual Cortex**  \n",
        "28. **Neural Connectivity Patterns in Brain Regions**  \n",
        "\n",
        "## HMAX Model and Deep SNNs\n",
        "\n",
        "29. **HMAX Computational Model for Object Recognition**  \n",
        "30. **Spiking HMAX Model with Learning**  \n",
        "31. **Shallow Spiking Neural Networks**  \n",
        "32. **Deep Spiking Neural Networks**  \n",
        "33. **Reinforcement Learning in Shallow Spiking Neural Networks**  \n",
        "34. **Reinforcement Learning in Deep Spiking Neural Networks**  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy_Gkw6RYwQ1"
      },
      "source": [
        "## **1. Introduction to Computational Neuroscience**  \n",
        "\n",
        "### Foundational Concepts and Debates\n",
        "\n",
        "This course explores the interdisciplinary field of computational neuroscience, integrating principles from biology, mathematics, and computer science to model neural systems. Topics span neural dynamics, learning mechanisms, brain imaging, and computational vision. Course evaluation includes problem sets, modeling projects, and exams.  \n",
        "\n",
        "#### Core Course Topics  \n",
        "**1. Fundamentals of Neuroscience**  \n",
        "- Biological basis of neurons and synapses.  \n",
        "- Brain anatomy and functional areas.  \n",
        "- Neural dynamics and modeling principles.  \n",
        "\n",
        "**2. Single Neuron Models**  \n",
        "- **Leaky Integrate-and-Fire (LIF)**:  \n",
        "  $\\tau_m \\frac{dV}{dt} = -(V - V_{\\text{rest}}) + R_m I(t)$  \n",
        "  (where $\\tau_m$ = membrane time constant, $V$ = membrane potential, $I$ = input current).  \n",
        "- **Hodgkin-Huxley (HH) Model**:  \n",
        "  $C_m \\frac{dV}{dt} = I_{\\text{ext}} - g_{\\text{Na}} m^3 h (V - E_{\\text{Na}}) - g_{\\text{K}} n^4 (V - E_{\\text{K}}) - g_L (V - E_L)$  \n",
        "  (with ion conductances $g_{\\text{Na}}$, $g_{\\text{K}}$, activation variables $m, n$, inactivation $h$).  \n",
        "\n",
        "**3. Neural Coding and Learning**  \n",
        "- **Neural encoding/decoding**: Tuning curves, spike-triggered averaging.  \n",
        "- **Synaptic plasticity**:  \n",
        "  - *Hebbian learning*: $\\Delta w_{ij} = \\eta x_i y_j$  \n",
        "  - *STDP*: $\\Delta w = \\begin{cases} A_+ e^{-\\Delta t / \\tau_+} & \\Delta t > 0 \\\\ A_- e^{\\Delta t / \\tau_-} & \\Delta t < 0 \\end{cases}$  \n",
        "  ($\\Delta t$ = pre/post spike time difference).  \n",
        "- Supervised, unsupervised, and reinforcement learning frameworks.  \n",
        "\n",
        "**4. Network Models and Cognition**  \n",
        "- Spiking neural networks (SNNs).  \n",
        "- Decision-making models (e.g., drift-diffusion).  \n",
        "- Polychronization: Time-locked spiking patterns.  \n",
        "\n",
        "**5. Brain Imaging and Vision**  \n",
        "- Techniques: fMRI, EEG, single-cell recording.  \n",
        "- **Visual system**:  \n",
        "  - Eye anatomy and visual pathways (dorsal/ventral streams).  \n",
        "  - Computational models:  \n",
        "    - *V1-like filters*: Gabor functions $G(x,y) = \\exp\\left(-\\frac{x'^2 + \\gamma^2 y'^2}{2\\sigma^2}\\right) \\cos(2\\pi f x')$.  \n",
        "    - Deep networks (CNNs, VisNet).  \n",
        "\n",
        "**6. Specialized Topics**  \n",
        "- Advanced neural architectures (e.g., HMAX, NeoCognitron).  \n",
        "- Emerging research in neural computation.  \n",
        "\n",
        "#### Key References  \n",
        "1. Gerstner et al. (2014) *Neural Dynamics*.  \n",
        "2. Miller (2018) *Introductory Course in Computational Neuroscience*.  \n",
        "3. Eliasmith (2013) *How to Build a Brain*.  \n",
        "4. Abbott et al. (2001) *Theoretical Neuroscience*.  \n",
        "5. Stone et al. (2010) *Seeing: Computational Approach to Vision*.  \n",
        "6. Snowden et al. (2012) *Basic Vision*.  \n",
        "7. Supplementary research papers.  \n",
        "\n",
        "> Note: Formulas use standard notation: $V$ (membrane potential), $I$ (current), $\\tau$ (time constants), $w_{ij}$ (synaptic weights), $\\eta$ (learning rate)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udGbEZNLN2cv"
      },
      "source": [
        "## **2. What is Intelligence?**  \n",
        "### Conceptual Frameworks and Debates\n",
        "\n",
        "#### The Definition Problem  \n",
        "No universal definition exists. A 1921 survey of 14 experts yielded divergent perspectives:  \n",
        "- Abstract thinking (Terman)  \n",
        "- Adaptive learning to environmental demands (Colvin)  \n",
        "- Biological unification of stimuli into behavior (Peterson)  \n",
        "- Meta-capacity to acquire capacities (Woodrow)  \n",
        "- Learning from experience (Dearborn)  \n",
        "\n",
        "#### Core Attributes in Commonsense Understanding  \n",
        "Intelligence is commonly associated with:  \n",
        "1. Logical reasoning and problem-solving  \n",
        "2. Learning mechanisms and memory systems  \n",
        "3. Language processing and communication  \n",
        "4. Intuitive creativity and consciousness  \n",
        "5. Emotional regulation and social cognition  \n",
        "6. Survival adaptation in complex environments  \n",
        "7. Sensorimotor integration and perceptual acuity  \n",
        "The neurocomputational bases for these traits remain partially unknown.  \n",
        "\n",
        "#### Functional Purpose of Intelligence  \n",
        "Natural intelligence serves biological imperatives:  \n",
        "- Evolved for survival optimization, not abstract computation  \n",
        "- Always expressed through observable behavior  \n",
        "- Requires physical instantiation (*embodied cognition*)  \n",
        "Example: Drinking water integrates thirst detection, motor planning, and environmental navigation – a sensorimotor computation chain.  \n",
        "\n",
        "#### Continuum of Cognitive Capabilities  \n",
        "Hierarchical manifestations across species:  \n",
        "- **Bacteria**: Chemotactic response gradients  \n",
        "- **Ants/Colonies**: Swarm optimization via pheromone signaling  \n",
        "- **Mice/Cats**: Spatial memory ($x(t+1) = f(x(t), s(t))$ where $s$ = sensory input)  \n",
        "- **Humans**: Symbolic reasoning and theory of mind  \n",
        "- **Machines**: Task-specific optimization (e.g., $\\min_\\theta \\mathcal{L}(y, \\hat{y})$)\n",
        "\n",
        "#### Computational Neuroscience Roadmap  \n",
        "Three-stage research paradigm:  \n",
        "1. **Reverse-engineering**: Study neurobiological systems to extract computational principles  \n",
        "2. **Feature abstraction**: Formalize neural algorithms (e.g., $W_{ij} = \\eta \\sum_t x_i(t) y_j(t)$ for Hebbian plasticity)  \n",
        "3. **Model implementation**: Construct biomimetic architectures (SNNs, predictive coding models)  \n",
        "\n",
        "*Note: Intelligence is contextually embedded – a bacterium's nutrient foraging and AlphaZero's game strategy both represent adaptive problem-solving within their operational constraints.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zajrli0gQi7B"
      },
      "source": [
        "## **3. Introduction to Neuroscience**  \n",
        "###Foundations and Computational Approaches\n",
        "\n",
        "#### Defining the Disciplines  \n",
        "Neuroscience is the scientific study of the nervous system, integrating biology, chemistry, computer science, engineering, mathematics, medicine, philosophy, physics, and psychology. Computational neuroscience aims to explain how brains generate behavior using computational frameworks.  \n",
        "\n",
        "#### Core Objectives of Computational Neuroscience  \n",
        "1. **Descriptive Models**: Characterize neural system functions  \n",
        "   - Neural encoding: Quantify stimulus-response relationships ($r(t) = f(s(t))$ where $r$ = neural response, $s$ = stimulus)  \n",
        "   - Neural decoding: Reconstruct stimuli from neural activity ($\\hat{s}(t) = g(r(t))$)  \n",
        "2. **Mechanistic Models**: Simulate biological processes  \n",
        "   - Single neuron dynamics (e.g., Hodgkin-Huxley: $C_m \\frac{dV}{dt} = I_{\\text{ext}} - \\sum I_{\\text{ion}}$)  \n",
        "   - Network interactions (e.g., $\\tau \\frac{dr_i}{dt} = -r_i + \\sum_j W_{ij} r_j + I_i$)  \n",
        "3. **Interpretive Models**: Uncover computational principles  \n",
        "   - Optimization of neural circuits (e.g., $\\min_{W} \\mathcal{E}(x, \\hat{x})$ for efficient coding)  \n",
        "   - Evolutionary constraints on information processing  \n",
        "\n",
        "#### Methodological Framework  \n",
        "Computational neuroscience employs:  \n",
        "- **Advanced data analysis**: Extract patterns from neural recordings (e.g., dimensionality reduction $X = U\\Sigma V^T$)  \n",
        "- **Dynamical systems theory**: Model state transitions ($\\dot{\\mathbf{x}} = F(\\mathbf{x}, t)$)  \n",
        "- **Statistical inference**: Reverse-engineer neural computation (e.g., Bayesian decoding $P(s|\\mathbf{r}) \\propto P(\\mathbf{r}|s)P(s)$)  \n",
        "- **Theoretical synthesis**:  \n",
        "  - Identify minimal sufficient mechanisms (e.g., integrate-and-fire neurons capturing threshold phenomena)  \n",
        "  - Discover universal computational motifs (e.g., recurrent excitation/inhibition balancing)  \n",
        "\n",
        "#### Modeling Hierarchy  \n",
        "| Level                  | Key Questions                                                                 |\n",
        "|------------------------|-------------------------------------------------------------------------------|\n",
        "| Descriptive            | How do neurons encode stimuli? How to decode neural signals?                  |\n",
        "| Mechanistic            | How to simulate neurons/networks? What biophysical parameters govern dynamics? |\n",
        "| Interpretive (Normative)| Why do circuits use specific architectures? What principles optimize function? |  \n",
        "\n",
        "> Note: $W_{ij}$ denotes synaptic weights, $I_{\\text{ext}}$ external current, $C_m$ membrane capacitance, $\\mathcal{E}$ error function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KzWQoSCSi53"
      },
      "source": [
        "## **4. Biological Neural Systems**  \n",
        "###Organization and Computational Principles\n",
        "\n",
        "#### Structural Hierarchy  \n",
        "The nervous system comprises two primary divisions:  \n",
        "1. **Central Nervous System (CNS)**: Brain and spinal cord  \n",
        "   - Spinal cord functions:  \n",
        "     - Reflex loops (local feedback)  \n",
        "     - Descending motor control ($\\text{brain} \\rightarrow \\text{motor neurons}$)  \n",
        "     - Ascending sensory pathways ($\\text{periphery} \\rightarrow \\text{brain}$)  \n",
        "2. **Peripheral Nervous System (PNS)**:  \n",
        "   - **Somatic division**: Voluntary control  \n",
        "     - Afferent fibers: Sensory input to CNS ($I_{\\text{sensory}} = \\sum \\delta(t - t_{\\text{spike}})$)  \n",
        "     - Efferent fibers: Motor output to skeletal muscles  \n",
        "   - **Autonomic division**: Involuntary regulation  \n",
        "     - Controls cardiac/smooth muscle, glands, adipose tissue  \n",
        "\n",
        "#### Information Processing Framework  \n",
        "- **Sensory encoding**:  \n",
        "  - Somatic receptors: Position, touch, pain, temperature  \n",
        "  - Special receptors: Vision, audition, olfaction, gustation  \n",
        "  - Visceral receptors: Internal organ monitoring  \n",
        "- **CNS integration**:  \n",
        "  - Spatial summation: $V_{\\text{post}} = \\sum_i w_i I_i$  \n",
        "  - Temporal dynamics: $\\tau \\frac{dx}{dt} = -x + f_{\\text{input}}$  \n",
        "- **Motor output**:  \n",
        "  - Somatic pathway: $\\text{CNS} \\xrightarrow{\\text{efferent}} \\text{skeletal muscles}$  \n",
        "  - Autonomic pathway: Homeostatic control loops  \n",
        "\n",
        "#### Neural vs. Digital Computing  \n",
        "| Property             | Biological Neural Systems               | Digital Computers               |  \n",
        "|----------------------|-----------------------------------------|---------------------------------|  \n",
        "| **Scale**            | $10^{11}$ neurons, $10^{15}$ synapses   | $10^{10}$ transistors           |  \n",
        "| **Connectivity**     | Dense ($\\sim 10^4$ links/neuron)        | Sparse (fanout $\\sim 10^2$)     |  \n",
        "| **Speed**            | $\\sim 10^2$ Hz ($\\Delta t \\approx 10^{-4}$ s) | $\\sim 10^{10}$ Hz             |  \n",
        "| **Architecture**     | Parallel analog computation              | Sequential digital processing    |  \n",
        "| **Adaptivity**       | Plastic synapses ($\\Delta w_{ij} = f(t_{\\text{pre}}, t_{\\text{post}})$) | Fixed circuits               |  \n",
        "| **Specialization**   | Ill-posed problems (vision, speech)     | Symbolic/mathematical operations |  \n",
        "\n",
        "#### Functional Principles  \n",
        "- **Receptor-operator dynamics**:  \n",
        "  Sensory transduction: $r(t) = g(s(t)) + \\epsilon$ (with noise $\\epsilon$)  \n",
        "- **Effector control**:  \n",
        "  Muscle activation: $F_{\\text{muscle}} = k \\cdot \\text{spike_rate}$  \n",
        "- **Energy efficiency**:  \n",
        "  Biological systems optimize $\\frac{\\text{computational power}}{\\text{energy}}$ ratio ($\\sim 10^{17}$ FLOPs/J vs. $10^{12}$ FLOPs/J for silicon)  \n",
        "\n",
        "> Note: $w_i$ denotes synaptic weights, $\\tau$ time constants, $s(t)$ sensory stimuli, $r(t)$ neural response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPAdJi64Vddm"
      },
      "source": [
        "## **5. Neuron and its Function**  \n",
        "### Structural and Electrophysiological Properties  \n",
        "\n",
        "#### Morphological Components  \n",
        "1. **Dendrites**: Receive synaptic inputs; perform spatial integration ($V_{\\text{dend}} = \\sum w_i I_i$)  \n",
        "2. **Soma (Cell body)**: Integrates dendritic inputs; initiates action potentials if threshold is exceeded ($V_{\\text{soma}} > \\theta$)  \n",
        "3. **Axon**: Conducts electrical impulses (action potentials)  \n",
        "   - Myelin sheath: Insulating layer enabling saltatory conduction (signal jumps between Nodes of Ranvier)  \n",
        "4. **Terminal buttons**: Release neurotransmitters into synaptic clefts  \n",
        "\n",
        "#### Bioelectric Phenomena  \n",
        "- **Resting potential**: Steady-state voltage ($V_{\\text{rest}} \\approx -70 \\text{ mV}$) maintained by K⁺ leak channels and Na⁺/K⁺ pumps  \n",
        "- **Action potential dynamics**:  \n",
        "  1. *Depolarization*: Voltage-gated Na⁺ channels open ($g_{\\text{Na}} \\uparrow \\rightarrow V_m \\nearrow +30 \\text{ mV}$)  \n",
        "  2. *Repolarization*: K⁺ channels open, Na⁺ channels inactivate ($g_{\\text{K}} \\uparrow \\rightarrow V_m \\searrow$)  \n",
        "  3. *Hyperpolarization*: Transient undershoot below $V_{\\text{rest}}$  \n",
        "\n",
        "#### Ion Channel Mechanisms  \n",
        "| Channel Type         | Gating Mechanism                  | Primary Ions      |  \n",
        "|----------------------|-----------------------------------|-------------------|  \n",
        "| Voltage-gated        | Membrane potential change         | Na⁺, K⁺, Ca²⁺    |  \n",
        "| Ligand-gated         | Neurotransmitter binding          | Na⁺, Cl⁻          |  \n",
        "| Mechanically-gated   | Physical deformation              | Na⁺, K⁺          |  \n",
        "| Leak (always open)   | Non-gated                         | K⁺               |  \n",
        "\n",
        "#### Signal Transmission Principles  \n",
        "- **Directionality**: Dendrites → Soma → Axon → Terminals  \n",
        "- **Conduction velocity**:  \n",
        "  - Myelinated axons: $v \\propto \\text{diameter} \\times \\text{myelination}$ (up to 120 m/s)  \n",
        "  - Unmyelinated axons: $v \\propto \\sqrt{\\text{diameter}}$ (0.5–2 m/s)  \n",
        "\n",
        "#### Synaptic Interactions  \n",
        "- **Excitatory neurons**:  \n",
        "  - Induce EPSP via Na⁺/Ca²⁺ influx ($\\Delta V_m > 0$)  \n",
        "  - Increase firing probability: $P(\\text{spike}) \\propto \\sum \\text{EPSP}$  \n",
        "- **Inhibitory neurons**:  \n",
        "  - Induce IPSP via Cl⁻ influx/K⁺ efflux ($\\Delta V_m < 0$)  \n",
        "  - Decrease firing probability: $P(\\text{spike}) \\propto 1 / \\sum |\\text{IPSP}|$  \n",
        "- **Integration at axon hillock**:  \n",
        "  $V_{\\text{net}} = \\sum \\text{EPSP} + \\sum \\text{IPSP}$  \n",
        "  Action potential initiated if $V_{\\text{net}} \\geq \\theta_{\\text{th}} \\approx -55 \\text{ mV}$  \n",
        "\n",
        "#### Neurotransmitter Release  \n",
        "Calcium-dependent exocytosis:  \n",
        "1. Action potential opens voltage-gated Ca²⁺ channels  \n",
        "2. $[\\text{Ca}^{2+}]_{\\text{int}} \\uparrow$ triggers vesicle fusion  \n",
        "3. Neurotransmitter release $\\propto \\text{spike rate} \\times \\Delta t$  \n",
        "\n",
        "> Note: $g$ = conductance, $V_m$ = membrane potential, $\\theta$ = threshold, EPSP/IPSP = excitatory/inhibitory postsynaptic potential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zVRhSTKV1wX"
      },
      "source": [
        "## **6. Synapse and its Function**\n",
        "### Mechanisms of Neural Communication\n",
        "\n",
        "#### Structural Components  \n",
        "1. **Presynaptic terminal**: Contains vesicles storing neurotransmitters (e.g., glutamate, GABA)  \n",
        "2. **Synaptic cleft**: 20-40 nm gap separating neurons  \n",
        "3. **Postsynaptic membrane**: Embedded with ligand-gated receptors (e.g., AMPA, NMDA, GABAₐ)  \n",
        "\n",
        "#### Neurotransmission Sequence  \n",
        "1. **Action potential arrival**: Opens voltage-gated Ca²⁺ channels ($I_{Ca} = g_{Ca} (V_m - E_{Ca})$)  \n",
        "2. **Vesicle fusion**: Ca²⁺-dependent exocytosis (release probability $P_r \\propto [Ca^{2+}]^4$)  \n",
        "3. **Receptor binding**: Neurotransmitter $\\rightleftharpoons$ receptor $\\rightarrow$ ion channel opening  \n",
        "4. **Postsynaptic potential**:  \n",
        "   - Excitatory (EPSP): Na⁺ influx $\\Delta V_m \\approx +0.2-5 \\text{ mV}$  \n",
        "   - Inhibitory (IPSP): Cl⁻ influx/K⁺ efflux $\\Delta V_m \\approx -0.2-5 \\text{ mV}$  \n",
        "5. **Termination**: Reuptake or enzymatic degradation  \n",
        "\n",
        "#### Signal Integration  \n",
        "- **Temporal summation**: Sequential EPSPs/IPSPs within $\\tau_m$ (membrane time constant):  \n",
        "  $V_{\\text{sum}} = \\sum_{k} \\Delta V_k e^{-(t - t_k)/\\tau_m}$  \n",
        "- **Spatial summation**: Concurrent inputs from multiple synapses:  \n",
        "  $V_{\\text{net}} = \\sum_i w_i \\Delta V_i$  \n",
        "  (where $w_i$ = synaptic weight)  \n",
        "\n",
        "#### Synaptic Plasticity  \n",
        "- **Long-Term Potentiation (LTP)**:  \n",
        "  1. Glutamate binds NMDA receptors  \n",
        "  2. Postsynaptic depolarization removes Mg²⁺ block  \n",
        "  3. Ca²⁺ influx $\\rightarrow$ kinase activation $\\rightarrow$ AMPA receptor insertion  \n",
        "  $\\Delta w_{\\text{LTP}} \\propto [Ca^{2+}]$  \n",
        "- **Long-Term Depression (LTD)**:  \n",
        "  1. Low-frequency stimulation → moderate Ca²⁺ influx  \n",
        "  2. Phosphatase activation $\\rightarrow$ AMPA receptor removal  \n",
        "  $\\Delta w_{\\text{LTD}} \\propto -[Ca^{2+}]$  \n",
        "\n",
        "#### Key Receptor Dynamics  \n",
        "| Receptor Type | Agonist | Primary Effect | Ion Flux |  \n",
        "|---------------|---------|----------------|----------|  \n",
        "| AMPA          | Glutamate | Fast EPSP      | Na⁺/K⁺  |  \n",
        "| NMDA          | Glutamate | Voltage-dependent EPSP | Ca²⁺/Na⁺ |  \n",
        "| GABAₐ         | GABA     | Fast IPSP      | Cl⁻      |  \n",
        "\n",
        "#### Computational Significance  \n",
        "- **Short-term plasticity**:  \n",
        "  Facilitation: $\\Delta P_r \\propto 1 - e^{-t/\\tau_f}$  \n",
        "  Depression: $\\Delta P_r \\propto e^{-t/\\tau_d}$  \n",
        "- **Information filtering**:  \n",
        "  High-pass: Facilitating synapses ($\\tau_f \\approx 100 \\text{ ms}$)  \n",
        "  Low-pass: Depressing synapses ($\\tau_d \\approx 500 \\text{ ms}$)  \n",
        "\n",
        "> Note: $V_m$ = membrane potential, $E_{Ca}$ Ca²⁺ reversal potential, $g$ conductance, $\\tau$ time constants."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cYBNzhDdTif"
      },
      "source": [
        "## **7. Introduction to Modeling and its Levels**  \n",
        "### Principles and Hierarchies in Computational Neuroscience\n",
        "\n",
        "#### Purpose of Computational Models  \n",
        "Models are concrete implementations of theoretical frameworks that:  \n",
        "- Integrate empirical data from diverse experiments  \n",
        "- Answer specific scientific questions  \n",
        "- Balance abstraction with biological relevance  \n",
        "- Exclude irrelevant details to maintain focus  \n",
        "\n",
        "#### Challenges in Ideal Modeling  \n",
        "An exhaustive model simulating all neural properties is infeasible due to:  \n",
        "1. **Computational limits**: Simulating $10^{11}$ neurons with $10^{15}$ synapses exceeds current capacity  \n",
        "2. **Incomplete data**: Key parameters (e.g., ion channel distributions, developmental history) remain unmeasured  \n",
        "3. **System complexity**: Unknown interactions between neural subsystems  \n",
        "4. **Interpretability**: Overly detailed models become as opaque as the biological system itself  \n",
        "\n",
        "#### Modeling Hierarchy  \n",
        "\n",
        "**Descriptive Models (What?)**  \n",
        "Quantify input-output relationships:  \n",
        "- *Neural encoding*: $P(\\text{spike} | \\text{stimulus}) = f(s(t))$  \n",
        "- *Neural decoding*: $\\hat{s}(t) = g(\\text{spike train})$  \n",
        "Example: Receptive field mapping via linear-nonlinear models $r(t) = k \\cdot s(t) + b$  \n",
        "\n",
        "**Mechanistic Models (How?)**  \n",
        "Simulate biophysical processes:  \n",
        "- *Single neuron*:  \n",
        "  Hodgkin-Huxley: $C_m \\frac{dV}{dt} = I_{\\text{ext}} - g_{\\text{Na}} m^3 h (V-E_{\\text{Na}}) - g_{\\text{K}} n^4 (V-E_{\\text{K}}) - g_L (V-E_L)$  \n",
        "- *Networks*:  \n",
        "  Mean-field approximation: $\\tau \\frac{dr_i}{dt} = -r_i + F\\left(\\sum_j W_{ij} r_j\\right)$  \n",
        "\n",
        "**Interpretive Models (Why?)**  \n",
        "Identify optimization principles:  \n",
        "- Efficient coding: $\\min_{W} \\mathcal{I}(X; \\hat{X})$ (maximize mutual information)  \n",
        "- Energy constraints: $\\min \\left( \\text{prediction error} + \\lambda \\cdot \\text{metabolic cost} \\right)$  \n",
        "Example: Predictive coding architectures minimizing prediction errors $\\mathcal{E} = \\frac{1}{2} || x - \\hat{x} ||^2$  \n",
        "\n",
        "#### Design Philosophy  \n",
        "- **Relevance-driven**: Include only features critical to the research question  \n",
        "- **Scale-appropriate**: Model neuronal populations rather than individual ion channels for cognitive tasks  \n",
        "- **Validation-oriented**: Compare model predictions with empirical data at matching biological scales  \n",
        "\n",
        "> Note: $r$ = firing rate, $s(t)$ = stimulus, $W_{ij}$ = synaptic weights, $\\mathcal{I}$ = mutual information, $\\mathcal{E}$ = error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mifBYFLRfTD6"
      },
      "source": [
        "## **8. Neural Dynamics**\n",
        "### Electrophysiological Principles and Integration Mechanisms  \n",
        "\n",
        "#### Membrane Potential Fundamentals  \n",
        "- **Resting state**: Steady potential $u_{\\text{rest}} \\approx -65 \\text{ mV}$  \n",
        "- **Postsynaptic potentials**:  \n",
        "  - Excitatory (EPSP): $\\Delta u_i(t) = \\epsilon_{ij}(t) > 0$ (depolarization)  \n",
        "  - Inhibitory (IPSP): $\\Delta u_i(t) = \\epsilon_{ij}(t) < 0$ (hyperpolarization)  \n",
        "- **Dynamics**: Without input, $u(t)$ returns to $u_{\\text{rest}}$ after perturbation  \n",
        "\n",
        "#### Linear Integration Regime  \n",
        "For sparse input spikes from presynaptic neuron $j$ at times $t_j^{(f)}$:  \n",
        "$u_i(t) = u_{\\text{rest}} + \\sum_j \\sum_f \\epsilon_{ij}(t - t_j^{(f)})$  \n",
        "where $\\epsilon_{ij}$ is the postsynaptic potential kernel. This linear summation holds for low input frequencies.  \n",
        "\n",
        "#### Threshold-Driven Nonlinearity  \n",
        "- **Action potential initiation**:  \n",
        "  Spike emitted when $u_i(t) \\geq \\theta \\approx -55 \\text{ mV}$  \n",
        "  Amplitude $\\approx 100 \\text{ mV}$, duration $\\approx 1-2 \\text{ ms}$  \n",
        "- **Nonlinear breakdown**:  \n",
        "  High-frequency inputs violate linearity due to:  \n",
        "  1. Ion channel inactivation  \n",
        "  2. Refractory periods  \n",
        "  3. Shunting inhibition  \n",
        "- **Afterpotentials**:  \n",
        "  Post-spike hyperpolarization ($u_i < u_{\\text{rest}}$) via K⁺ efflux  \n",
        "\n",
        "#### Phase Response Analysis  \n",
        "- **Subthreshold dynamics**:  \n",
        "  $\\tau_m \\frac{du}{dt} = - (u - u_{\\text{rest}}) + R_m I_{\\text{syn}}(t)$  \n",
        "  where $I_{\\text{syn}}(t) = \\sum_f \\alpha(t - t_f)$ (synaptic current kernel)  \n",
        "- **Spike-timing effects**:  \n",
        "  Critical dependence on input temporal coherence:  \n",
        "  $\\sigma_t = \\left( \\frac{1}{N} \\sum_k (t_k - \\bar{t})^2 \\right)^{1/2}$  \n",
        "\n",
        "#### Key Dynamical Regimes  \n",
        "| Condition                      | Membrane Behavior                     | Mathematical Form                  |  \n",
        "|--------------------------------|---------------------------------------|------------------------------------|  \n",
        "| Sparse inputs                  | Linear PSP summation                  | $u_i(t) \\propto \\sum \\epsilon_{ij}$ |  \n",
        "| $u_i \\approx \\theta$           | Stochastic spiking                    | $P(\\text{spike}) \\propto e^{u_i - \\theta}$ |  \n",
        "| $u_i \\geq \\theta$              | Action potential initiation           | $\\frac{du}{dt} \\rightarrow \\infty$ |  \n",
        "| Post-spike                     | Refractory period (no spiking)        | $u_i(t) \\leq u_{\\text{reset}}$     |  \n",
        "\n",
        "> Note: $\\tau_m$ = membrane time constant, $R_m$ = membrane resistance, $\\alpha$ = synaptic current profile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPSnx1pPg6xr"
      },
      "source": [
        "## **9. Neuron Modeling**  \n",
        "###From Biophysical to Phenomenological Approaches\n",
        "\n",
        "#### Modeling Objectives  \n",
        "Single neuron models must:  \n",
        "- Balance computational efficiency with biological fidelity  \n",
        "- Reproduce diverse firing patterns (e.g., tonic bursting, spike adaptation)  \n",
        "- Scale to large networks ($N \\sim 10^6$ neurons)  \n",
        "\n",
        "#### Core Electrophysiology (Review)  \n",
        "- **Resting potential**: $V_{\\text{rest}} \\approx -70 \\text{ mV}$  \n",
        "- **Spike initiation**: Threshold $\\theta \\approx -55 \\text{ mV}$  \n",
        "- **Synaptic impact**:  \n",
        "  Excitatory: $\\Delta V_m > 0$  \n",
        "  Inhibitory: $\\Delta V_m < 0$  \n",
        "- **Fixed neuron type**: Excitatory/inhibitory distinction is immutable  \n",
        "\n",
        "#### Firing Pattern Taxonomy  \n",
        "Biological neurons exhibit 20+ dynamic regimes, including:  \n",
        "- **Tonic spiking**: Sustained regular firing  \n",
        "- **Phasic bursting**: Bursts followed by silence  \n",
        "- **Class 1 excitability**: Continuous frequency-current ($f$-$I$) relationship  \n",
        "- **Class 2 excitability**: Discontinuous $f$-$I$ jump  \n",
        "- **Spike-frequency adaptation**: $f \\propto e^{-t/\\tau_{\\text{adapt}}}$  \n",
        "- **Bistability**: Two stable voltage states  \n",
        "\n",
        "#### Model Comparisons  \n",
        "\n",
        "**Integrate-and-Fire (IF)**  \n",
        "- *Dynamics*:  \n",
        "  $\\tau_m \\frac{dV}{dt} = -(V - V_{\\text{rest}}) + R_m I_{\\text{syn}}$  \n",
        "  Spike when $V \\geq \\theta$; then $V \\leftarrow V_{\\text{reset}}$  \n",
        "- *Advantages*: Computationally lightweight ($O(1)$ operations/step)  \n",
        "- *Limitations*: Cannot reproduce bursting or adaptation  \n",
        "\n",
        "**Hodgkin-Huxley (HH)**  \n",
        "- *Dynamics*:  \n",
        "  $C_m \\frac{dV}{dt} = I_{\\text{ext}} - g_{\\text{Na}} m^3 h (V-E_{\\text{Na}}) - g_{\\text{K}} n^4 (V-E_{\\text{K}}) - g_L (V-E_L)$  \n",
        "  with $\\frac{dm}{dt} = \\alpha_m (1-m) - \\beta_m m$ (similar for $h,n$)  \n",
        "- *Advantages*: Biophysically realistic; captures ion channel dynamics  \n",
        "- *Limitations*: Computationally expensive ($O(4)$ coupled ODEs)  \n",
        "\n",
        "**Izhikevich Model**  \n",
        "- *Dynamics*:  \n",
        "  $\\frac{dV}{dt} = 0.04V^2 + 5V + 140 - u + I$  \n",
        "  $\\frac{du}{dt} = a(bV - u)$  \n",
        "  If $V \\geq 30 \\text{ mV}$, then $V \\leftarrow c$, $u \\leftarrow u + d$  \n",
        "- *Parameters*:  \n",
        "  $a$: Time scale of recovery variable  \n",
        "  $b$: Sensitivity of $u$ to $V$  \n",
        "  $c$: Post-spike reset voltage  \n",
        "  $d$: Post-spike recovery reset  \n",
        "- *Advantages*:  \n",
        "  - Computationally efficient ($O(2)$ ODEs)  \n",
        "  - Reproduces 20+ firing patterns with parameter tuning  \n",
        "  - Scalable to large networks  \n",
        "\n",
        "#### Tradeoffs in Model Selection  \n",
        "| Model              | Computational Cost | Biological Detail | Firing Pattern Diversity |  \n",
        "|--------------------|--------------------|-------------------|--------------------------|  \n",
        "| Integrate-and-Fire | Low                | Low               | Limited                  |  \n",
        "| Hodgkin-Huxley     | High               | High              | High                     |  \n",
        "| Izhikevich         | Low                | Medium            | High                     |  \n",
        "\n",
        "> Note: $I_{\\text{syn}}$ = synaptic current, $\\tau_m$ = membrane time constant, $a,b,c,d$ = Izhikevich parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVTIcEB-hWR6"
      },
      "source": [
        "## **10. Passive Neuronal Membrane**\n",
        "### Subthreshold Dynamics and Linear Response Theory\n",
        "\n",
        "#### RC Circuit Analogy  \n",
        "The neuronal membrane is modeled as an electrical circuit:  \n",
        "- **Resistance ($R_m$)**: Ion channel conductivity ($\\sim 1-100 \\text{ M}\\Omega$)  \n",
        "- **Capacitance ($C_m$)**: Lipid bilayer charge storage ($\\sim 0.1-1 \\mu\\text{F/cm}^2$)  \n",
        "- **Time constant**: $\\tau_m = R_m C_m \\approx 5-100 \\text{ ms}$  \n",
        "Governing equation:  \n",
        "$\\tau_m \\frac{du}{dt} = -(u - u_{\\text{rest}}) + R_m I(t)$  \n",
        "where $u$ = membrane potential, $I(t)$ = input current.  \n",
        "\n",
        "#### Solutions for Canonical Inputs  \n",
        "\n",
        "**Step Current ($I(t) = I_0 \\Theta(t)$)**  \n",
        "Steady-state potential: $u_{\\infty} = u_{\\text{rest}} + R_m I_0$  \n",
        "Transient response:  \n",
        "$u(t) = u_{\\text{rest}} + R_m I_0 \\left(1 - e^{-t/\\tau_m}\\right)$  \n",
        "\n",
        "**Delta Pulse ($I(t) = q \\delta(t - t_0)$)**  \n",
        "Impulse response:  \n",
        "$u(t) = u_{\\text{rest}} + \\frac{q}{C_m} e^{-(t - t_0)/\\tau_m}$  \n",
        "(Peak voltage $\\Delta u_{\\text{max}} = q / C_m$)  \n",
        "\n",
        "**Arbitrary Input**  \n",
        "Convolution with Green's function:  \n",
        "$u(t) = u_{\\text{rest}} + \\frac{1}{C_m} \\int_{-\\infty}^t e^{-(t - t')/\\tau_m} I(t')  dt'$  \n",
        "\n",
        "#### Subthreshold Integration Properties  \n",
        "- **Linearity**: $u_{\\text{total}} = u_{\\text{rest}} + \\sum_k \\Delta u_k$ for independent inputs  \n",
        "- **Temporal filtering**:  \n",
        "  Low-pass characteristic with cutoff $f_c = 1/(2\\pi\\tau_m) \\approx 3-50 \\text{ Hz}$  \n",
        "- **Fluctuation-dissipation**:  \n",
        "  Thermal noise variance $\\sigma_u^2 = \\frac{k_B T}{2C_m}$  \n",
        "\n",
        "#### Phase Plane Analysis  \n",
        "- **Fixed points**: $\\frac{du}{dt} = 0$ when $u = u_{\\text{rest}} + R_m I_0$  \n",
        "- **Stability**: All fixed points are attractors for $I_0$ constant  \n",
        "- **Nullcline**: $\\dot{u} = 0$ along $u = u_{\\text{rest}} + R_m I_0$  \n",
        "\n",
        "#### Experimental Validation  \n",
        "| Property               | Experimental Range | Model Prediction |  \n",
        "|------------------------|---------------------|------------------|  \n",
        "| Time constant ($\\tau_m$) | 5-100 ms           | $\\tau_m = R_m C_m$ |  \n",
        "| Input resistance ($R_m$) | 10-200 M$\\Omega$   | $R_m = \\Delta u / I_0$ |  \n",
        "| Capacitance ($C_m$)     | 0.1-1 $\\mu$F/cm²   | $C_m = \\tau_m / R_m$ |  \n",
        "\n",
        "> Note: $q$ = charge in coulombs, $\\Theta(t)$ = Heaviside step function, $\\delta(t)$ = Dirac delta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynn_Cdr0hqVw"
      },
      "source": [
        "## **11. Leaky Integrate-and-Fire (LIF) Neuron Model**\n",
        "### Simplified Spiking Neuron Dynamics  \n",
        "\n",
        "#### Core Equation  \n",
        "The LIF model extends passive membrane dynamics with a firing threshold:  \n",
        "$\\tau_m \\frac{du}{dt} = -(u - u_{\\text{rest}}) + R_m I(t)$  \n",
        "where:  \n",
        "- $u$ = membrane potential  \n",
        "- $\\tau_m = R_m C_m$ = membrane time constant ($\\sim$10-100 ms)  \n",
        "- $I(t)$ = synaptic input current  \n",
        "Spike emission occurs when $u \\geq \\theta$; then $u \\leftarrow u_{\\text{reset}}$  \n",
        "\n",
        "#### Response to Step Input  \n",
        "For constant current $I(t) = I_0$:  \n",
        "1. Subthreshold integration:  \n",
        "   $u(t) = u_{\\text{rest}} + R_m I_0 \\left(1 - e^{-t/\\tau_m}\\right)$  \n",
        "2. Firing condition: $u(t) = \\theta$ at time $T$  \n",
        "3. Firing rate:  \n",
        "   $f = \\frac{1}{T} = \\left[ \\tau_m \\ln \\left( \\frac{R_m I_0 + u_{\\text{rest}} - u_{\\text{reset}}}{R_m I_0 + u_{\\text{rest}} - \\theta} \\right) \\right]^{-1}$  \n",
        "   - For $I_0 \\gg \\frac{\\theta - u_{\\text{rest}}}{R_m}$, $f \\approx \\frac{I_0}{C_m (\\theta - u_{\\text{reset}})}$  \n",
        "\n",
        "#### Phase Plane Analysis  \n",
        "- **Nullcline**: $\\frac{du}{dt} = 0$ when $u = u_{\\text{rest}} + R_m I_0$  \n",
        "- **Threshold crossing**:  \n",
        "  - $I_0 < I_{\\text{th}} = \\frac{\\theta - u_{\\text{rest}}}{R_m}$: No spiking  \n",
        "  - $I_0 > I_{\\text{th}}$: Repetitive spiking with frequency $f(I_0)$  \n",
        "\n",
        "#### Key Features  \n",
        "1. **Linear integration**: Inputs sum linearly regardless of voltage state  \n",
        "2. **Instant reset**: No biophysical spike generation/recovery mechanisms  \n",
        "3. **Delta-like spikes**: Output represented as discrete events  \n",
        "\n",
        "#### Limitations  \n",
        "- Cannot reproduce:  \n",
        "  - Spike-frequency adaptation ($\\tau_{\\text{adapt}}$ missing)  \n",
        "  - Bursting (e.g., phasic/tonic bursts)  \n",
        "  - Inhibitory rebound  \n",
        "  - Subthreshold oscillations  \n",
        "- Oversimplifies:  \n",
        "  - Voltage-dependent ion channel dynamics  \n",
        "  - Active dendrites  \n",
        "\n",
        "#### Utility and Validation  \n",
        "- **Strengths**:  \n",
        "  - Computationally efficient ($O(1)$ operations per neuron)  \n",
        "  - Captures precise spike timing in response to transient inputs  \n",
        "  - Matches experimental data for regular spiking neurons  \n",
        "- **Empirical fit**:  \n",
        "\n",
        "\n",
        "  | Neuron Type               | LIF Prediction Error |  \n",
        "  |---------------------------|----------------------|  \n",
        "  | Regular spiking           | $\\sim$5-10%          |  \n",
        "  | Fast spiking              | $\\sim$15-30%         |  \n",
        "  | Intrinsically bursting    | $\\sim$40-70%         |\n",
        "\n",
        "> Note: $u_{\\text{reset}}$ typically $\\approx u_{\\text{rest}}$, $\\theta \\approx -55 \\text{ mV}$, $u_{\\text{rest}} \\approx -65 \\text{ mV}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CX3VI-XjsPr"
      },
      "source": [
        "## **12. Nonlinear LIF Models**  \n",
        "### Enhanced Biophysical Realism  \n",
        "\n",
        "#### Generalized Framework  \n",
        "Nonlinear LIF models extend the standard LIF with voltage-dependent dynamics:  \n",
        "$\\tau \\frac{du}{dt} = F(u) + R_m I(t)$  \n",
        "Spike emitted when $u \\geq \\theta_{\\text{reset}}$; then $u \\leftarrow u_{\\text{reset}}$  \n",
        "$F(u)$ introduces nonlinearity to capture active membrane properties.  \n",
        "\n",
        "#### Threshold Dynamics  \n",
        "- **Bifurcation threshold ($\\theta_{\\text{rh}}$)**:  \n",
        "  Critical voltage where stability changes:  \n",
        "  - $u < \\theta_{\\text{rh}}$: Stable fixed point ($u \\rightarrow u_{\\text{rest}}$)  \n",
        "  - $u > \\theta_{\\text{rh}}$: Runaway depolarization  \n",
        "- **Disappearance of fixed points**:  \n",
        "  For $I(t) > I_{\\text{th}}$, no stable equilibrium → repetitive firing  \n",
        "\n",
        "#### Exponential Integrate-and-Fire (EIF)  \n",
        "$\\tau \\frac{du}{dt} = -(u - u_{\\text{rest}}) + \\Delta_T \\exp\\left(\\frac{u - \\theta_{\\text{rh}}}{\\Delta_T}\\right) + R_m I(t)$  \n",
        "- $\\Delta_T$: Spike slope factor ($\\sim 0.5-4 \\text{ mV}$)  \n",
        "- $\\theta_{\\text{rh}}$: Rheobase threshold  \n",
        "- **Biophysical basis**: Approximates Na⁺ channel activation  \n",
        "- **Spike initiation**: Exponential term dominates near $\\theta_{\\text{rh}}$  \n",
        "\n",
        "#### Quadratic Integrate-and-Fire (QIF)  \n",
        "$\\tau \\frac{du}{dt} = a_0 (u - u_{\\text{rest}})(u - u_c) + R_m I(t)$  \n",
        "- $a_0$: Scaling parameter ($>0$)  \n",
        "- $u_c$: Critical voltage ($u_c > u_{\\text{rest}}$)  \n",
        "- **Dynamics**:  \n",
        "  - $u < u_c$: Decay to $u_{\\text{rest}}$  \n",
        "  - $u > u_c$: Divergence to spike  \n",
        "- **Canonical form**:  \n",
        "  $\\tau \\frac{du}{dt} = u^2 + I$ (after coordinate shift)  \n",
        "\n",
        "#### Phase Plane Analysis  \n",
        "| Condition              | EIF Behavior                          | QIF Behavior                          |  \n",
        "|------------------------|---------------------------------------|---------------------------------------|  \n",
        "| $I(t) = 0$             | Single stable fixed point             | Stable $u_{\\text{rest}}$, unstable $u_c$ |  \n",
        "| $0 < I(t) < I_{\\text{th}}$ | Subthreshold oscillations           | Depolarizing block                 |  \n",
        "| $I(t) > I_{\\text{th}}$   | Regular spiking                     | Bursting or chaotic firing         |  \n",
        "\n",
        "#### Comparative Advantages  \n",
        "| Model        | Spike Initiation Realism | Computational Cost | Firing Pattern Diversity |  \n",
        "|--------------|--------------------------|-------------------|--------------------------|  \n",
        "| Standard LIF | Low                      | Low               | Low                      |  \n",
        "| EIF          | High                     | Medium            | Medium                   |  \n",
        "| QIF          | Medium                   | Low               | High                     |  \n",
        "\n",
        "> Note: $I_{\\text{th}} = \\min I \\text{ such that } \\frac{du}{dt} > 0 \\ \\forall u$, $\\theta_{\\text{rh}} \\approx -50 \\text{ mV}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX3iVJSBl5Cw"
      },
      "source": [
        "## **13. Adaptive LIF Models**  \n",
        "###Capturing Firing Pattern Diversity\n",
        "\n",
        "#### Adaptation Mechanisms  \n",
        "Neurons dynamically adjust firing rates during sustained stimulation (e.g., tonic spiking with rate reduction). This conserves energy and improves signal detection. The adaptive LIF model extends standard formulations with auxiliary variables:  \n",
        "\n",
        "$\\tau_m \\frac{du}{dt} = F(u) - R_m \\sum_k w_k + R_m I(t)$  \n",
        "$\\tau_k \\frac{dw_k}{dt} = a_k (u - u_{\\text{rest}}) - w_k + b_k \\tau_k \\sum_{t^{(f)}} \\delta(t - t^{(f)})$  \n",
        "Spike reset: $u \\leftarrow u_{\\text{reset}}$  \n",
        "- $w_k$: Adaptation currents  \n",
        "- $a_k$: Subthreshold coupling strength  \n",
        "- $b_k$: Spike-triggered adaptation increment  \n",
        "\n",
        "#### Adaptive Exponential Integrate-and-Fire (aEIF)  \n",
        "Specialized form with exponential spike initiation and single adaptation variable:  \n",
        "$\\tau_m \\frac{du}{dt} = -(u - u_{\\text{rest}}) + \\Delta_T \\exp\\left(\\frac{u - \\theta_{\\text{rh}}}{\\Delta_T}\\right) - R_m w + R_m I(t)$  \n",
        "$\\tau_w \\frac{dw}{dt} = a(u - u_{\\text{rest}}) - w + b \\tau_w \\sum_{t^{(f)}} \\delta(t - t^{(f)})$  \n",
        "Spike reset: $u \\leftarrow u_{\\text{reset}}$  \n",
        "\n",
        "#### Parameter-Dependent Firing Patterns  \n",
        "| Parameter Combination | Resulting Firing Pattern | Biological Equivalent |  \n",
        "|------------------------|--------------------------|------------------------|  \n",
        "| $a < 0, b > 0$         | Spike-frequency adaptation | Regular spiking neurons |  \n",
        "| $a = 0, b > 0$         | Phasic bursting           | Thalamic neurons      |  \n",
        "| $a > 0, b = 0$         | Subthreshold oscillations | Cortical layer V neurons |  \n",
        "| $a > 0, b > 0$         | Mixed mode (bursting/tonic) | Hippocampal neurons   |  \n",
        "\n",
        "#### Phase Space Interpretation  \n",
        "- **Subthreshold adaptation ($a$)**:  \n",
        "  Scales voltage-dependent current $w \\propto (u - u_{\\text{rest}})$  \n",
        "  Slows depolarization $\\rightarrow$ increases interspike intervals  \n",
        "- **Spike-triggered adaptation ($b$)**:  \n",
        "  Instantaneous current jump $\\Delta w = b$ at spike time  \n",
        "  Hyperpolarizes neuron $\\rightarrow$ delays next spike  \n",
        "\n",
        "#### Dynamics Validation  \n",
        "- **Steady-state firing rate**:  \n",
        "  $f_{\\infty} = \\frac{I}{b C_m} \\left(1 - e^{-1/(f_{\\infty} \\tau_w)}\\right)^{-1}$  \n",
        "- **Adaptation time constant**:  \n",
        "  $\\tau_w$ governs rate decay: $f(t) = f_0 e^{-t/\\tau_w} + f_{\\infty}$  \n",
        "  (Typically $\\tau_w \\approx 50-200 \\text{ ms}$)  \n",
        "\n",
        "> Note: $\\theta_{\\text{rh}} \\approx -50 \\text{ mV}$, $\\Delta_T \\approx 2 \\text{ mV}$, $u_{\\text{reset}} \\approx -65 \\text{ mV}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvxb-I8u1G7e"
      },
      "source": [
        "## **14. Hodgkin-Huxley Neuron Model**  \n",
        "### Biophysical Basis of Action Potentials\n",
        "\n",
        "#### Ion Concentration Gradients  \n",
        "- **Nernst potential**: Electrochemical equilibrium voltage for ion $X$  \n",
        "  $E_X = \\frac{RT}{zF} \\ln \\left( \\frac{[X]_{\\text{out}}}{[X]_{\\text{in}}} \\right)$  \n",
        "  where $R$ = gas constant, $T$ = temperature, $z$ = ion valence, $F$ = Faraday constant  \n",
        "  - $E_{\\text{Na}} \\approx +67 \\text{ mV}$ ($[Na^+]_{\\text{out}} = 145 \\text{ mM}, [Na^+]_{\\text{in}} = 10 \\text{ mM}$)  \n",
        "  - $E_{\\text{K}} \\approx -83 \\text{ mV}$ ($[K^+]_{\\text{in}} = 140 \\text{ mM}, [K^+]_{\\text{out}} = 5 \\text{ mM}$)  \n",
        "- **Resting potential**: $u_{\\text{rest}} \\approx -65 \\text{ mV}$ (dynamic balance between Na⁺ influx and K⁺ efflux)  \n",
        "\n",
        "#### Circuit Representation  \n",
        "Membrane equivalent circuit:  \n",
        "- **Capacitance** $C_m \\approx 1 \\mu\\text{F/cm}^2$  \n",
        "- **Ion channels**: Voltage-dependent conductances  \n",
        "  - Sodium ($g_{\\text{Na}}$), potassium ($g_{\\text{K}}$), leak ($g_L$)  \n",
        "- **Batteries**: Nernst potentials $E_{\\text{Na}}, E_{\\text{K}}, E_L$  \n",
        "\n",
        "#### Current Balance Equation  \n",
        "$C_m \\frac{du}{dt} = I(t) - I_{\\text{Na}} - I_{\\text{K}} - I_L$  \n",
        "where ionic currents:  \n",
        "$I_{\\text{Na}} = g_{\\text{Na}} m^3 h (u - E_{\\text{Na}})$  \n",
        "$I_{\\text{K}} = g_{\\text{K}} n^4 (u - E_{\\text{K}})$  \n",
        "$I_L = g_L (u - E_L)$  \n",
        "\n",
        "#### Gating Dynamics  \n",
        "Voltage-dependent activation/inactivation:  \n",
        "$\\frac{dm}{dt} = \\alpha_m(u)(1 - m) - \\beta_m(u)m$  \n",
        "$\\frac{dn}{dt} = \\alpha_n(u)(1 - n) - \\beta_n(u)n$  \n",
        "$\\frac{dh}{dt} = \\alpha_h(u)(1 - h) - \\beta_h(u)h$  \n",
        "Empirical rate functions (squid axon, 6.3°C):  \n",
        "$\\alpha_m = \\frac{0.1(25-u)}{\\exp(2.5-0.1u)-1}, \\beta_m = 4 \\exp(-u/18)$  \n",
        "$\\alpha_h = 0.07 \\exp(-u/20), \\beta_h = \\frac{1}{\\exp(3-0.1u)+1}$  \n",
        "$\\alpha_n = \\frac{0.01(10-u)}{\\exp(1-0.1u)-1}, \\beta_n = 0.125 \\exp(-u/80)$  \n",
        "\n",
        "#### Action Potential Mechanism  \n",
        "1. **Depolarization**: $u \\uparrow \\rightarrow m \\uparrow \\rightarrow I_{\\text{Na}} \\uparrow$ (positive feedback)  \n",
        "2. **Inactivation**: $h \\downarrow \\rightarrow I_{\\text{Na}} \\downarrow$  \n",
        "3. **Delayed activation**: $n \\uparrow \\rightarrow I_{\\text{K}} \\uparrow$ (repolarization)  \n",
        "4. **Hyperpolarization**: $u \\downarrow \\rightarrow n \\downarrow$ (return to rest)  \n",
        "\n",
        "#### Stochastic Channel Behavior  \n",
        "- Finite channels $\\rightarrow$ conductance fluctuations  \n",
        "- Open probability: $P_{\\text{open}} = m^3 h$ (Na⁺), $n^4$ (K⁺)  \n",
        "- Transition rates:  \n",
        "  $P_{\\text{closed→open}} = \\alpha_x \\Delta t$  \n",
        "  $P_{\\text{open→closed}} = \\beta_x \\Delta t$  \n",
        "\n",
        "#### Key Parameters  \n",
        "| Parameter   | Sodium Channel | Potassium Channel |  \n",
        "|-------------|----------------|-------------------|  \n",
        "| Max conductance | $g_{\\text{Na}} = 120 \\text{ mS/cm}^2$ | $g_{\\text{K}} = 36 \\text{ mS/cm}^2$ |  \n",
        "| Gating variables | Activation $m$, Inactivation $h$ | Activation $n$ |  \n",
        "| Time constants | $\\tau_m \\approx 0.1 \\text{ ms}$, $\\tau_h \\approx 1 \\text{ ms}$ | $\\tau_n \\approx 1 \\text{ ms}$ |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDUzDXuu1NIR"
      },
      "source": [
        "## **15. Neural Populations**\n",
        "### Collective Dynamics and Network Organization\n",
        "\n",
        "#### Spatial Scales of Neural Organization  \n",
        "1. **Microscopic** (1-100 μm):  \n",
        "   - Ion channels, single neurons  \n",
        "   - Morphological and cellular properties  \n",
        "2. **Mesoscopic** (100 μm-1 mm):  \n",
        "   - Local populations (~10⁴ neurons)  \n",
        "   - Laminar/circuit interactions  \n",
        "3. **Macroscopic** (1 mm-10 cm):  \n",
        "   - Brain areas and whole-brain dynamics  \n",
        "   - Field potentials and fMRI signals  \n",
        "\n",
        "#### Population Activity Definition  \n",
        "For $N$ neurons, the population activity $A(t)$ is:  \n",
        "$A(t) = \\lim_{\\Delta t \\to 0} \\frac{1}{\\Delta t} \\frac{n_{\\text{act}}(t; t+\\Delta t)}{N} = \\frac{1}{N} \\sum_{j=1}^N \\sum_f \\delta(t - t_j^{(f)})$  \n",
        "where $n_{\\text{act}}$ = active neurons in $\\Delta t$, $t_j^{(f)}$ = spike times.  \n",
        "\n",
        "#### Homogeneous vs. Heterogeneous Populations  \n",
        "| Property          | Homogeneous Populations                          | Heterogeneous Populations                |  \n",
        "|-------------------|--------------------------------------------------|------------------------------------------|  \n",
        "| **Neuron parameters** | Identical ($\\theta_i = \\theta$, $\\tau_{m,i} = \\tau_m$) | Vary per neuron                          |  \n",
        "| **External input**    | Uniform $I_{\\text{ext},i}(t) = I_{\\text{ext}}(t)$ | Neuron-specific $I_{\\text{ext},i}(t)$    |  \n",
        "| **Connectivity**      | Uniform strength $w_{ij} \\approx w_0$            | Structured/diverse connectivity          |  \n",
        "| **Analysis**          | Tractable mean-field approximations              | Require subpopulation partitioning       |  \n",
        "\n",
        "#### Connectivity Architectures  \n",
        "**Full Connectivity (All-to-All):**  \n",
        "- Weight scaling: $w_{ij} = J_0 / N$ (maintains fixed mean input $\\langle I_{\\text{syn}} \\rangle$)  \n",
        "- Variant: Gaussian weights $w_{ij} \\sim \\mathcal{N}(J_0/N, \\sigma_0/N)$  \n",
        "\n",
        "**Random Connectivity (Fixed Probability):**  \n",
        "- Connection probability $p \\approx 0.1$  \n",
        "- Mean in-degree: $\\langle C \\rangle = pN$  \n",
        "- Weight scaling: $w_{ij} = J_0 / \\langle C \\rangle = J_0/(pN)$  \n",
        "\n",
        "**Random Connectivity (Fixed In-Degree):**  \n",
        "- Fixed presynaptic partners $C = \\text{const}$  \n",
        "- No $N$-scaling: $w_{ij} = J_0 / C$  \n",
        "- Input: $I_{\\text{syn},i} = \\sum_{j \\in \\mathcal{P}_i} w_{ij} s_j(t)$ where $\\mathcal{P}_i$ = presynaptic set  \n",
        "\n",
        "#### Scaling Principles  \n",
        "| Connectivity Scheme    | Synaptic Weight Scaling | Input Statistics                     |  \n",
        "|-------------------------|--------------------------|--------------------------------------|  \n",
        "| Full                    | $w_{ij} \\propto 1/N$    | Low variance                         |  \n",
        "| Random (probabilistic)  | $w_{ij} \\propto 1/N$    | Variance $\\sigma^2_I \\propto p(1-p)$ |  \n",
        "| Random (fixed in-degree)| $w_{ij} \\propto 1/C$    | Variance $\\sigma^2_I \\propto 1/C$    |  \n",
        "\n",
        "#### Population Response Dynamics  \n",
        "- **Mean-field approximation**:  \n",
        "  $\\tau \\frac{dA}{dt} = -A + F(I_{\\text{ext}} + J_0 A)$  \n",
        "  where $F$ = transfer function (e.g., $F(x) = (x - \\theta)_+$)  \n",
        "- **Critical phenomena**:  \n",
        "  Phase transitions at $J_0 \\cdot F'(I_{\\text{ext}}) = 1$  \n",
        "\n",
        "> Note: $s_j(t) = \\sum_f \\delta(t - t_j^{(f)})$, $J_0$ = mean synaptic efficacy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ucsWRTX3UaM"
      },
      "source": [
        "## **16. Balanced Neural Networks**  \n",
        "### Stable Dynamics Through Excitation-Inhibition Balance\n",
        "\n",
        "#### Core Architecture  \n",
        "- **Dual-population design**:  \n",
        "  Excitatory ($E$) and inhibitory ($I$) populations with reciprocal connectivity  \n",
        "- **Balance condition**:  \n",
        "  Mean excitatory and inhibitory inputs cancel:  \n",
        "  $\\langle I_E \\rangle + \\langle I_I \\rangle \\approx 0$  \n",
        "- **Weight scaling**:  \n",
        "  $w_{ij} = \\frac{J_0}{\\sqrt{C}} = \\frac{J_0}{\\sqrt{pN}}$  \n",
        "  where $C$ = mean in-degree, $p$ = connection probability, $N$ = neurons  \n",
        "\n",
        "#### Fluctuation Control  \n",
        "- **Input statistics**:  \n",
        "  Total current to neuron $i$: $I_i(t) = I_{\\text{ext}} + \\underbrace{\\sum_j w_{ij} s_j(t)}_{\\text{fluctuating}}$  \n",
        "- **Variance scaling**:  \n",
        "  $\\text{Var}(I_i) \\propto \\frac{J_0^2}{C} \\cdot A_{\\text{pop}}$  \n",
        "  Adjust $J_0$ to tune variability without affecting $\\langle I_i \\rangle \\approx 0$  \n",
        "\n",
        "#### Population Dynamics  \n",
        "For pool $n$ with $N_n$ neurons:  \n",
        "Activity: $A_n(t) = \\frac{1}{N_n} \\sum_{j \\in \\Gamma_n} \\sum_f \\delta(t - t_j^{(f)})$  \n",
        "Synaptic input to pool $n$ from pool $m$:  \n",
        "$I_n(t) = \\sum_m J_{nm} \\int_0^\\infty \\alpha_{nm}(s) A_m(t-s)  ds$  \n",
        "- $J_{nm}$: Coupling strength ($J_{EE}, J_{EI}, J_{IE}, J_{II}$)  \n",
        "- $\\alpha_{nm}(s)$: Synaptic kernel (e.g., $\\alpha(s) = \\frac{s}{\\tau_s^2} e^{-s/\\tau_s}$)  \n",
        "\n",
        "#### Critical Properties  \n",
        "1. **Asynchronous irregular (AI) state**:  \n",
        "   Low firing rates ($\\sim$1-10 Hz) with Poisson-like variability  \n",
        "2. **Stability condition**:  \n",
        "   $|J_{EI} \\cdot J_{IE}| > |J_{EE} \\cdot J_{II}|$ (inhibition dominates feedback)  \n",
        "3. **Input scaling**:  \n",
        "   External drive $I_{\\text{ext}} \\propto \\sqrt{K}$ where $K$ = number of inputs  \n",
        "\n",
        "#### Functional Advantages  \n",
        "- **Noise suppression**:  \n",
        "  Fluctuations $\\sigma_I \\propto 1/\\sqrt{C}$ vanish for large $N$  \n",
        "- **Gain control**:  \n",
        "  Linear response to inputs $A_{\\text{out}} \\propto I_{\\text{ext}}$  \n",
        "- **Criticality**:  \n",
        "  Maximized dynamic range and sensitivity  \n",
        "\n",
        "> Note: $\\Gamma_n$ = neuron set in pool $n$, $s_j(t)$ = spike train of neuron $j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W5spqbR3w9a"
      },
      "source": [
        "## **17. Decision-Making Processes in the Brain**  \n",
        "### Neural Mechanisms and Computational Models  \n",
        "\n",
        "#### Core Requirements  \n",
        "1. **Representation**: Encoding of inputs, outcomes, and value assessments  \n",
        "2. **Selection**: Mechanism for choosing among options  \n",
        "3. **Learning**: Feedback-driven performance improvement  \n",
        "Key brain areas: Prefrontal cortex, parietal cortex, basal ganglia (exact locus unknown)  \n",
        "\n",
        "#### Perceptual Decision Paradigm  \n",
        "- **Experimental basis**: Neuronal recordings during stimulus discrimination tasks  \n",
        "- **Key feature**: Measurable neural correlates of choice selection  \n",
        "- **Computational formulation**: Binary/multi-alternative forced choice  \n",
        "\n",
        "#### Neural Circuit Architecture  \n",
        "**Dual-Choice Model**:  \n",
        "- Two excitatory populations ($E_1$, $E_2$) representing options  \n",
        "- Common inhibitory population ($I$) mediating competition  \n",
        "- Connectivity:  \n",
        "  - Intra-excitatory: $w_{EE}$  \n",
        "  - $E \\rightarrow I$: $w_{EI}$  \n",
        "  - $I \\rightarrow E$: $w_{IE}$  \n",
        "- Dynamics: Spontaneous activity $\\approx$ 5 Hz; biased by inputs $I_1$, $I_2$  \n",
        "\n",
        "#### Decision Dynamics  \n",
        "**Original Equations**:  \n",
        "$\\tau_E \\frac{dh_{E,k}}{dt} = -h_{E,k} + w_{EE} g_E(h_{E,k}) + w_{EI} g_I(h_I) + R I_k$  \n",
        "$\\tau_I \\frac{dh_I}{dt} = -h_I + w_{IE} \\sum_{k=1}^2 g_E(h_{E,k})$  \n",
        "where $g_x$ = gain functions, $h$ = input potentials  \n",
        "\n",
        "**Reduced Model (Fast Inhibition Approximation)**:  \n",
        "Assumptions:  \n",
        "1. $\\tau_I \\ll \\tau_E$ → $h_I \\approx w_{IE} [g_E(h_{E,1}) + g_E(h_{E,2})]$  \n",
        "2. Linear inhibitory gain: $g_I(h_I) = \\gamma h_I$  \n",
        "Reduced equations:  \n",
        "$\\tau_E \\frac{dh_{E,k}}{dt} = -h_{E,k} + (w_{EE} - \\alpha) g_E(h_{E,k}) - \\alpha g_E(h_{E,j}) + R I_k$  \n",
        "where $\\alpha = -\\gamma w_{EI} w_{IE} > 0$ (effective inhibition), $j \\neq k$  \n",
        "\n",
        "#### Winner-Take-All (WTA) Networks  \n",
        "**K-Option Generalization**:  \n",
        "$\\tau \\frac{dh_k}{dt} = -h_k + w_0 g(h_k) - \\alpha \\sum_{j \\neq k} g(h_j) + R I_k$  \n",
        "- $w_0 = w_{EE} - \\alpha$ (effective self-excitation)  \n",
        "- $\\alpha$: Global inhibition strength  \n",
        "**Selection Conditions**:  \n",
        "- **Biased input**: $I_k > I_j + \\Delta_{\\text{crit}}$  \n",
        "- **Hysteresis**: Once dominant, $E_k$ suppresses alternatives  \n",
        "\n",
        "#### Phase Portrait Analysis  \n",
        "\n",
        "| State                           | Condition                                    | Stability            |\n",
        "|--------------------------------|-----------------------------------------------|----------------------|\n",
        "| **Spontaneous activity**       | $I_1 = I_2 = 0$                               | Stable fixed point   |\n",
        "| **Coexistence**                | $ \| I_1 - I_2 \| < \\Delta_{\\text{crit}}$      | Unstable             |\n",
        "| **Decision (winner-take-all)** | $ \| I_1 - I_2 \| > \\Delta_{\\text{crit}}$      | Bistable attractors  |\n",
        "\n",
        "#### Biological Evidence  \n",
        "- **Neural correlates**:  \n",
        "  - LIP neurons encode decision commitment (ramping activity)  \n",
        "  - Firing rates $\\propto$ evidence accumulation: $r(t) = \\beta \\int_0^t (I_1(\\tau) - I_2(\\tau)) d\\tau$  \n",
        "- **Timescales**:  \n",
        "  Sensory $\\tau_{\\text{sens}} \\approx 20 \\text{ ms}$ → Decision $\\tau_{\\text{dec}} \\approx 100-500 \\text{ ms}$ → Action $\\tau_{\\text{mot}} \\approx 50 \\text{ ms}$  \n",
        "\n",
        "> Note: $g_E(h) = \\frac{1}{1 + e^{-k(h-\\theta)}}$ (sigmoidal gain), $\\Delta_{\\text{crit}} \\propto \\alpha / w_0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCySxbbT-DHa"
      },
      "source": [
        "## **18. Neural Encoding and Decoding**  \n",
        "### Principles of Information Representation in Spiking Networks  \n",
        "\n",
        "#### Encoding-Decoding Framework  \n",
        "- **Encoding**: Map stimulus $x \\in \\mathbb{R}^n$ to spike trains $S(t)$  \n",
        "  $I_i(t) = \\alpha_i x(t) + I_i^{\\text{bias}}$  \n",
        "  $a_i(t) = \\text{NL}(I_i(t))$  \n",
        "  where $\\alpha_i$ = tuning slope, $\\text{NL}$ = neuron model (e.g., LIF), $a_i$ = activity  \n",
        "- **Decoding**: Reconstruct $\\hat{x}$ from population activity  \n",
        "  $\\hat{x}(t) = \\sum_{i=1}^N d_i a_i(t)$  \n",
        "  $d_i$ = decoding weights (optimized via $\\min \\langle ||x - \\hat{x}||^2 \\rangle$)  \n",
        "\n",
        "#### Tuning Properties  \n",
        "- **Neuron response curves**:  \n",
        "  $a_i(x) = G \\left[ \\cos \\left( \\frac{2\\pi}{T} (x - x_i^{\\text{pref}}) \\right) \\right]$  \n",
        "  (Bell-shaped tuning for orientation/position)  \n",
        "- **Reconstruction accuracy**:  \n",
        "  $\\text{MSE} \\propto 1/N$ (improves with population size)  \n",
        "\n",
        "#### Temporal Coding Schemes  \n",
        "| Scheme                  | Information Carrier                     | Mathematical Representation      |  \n",
        "|-------------------------|-----------------------------------------|----------------------------------|  \n",
        "| **Rate Coding**         | Mean firing rate $\\bar{r} = \\frac{1}{T}\\int_0^T s(t)dt$ | $\\hat{x} \\propto \\bar{r}$        |  \n",
        "| **Time-to-First-Spike** | Latency $\\Delta t = t_{\\text{spike}} - t_{\\text{stim}}$ | $\\hat{x} \\propto 1/\\Delta t$     |  \n",
        "| **Rank-Order Coding**   | Spike sequence order                    | $\\text{rank}(i) = f(x)$          |  \n",
        "| **Latency Coding**      | Relative timing $\\{\\Delta t_k\\}$        | $\\hat{x} = g(\\Delta t_1, \\dots, \\Delta t_k)$ |  \n",
        "| **Synchrony Coding**    | Coincident spikes across neurons        | $\\hat{x} \\propto \\sum_{i<j} \\delta(t_i - t_j)$ |  \n",
        "| **Phase Coding**        | Spike phase $\\phi$ in oscillation cycle | $\\phi_i = 2\\pi \\frac{t_i - t_{\\text{cycle}}}{T_{\\text{osc}}}$ |  \n",
        "| **Resonant Burst**      | Burst frequency $f_{\\text{burst}}$      | $\\hat{x} \\propto f_{\\text{burst}}$ |  \n",
        "\n",
        "#### Population Temporal Coding  \n",
        "- **Temporal population code**:  \n",
        "  Preferred time $T_k(a)$ for stimulus $a$:  \n",
        "  $T_k(a) = t_{\\text{max}} \\left(1 - \\frac{|a - a_k^{\\text{pref}}|}{\\Delta a_{\\text{max}}}\\right)$  \n",
        "- **Decoding**:  \n",
        "  $\\hat{a} = \\underset{a}{\\text{argmin}} \\sum_k (t_k - T_k(a))^2$  \n",
        "\n",
        "#### Key Principles  \n",
        "1. **Efficiency tradeoff**:  \n",
        "   Rate coding: Low bandwidth, high noise immunity  \n",
        "   Temporal coding: High information density, precise timing  \n",
        "2. **Biological evidence**:  \n",
        "   - Retina: Latency coding for rapid transmission  \n",
        "   - Cortex: Phase coding in theta/gamma oscillations  \n",
        "   - Hippocampus: Theta-phase precession  \n",
        "3. **Decoding optimization**:  \n",
        "   Linear decoders $d_i$ minimize MSE when $a_i(x)$ are linearly independent  \n",
        "\n",
        "> Note: $s(t) = \\sum_f \\delta(t - t^{(f)})$, $G$ = activation function, $t_{\\text{cycle}}$ = oscillation reference time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYBdKFdr-OPo"
      },
      "source": [
        "## **19. Spiking Neural Networks (SNNs)**  \n",
        "### Architecture and Computational Principles  \n",
        "\n",
        "#### Core Principles  \n",
        "1. **Representation**:  \n",
        "   - **Encoding**: Nonlinear mapping of stimuli to spikes  \n",
        "     $I_j = \\sum_i w_{ij} a_i + I_j^{\\text{bias}}$  \n",
        "   - **Decoding**: Linear reconstruction $\\hat{x} = \\sum_j d_j a_j$  \n",
        "2. **Transformation**:  \n",
        "   Functions $f(x)$ implemented via synaptic weights:  \n",
        "   $\\hat{f}(x) = \\sum_j d_j f(a_j(x))$  \n",
        "3. **Dynamics**:  \n",
        "   Neural representations as state variables in control theory:  \n",
        "   $\\tau \\frac{d\\mathbf{u}}{dt} = -\\mathbf{u} + W \\mathbf{a} + \\mathbf{I}_{\\text{ext}}$  \n",
        "\n",
        "#### Network Architecture  \n",
        "- **Layered structure**:  \n",
        "  Input → Hidden layers → Output  \n",
        "- **Neuron state**:  \n",
        "  Membrane potential $u_j(t)$  \n",
        "- **Spike emission**:  \n",
        "  $F_j = \\{ t_j^{(f)} \\mid u_j(t_j^{(f)}) = \\theta \\}$  \n",
        "- **Postsynaptic integration**:  \n",
        "  $\\Delta u_j(t) = \\sum_{i \\in \\Gamma_j} w_{ji} \\epsilon(t - t_i^{(g)} - d_{ji})$  \n",
        "  where $\\Gamma_j$ = presynaptic neurons, $d_{ji}$ = synaptic delay  \n",
        "\n",
        "#### Spike Response Model  \n",
        "- **Response function**:  \n",
        "  $\\epsilon(s) = \\frac{s}{\\tau} \\exp\\left(1 - \\frac{s}{\\tau}\\right) H(s)$  \n",
        "  $H(s) = \\begin{cases} 0 & s \\leq 0 \\\\ 1 & s > 0 \\end{cases}$  \n",
        "- **Parameters**:  \n",
        "  $\\tau$ = decay constant (≈10-100 ms), $w_{ji}$ = synaptic efficacy  \n",
        "\n",
        "#### Synaptic Transmission  \n",
        "| Component       | Symbol    | Role                          |  \n",
        "|-----------------|-----------|-------------------------------|  \n",
        "| Presynaptic spike | $t_i^{(g)}$ | Trigger of PSP               |  \n",
        "| Synaptic delay  | $d_{ji}$  | Axonal/conduction delay (1-100 ms) |  \n",
        "| Weight          | $w_{ji}$  | Excitatory ($>0$)/inhibitory ($<0$) |  \n",
        "| PSP kernel      | $\\epsilon(s)$ | Temporal filtering          |  \n",
        "\n",
        "#### Information Flow  \n",
        "1. **Input encoding**: Stimulus → Spike trains via tuning curves  \n",
        "2. **Hidden processing**:  \n",
        "   - Potential dynamics: $u_j(t) = \\sum_i \\int_0^\\infty \\epsilon(s) w_{ji} s_i(t-s) ds$  \n",
        "   - Spike generation: $u_j \\geq \\theta \\rightarrow$ output spike  \n",
        "3. **Output decoding**: $\\hat{y} = \\sum_k d_k a_k(t)$  \n",
        "\n",
        "#### Biological Alignment  \n",
        "- **Temporal precision**: Millisecond-scale spike timing  \n",
        "- **Energy efficiency**: Event-driven computation  \n",
        "- **Plasticity**: STDP learning rule $\\Delta w_{ij} \\propto e^{-\\Delta t / \\tau}$  \n",
        "  ($\\Delta t = t_{\\text{post}} - t_{\\text{pre}}$)  \n",
        "\n",
        "> Note: $s_i(t) = \\sum_g \\delta(t - t_i^{(g)})$, $a_j(t)$ = postsynaptic activity, $\\theta \\approx -55 \\text{ mV}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhVSd0kr_3ph"
      },
      "source": [
        "## **20. Learning and Plasticity**\n",
        "### Mechanisms of Adaptive Neural Circuits\n",
        "\n",
        "#### Synaptic Plasticity  \n",
        "Long-term changes in synaptic efficacy:  \n",
        "- **Long-Term Potentiation (LTP)**: Persistent increase in synaptic strength following high-frequency stimulation.  \n",
        "- **Long-Term Depression (LTD)**: Decrease in synaptic strength after prolonged low-frequency stimulation.  \n",
        "Experimental basis: LTP/LTD can last hours/days, with weight change $\\Delta w_{ij} \\propto$ activity correlation.  \n",
        "\n",
        "#### Learning Rules  \n",
        "Adaptation of synaptic weights $w_{ij}$ to optimize network performance:  \n",
        "1. **Supervised Learning**:  \n",
        "   Error-driven weight updates (e.g., $\\Delta w_{ij} = \\eta (y_{\\text{true}} - y_{\\text{pred}}) x_i$); limited biological evidence.  \n",
        "2. **Unsupervised Learning**:  \n",
        "   Self-organized plasticity driven by input statistics:  \n",
        "   - Hebbian rule: $\\Delta w_{ij} = \\eta a_i a_j$  \n",
        "   - Spike-timing-dependent plasticity (STDP):  \n",
        "$\\Delta w = \\begin{cases}\n",
        "A_+ e^{-\\Delta t / \\tau_+} & \\text{if } \\Delta t > 0 \\\\\n",
        "A_- e^{\\Delta t / \\tau_-} & \\text{if } \\Delta t < 0\n",
        "\\end{cases}$  \n",
        "($\\Delta t = t_{\\text{post}} - t_{\\text{pre}}$)\n",
        "\n",
        "3. **Reinforcement Learning**:  \n",
        "   Reward-modulated plasticity (e.g., $\\Delta w_{ij} = \\eta R \\delta_i a_j$); aligns with dopaminergic signaling.  \n",
        "\n",
        "#### Learning in Spiking Neural Networks  \n",
        "- **Parameters**: Synaptic weights $w_{ij}$ and delays $d_{ij}$.  \n",
        "- **Challenge**: Simultaneous optimization of weights and delays.  \n",
        "- **Solution**: Multiple connections per neuron pair with fixed delays:  \n",
        "  Total PSP: $\\Delta u_j(t) = \\sum_{k=1}^m w_{ij}^k \\epsilon(t - t_i - d_{ij}^k)$  \n",
        "  Learning adjusts $\\{w_{ij}^k\\}$ while $d_{ij}^k$ remain fixed.  \n",
        "- **Biological correlates**:  \n",
        "  - Weight plasticity: LTP/LTD via Ca²⁺ dynamics  \n",
        "  - Delay plasticity: Myelination changes (long-term)  \n",
        "\n",
        "#### Structural Modifications  \n",
        "- **Axonal delay tuning**:  \n",
        "  Myelination alters conduction velocity ($v \\propto \\sqrt{d_{\\text{axon}}}$).  \n",
        "- **Multi-connection framework**:  \n",
        "  Enables temporal coding refinement without dynamic delay adjustments.  \n",
        "\n",
        "#### Functional Outcomes  \n",
        "- **Associative memory**: Hebbian plasticity stores pattern correlations.  \n",
        "- **Temporal coding**: STDP refines spike-time precision.  \n",
        "- **Behavioral adaptation**: Reinforcement learning optimizes reward acquisition.  \n",
        "\n",
        "> Note: $\\eta$ = learning rate, $R$ = reward signal, $\\delta_i$ = neural error signal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eAP4xgZBtAJ"
      },
      "source": [
        "## **21. Unsupervised Learning and Its Rules**  \n",
        "### Hebbian Plasticity and Beyond  \n",
        "\n",
        "#### Hebb's Principle  \n",
        "Core postulate: Synaptic strength increases when presynaptic and postsynaptic activity correlate:  \n",
        "$\\frac{dw_{ij}}{dt} = F(w_{ij}, v_i, v_j)$  \n",
        "- **Locality**: $F$ depends only on $w_{ij}$, $v_i$ (presynaptic), $v_j$ (postsynaptic)  \n",
        "- **Joint activity**: Taylor expansion:  \n",
        "  $\\frac{dw_{ij}}{dt} = c_0 + c_1^{\\text{pre}}v_i + c_1^{\\text{post}}v_j + c_{11}^{\\text{corr}}v_i v_j + \\mathcal{O}(v^2)$  \n",
        "  Hebbian term: $c_{11}^{\\text{corr}} > 0$  \n",
        "\n",
        "#### Stabilization Mechanisms  \n",
        "To prevent unbounded weight growth:  \n",
        "1. **Hard bounds**:  \n",
        "   $c_{11}^{\\text{corr}} = \\begin{cases} \\gamma & 0 < w_{ij} < w^{\\max} \\\\ 0 & \\text{otherwise} \\end{cases}$  \n",
        "2. **Soft bounds**:  \n",
        "   $c_{11}^{\\text{corr}} = \\gamma (w^{\\max} - w_{ij})^\\beta$  \n",
        "3. **Decay term**:  \n",
        "   $\\frac{dw_{ij}}{dt} = \\gamma(1 - w_{ij})v_i v_j - \\gamma_0 w_{ij}$  \n",
        "   (spontaneous decay when $v_i v_j = 0$)  \n",
        "\n",
        "#### Oja's Rule  \n",
        "Normalized Hebbian learning:  \n",
        "$\\frac{dw_{ij}}{dt} = \\gamma (v_i v_j - w_{ij} v_j^2)$  \n",
        "- **Convergence**: $\\sum_i w_{ij}^2 \\to 1$  \n",
        "- **Competition**: Synapses compete for limited resources  \n",
        "\n",
        "#### Spike-Timing-Dependent Plasticity (STDP)  \n",
        "**Pair-based model**:  \n",
        "- **Traces**:  \n",
        "  Presynaptic: $\\frac{dx_j}{dt} = -\\frac{x_j}{\\tau_+} + \\sum_f \\delta(t - t_j^{(f)})$  \n",
        "  Postsynaptic: $\\frac{dy_i}{dt} = -\\frac{y_i}{\\tau_-} + \\sum_f \\delta(t - t_i^{(f)})$  \n",
        "- **Weight update**:  \n",
        "  $\\frac{dw_{ij}}{dt} = A_+(w_{ij}) x_j \\sum_f\\delta(t - t_i^{(f)}) - A_-(w_{ij}) y_i \\sum_f\\delta(t - t_j^{(f)})$  \n",
        "  where $\\Delta t = t_{\\text{post}} - t_{\\text{pre}}$  \n",
        "  - LTP: $A_+ e^{-|\\Delta t|/\\tau_+}$ for $\\Delta t > 0$  \n",
        "  - LTD: $A_- e^{-|\\Delta t|/\\tau_-}$ for $\\Delta t < 0$  \n",
        "\n",
        "#### Triplet STDP Model  \n",
        "Addresses frequency dependence in pair-based STDP:  \n",
        "- **Traces**:  \n",
        "  Presynaptic: $x_j$ ($\\tau_+ \\approx 50 \\text{ ms}$)  \n",
        "  Postsynaptic: $y_{i1}$ ($\\tau_1 \\approx 20 \\text{ ms}$), $y_{i2}$ ($\\tau_2 \\approx 200 \\text{ ms}$)  \n",
        "- **Weight changes**:  \n",
        "  LTP: $\\Delta w^+ = A_+ x_j y_{i2}$ (at postsynaptic spike)  \n",
        "  LTD: $\\Delta w^- = A_- y_i x_j$ (at presynaptic spike)  \n",
        "- **Frequency response**:  \n",
        "  High repetition rates enhance potentiation via slow trace $y_{i2}$  \n",
        "\n",
        "#### Biological Significance  \n",
        "| Rule          | Biological Basis                     | Functional Role                |  \n",
        "|---------------|--------------------------------------|--------------------------------|  \n",
        "| Hebbian       | NMDA receptor Ca²⁺ influx            | Pattern association            |  \n",
        "| Oja's         | Homeostatic scaling                  | Input normalization            |  \n",
        "| Pair-based STDP | Pre/post Ca²⁺ dynamics              | Temporal sequence learning     |  \n",
        "| Triplet STDP  | Metabotropic glutamate receptors     | Frequency-dependent filtering  |  \n",
        "\n",
        ">Note: $\\tau_+, \\tau_-, \\tau_1, \\tau_2$ = time constants, $A_+, A_-$ = learning rates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMfNIBNHOng6"
      },
      "source": [
        "## **22. Reinforcement Learning and Its Rules**  \n",
        "### Biological Mechanisms and Computational Models\n",
        "\n",
        "#### Reinforcement Learning Framework  \n",
        "- **Agent-environment interaction**:  \n",
        "  Agent selects actions → Environment returns states and rewards  \n",
        "- **Key challenges**:  \n",
        "  - Delayed rewards (seconds after actions)  \n",
        "  - Credit assignment: Attributing rewards to prior cues/actions  \n",
        "  - Global neuromodulatory signals affecting specific synapses  \n",
        "\n",
        "#### Conditioning Paradigms  \n",
        "1. **Classical (Pavlovian) conditioning**:  \n",
        "   Neutral stimulus (NS) + Unconditioned stimulus (US) → Conditioned stimulus (CS) elicits Conditioned response (CR)  \n",
        "   Example: Tone (NS) + Food (US) → Salivation (CR) to tone alone  \n",
        "2. **Operant (instrumental) conditioning**:  \n",
        "   Actions associated with consequences (reward/punishment)  \n",
        "   Example: Lever press → Food reward  \n",
        "\n",
        "#### Dopaminergic System  \n",
        "- **Pathways**:  \n",
        "  - Ventral tegmental area (VTA) → Nucleus accumbens (reward processing)  \n",
        "  - Substantia nigra → Striatum (motor control)  \n",
        "- **Function**:  \n",
        "  - Phasic dopamine (DA) signals reward prediction error:  \n",
        "    $\\Delta \\text{DA} \\propto R_{\\text{actual}} - R_{\\text{predicted}}$  \n",
        "  - Modulates synaptic plasticity via D1/D5 receptors (enhances LTP)  \n",
        "\n",
        "#### Reward-Modulated STDP (R-STDP)  \n",
        "**Synaptic eligibility trace model**:  \n",
        "- **Variables**:  \n",
        "  - $s$: Synaptic weight  \n",
        "  - $c$: Eligibility trace (slow decay, $\\tau_c \\approx 1 \\text{ s}$)  \n",
        "  - $d$: Extracellular DA concentration  \n",
        "- **Dynamics**:  \n",
        "  $\\frac{dc}{dt} = -\\frac{c}{\\tau_c} + \\text{STDP}(\\tau) \\delta(t - t_{\\text{pre/post}})$  \n",
        "  $\\frac{ds}{dt} = c \\cdot d$  \n",
        "  $\\frac{dd}{dt} = -\\frac{d}{\\tau_d} + \\text{DA}(t)$  \n",
        "  where $\\tau = t_{\\text{post}} - t_{\\text{pre}}$, $\\text{STDP}(\\tau)$ = pair-based update  \n",
        "\n",
        "#### DA-STDP Interaction  \n",
        "| Condition               | DA Effect on STDP                   | Functional Outcome               |  \n",
        "|-------------------------|-------------------------------------|----------------------------------|  \n",
        "| **DA present**          | Enhances LTP, suppresses LTD        | Strengthens reward-triggering pathways |  \n",
        "| **DA absent**           | No modulation                       | Neutral plasticity               |  \n",
        "| **Unexpected reward**   | DA surge → Potentiation             | Learn novel associations         |  \n",
        "| **Predicted reward**    | No DA change                        | Maintain current weights         |  \n",
        "| **Omitted reward**      | DA dip → Depression                 | Extinguish incorrect predictions |  \n",
        "\n",
        "#### Spiking Network Implementation  \n",
        "- **Architecture**:  \n",
        "  1000 neurons with:  \n",
        "  - Input group $S$ (50 neurons)  \n",
        "  - Action groups $A$ and $B$ (50 neurons each)  \n",
        "- **Decision**:  \n",
        "  After stimulus, action selected if $|A| > |B|$ or $|B| > |A|$  \n",
        "- **Reward**:  \n",
        "  Delayed by $\\leq 1 \\text{ s}$ for correct action  \n",
        "- **Learning outcome**:  \n",
        "  Probability of rewarded action increases from 50% to >90% via R-STDP  \n",
        "\n",
        "#### Biological Insights  \n",
        "- **Eligibility traces**:  \n",
        "  Resolve distal reward problem by \"tagging\" synapses active near reward time  \n",
        "- **DA timescales**:  \n",
        "  Phasic signals ($\\tau_d \\approx 200 \\text{ ms}$) gate plasticity within behavioral windows  \n",
        "- **Striatal circuits**:  \n",
        "  Basal ganglia implement action selection policies  \n",
        "\n",
        "> Note: $\\text{STDP}(\\tau) = A_+ e^{-\\tau/\\tau_+}$ for $\\tau>0$, $A_- e^{\\tau/\\tau_-}$ for $\\tau<0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B59LwQzxPIRg"
      },
      "source": [
        "## **23. Polychronous Neuronal Groups**  \n",
        "### Concept Summary\n",
        "\n",
        "#### **Biological Evidence for Temporal Coding**  \n",
        "Classical views held that neurons transmit information via mean firing rates. Modern evidence shows neurons generate spike-timing patterns with **millisecond precision** *in vivo* and *in vitro*. These patterns occur in single neurons or across groups, forming **functional neuronal groups** activated by stimuli. Axonal conduction delays range from 0.1 ms to 44 ms and are precise/reproducible (±1 ms). This precision implies spike timing is critical, as random (asynchronous) firing weakens postsynaptic responses.  \n",
        "\n",
        "#### **Role of Delays in Polychronization**  \n",
        "Delays enable flexible signal routing. For example:  \n",
        "- Neurons $(b, c, d)$ fire with pattern $(0, 4, 8, 10)$ ms to activate neuron $a$.  \n",
        "- The same neurons fire with pattern $(0, 3, 7, 9)$ ms to activate neuron $e$.  \n",
        "Delays transform firing order into spatial targeting, creating multiple pathways from one neuron set.  \n",
        "\n",
        "#### **Polychronous Group Definition**  \n",
        "A **polychronous group (PG)** is a neuron ensemble activated by a **precise spike-timing pattern** (determined by connectivity and delays), generating reproducible millisecond-precision output. Example:  \n",
        "- Neurons $(125, 275, 490)$ fire at $(0, 3, 7)$ ms.  \n",
        "- Delays ensure simultaneous arrival at neuron $1$, triggering its spike at $t = 13$ ms.  \n",
        "\n",
        "#### **Network Architecture**  \n",
        "- **Size:** 1,000 neurons (80% excitatory, 20% inhibitory).  \n",
        "- **Connectivity:** Sparse random graph ($P(\\text{connection}) = 0.1$).  \n",
        "- **Delays:** Fixed integers $d \\in [1, 20]$ ms.  \n",
        "- **Synapses:** Excitatory weights evolve via **STDP**; inhibitory weights fixed.  \n",
        "- **Firing threshold:** $V_{\\text{th}} = 10$ mV (2 presynaptic spikes suffice).  \n",
        "\n",
        "#### **Emergent Rhythms**  \n",
        "- Initial state: Delta oscillations ($\\sim 4$ Hz).  \n",
        "- After STDP: Delta vanishes; activity becomes Poissonian.  \n",
        "- Later: Gamma rhythms emerge ($30–70$ Hz).  \n",
        "\n",
        "#### **Statistical Characteristics**  \n",
        "- **PG count:** 5,269 groups in 1,000-neuron network.  \n",
        "- **Core groups:** 471 PGs persist for 24h simulation.  \n",
        "- **Group size:** $25$ neurons (average).  \n",
        "- **Neuron sharing:** Each neuron belongs to $\\sim 131$ groups.  \n",
        "- **Time span:** Up to $2,000$ ms; **longest path:** $30$ ms.  \n",
        "\n",
        "#### **Computational Model: Selectionist Approach**  \n",
        "- **Instructionist:** Maps inputs to outputs via supervised/unsupervised learning.  \n",
        "- **Selectionist:**  \n",
        "  1. **Neuronal level:** STDP selects subgraphs with matched delays, forming PGs.  \n",
        "  2. **Group level:** PGs represent possible inputs; activation occurs when input matches a PG.  \n",
        "- **Learning:** Reinforce PGs for new inputs by potentiating weak connections between groups.  \n",
        "\n",
        "#### **Memory Capacity**  \n",
        "Networks with delays contain **more PGs than neurons**. A 1,000-neuron network holds >5,000 PGs, implying vast memory capacity for biological brains ($\\sim 10^{11}$ neurons).  \n",
        "\n",
        "> Note: Precise delays enable temporal coding, transforming neural circuits into reconfigurable delay-based networks with high memory density."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIXTIB2CPZnx"
      },
      "source": [
        "## **24. Visual System in the Brain**  \n",
        "### Core Principles and Pathways\n",
        "\n",
        "#### **Role of Thalamus in Sensory Processing**  \n",
        "The thalamus acts as a central relay station, directing sensory signals (visual, auditory, olfactory, gustatory, somatic, equilibrium) to corresponding cortical areas:  \n",
        "- Visual cortex (vision)  \n",
        "- Primary somatic sensory cortex (touch)  \n",
        "- Gustatory cortex (taste)  \n",
        "- Olfactory cortex (smell)  \n",
        "\n",
        "#### **Computational Vision Neuroscience (CVN)**  \n",
        "**Definition:** Field using computational models to understand biological visual processing.  \n",
        "**Motivation for study:**  \n",
        "1. Early visual stages are well-characterized  \n",
        "2. Precise stimulus control  \n",
        "3. Vision occupies large brain territory  \n",
        "4. Ideal test case for neural algorithms  \n",
        "\n",
        "#### **Fundamental Questions in Visual Processing**  \n",
        "1. **Algorithm:** What computations transform light input to perception?  \n",
        "2. **Implementation:** Which neural structures execute these algorithms?  \n",
        "3. **Functional decomposition:** How do components (retina, LGN, V1, etc.) contribute?  \n",
        "4. **Development:** Are circuits prewired or learned?  \n",
        "5. **Specificity:** Are algorithms vision-specific or domain-general?  \n",
        "\n",
        "#### **Visual Pathway Architecture**  \n",
        "**Retina → Cortex Pathway:**  \n",
        "1. Light enters retina → optic nerve  \n",
        "2. **Optic chiasm:** Partial decussation (left visual field → right hemisphere)  \n",
        "3. **Lateral geniculate nucleus (LGN):** Thalamic relay for visual data  \n",
        "4. **Optic radiations:** Project to striate cortex (V1)  \n",
        "5. **Superior colliculus:** Ancillary route for motion processing  \n",
        "\n",
        "#### **Dual Cortical Pathways**  \n",
        "- **Ventral (Temporal) Pathway:**  \n",
        "  - Route: V1 → V2 → V4 → Temporal cortex  \n",
        "  - Function: Object recognition (\"*what*\" pathway)  \n",
        "- **Dorsal (Parietal) Pathway:**  \n",
        "  - Route: V1 → V2 → Parietal cortex  \n",
        "  - Function: Spatial navigation/motor interaction (\"*where/how*\" pathway)  \n",
        "\n",
        "#### **Hierarchical Processing**  \n",
        "Visual information flows through increasingly complex representations:  \n",
        "```  \n",
        "Retina → LGN → V1 (edge detection) → V2 (shape primitives) → V4/MT (object features) → IT/PPC (categorical perception)  \n",
        "```  \n",
        "\n",
        "> Note: The visual system transforms photons into perception through hierarchically organized, parallel pathways specialized for object identity and spatial action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KqroLPsRCql"
      },
      "source": [
        "## **25. Retina and Lateral Geniculate Nucleus**  \n",
        "### Visual Pathway and Neural Processing\n",
        "\n",
        "#### **Eye Anatomy and Photoreceptors**  \n",
        "- **Retina**: Neural tissue with layered structure:  \n",
        "  - Photoreceptor layer (rods/cones)  \n",
        "  - Bipolar cell layer  \n",
        "  - Ganglion cell layer  \n",
        "- **Fovea**: Central region with highest cone density for acute vision.  \n",
        "- **Blind spot**: Optic nerve exit (no photoreceptors).  \n",
        "- **Photoreceptor types**:  \n",
        "  - **Rods**: Scotopic vision ($\\lambda_{\\text{peak}} \\approx 498$ nm). High sensitivity, low resolution, achromatic.  \n",
        "  - **Cones**: Photopic vision. Three types:  \n",
        "    - S-cones ($\\lambda_{\\text{peak}} \\approx 419$ nm)  \n",
        "    - M-cones ($\\lambda_{\\text{peak}} \\approx 531$ nm)  \n",
        "    - L-cones ($\\lambda_{\\text{peak}} \\approx 558$ nm)  \n",
        "\n",
        "#### **Retinal Circuitry**  \n",
        "**Signal Flow**:  \n",
        "Light → Photoreceptors → Bipolar cells → Ganglion cells → Optic nerve  \n",
        "- **Horizontal cells**: Lateral inhibition across $\\sim 100$ photoreceptors, enabling center-surround receptive fields.  \n",
        "- **Amacrine cells**: Modulate bipolar-ganglion synapses (motion processing).  \n",
        "\n",
        "#### **Bipolar Cell Receptive Fields**  \n",
        "- **On-center bipolar cells**:  \n",
        "  - Depolarize to light in center ($\\Delta V > 0$).  \n",
        "  - Hyperpolarize to glutamate.  \n",
        "- **Off-center bipolar cells**:  \n",
        "  - Depolarize to glutamate.  \n",
        "  - Hyperpolarize to light in center ($\\Delta V < 0$).  \n",
        "*Receptive field size*:  \n",
        "- Cone bipolar: Small (foveal, color-sensitive).  \n",
        "- Rod bipolar: Large (peripheral, color-insensitive).  \n",
        "\n",
        "#### **Ganglion Cell Types**  \n",
        "- **P-cells (Parvocellular)**:  \n",
        "  - Input: 1–3 cone bipolars.  \n",
        "  - Small receptive fields.  \n",
        "  - Color-sensitive, sustained responses.  \n",
        "  - Spatial resolution: $\\uparrow$  \n",
        "- **M-cells (Magnocellular)**:  \n",
        "  - Input: Many bipolars.  \n",
        "  - Large receptive fields.  \n",
        "  - Motion-sensitive, transient responses.  \n",
        "  - Temporal resolution: $\\uparrow$  \n",
        "\n",
        "#### **Receptive Field Model**  \n",
        "Center-surround antagonism modeled by **Difference of Gaussians (DoG)**:  \n",
        "$$ \\text{DoG}(x,y) = \\frac{1}{\\sqrt{2\\pi}} \\left( \\frac{1}{\\sigma_1} e^{-(x^2+y^2)/(2\\sigma_1^2)} - \\frac{1}{\\sigma_2} e^{-(x^2+y^2)/(2\\sigma_2^2)} \\right) $$  \n",
        "where $\\sigma_1 < \\sigma_2$ for on-center/off-surround.  \n",
        "\n",
        "#### **Lateral Geniculate Nucleus (LGN)**  \n",
        "- **Structure**: 6 layers per hemisphere:  \n",
        "  - Layers 1–2: Magnocellular (M-input).  \n",
        "  - Layers 3–6: Parvocellular (P-input).  \n",
        "  - Koniocellular: Interlaminar (color processing).  \n",
        "- **Retinotopy**:  \n",
        "  - Contralateral visual field → Layers 1,4,6.  \n",
        "  - Ipsilateral visual field → Layers 2,3,5.  \n",
        "- **Connectivity**:  \n",
        "  - 80–90% inputs from V1 (feedback), not retina.  \n",
        "\n",
        "#### **Visual Pathway Summary**  \n",
        "```  \n",
        "Light → Retina (photoreceptors → bipolar → ganglion) → Optic nerve → Optic chiasm (50% decussation) → LGN → Optic radiations → V1  \n",
        "```  \n",
        "\n",
        "> Note: Parallel processing (P/M pathways) enables simultaneous extraction of color/detail (ventral) and motion/spatial (dorsal) information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucJTeJzERSop"
      },
      "source": [
        "## **26. Visual Cortex Function**  \n",
        "### Hierarchical Processing and Neural Encoding\n",
        "\n",
        "#### **Dual Visual Pathways**  \n",
        "- **Ventral (Temporal) Pathway**: V1 → V2 → V4 → Inferotemporal cortex (IT).  \n",
        "  - Function: Object recognition (\"*what*\" pathway).  \n",
        "- **Dorsal (Parietal) Pathway**: V1 → V2 → MT → Posterior parietal cortex (PPC).  \n",
        "  - Function: Motion/spatial analysis (\"*where/how*\" pathway).  \n",
        "\n",
        "#### **V1 Organization: Hubel & Wiesel Findings**  \n",
        "- **Orientation Selectivity**: Neurons respond preferentially to edge orientations (0–180°).  \n",
        "  - *Tuning curve*: Quantifies response vs. orientation (peak = preferred orientation).  \n",
        "- **Columnar Architecture**:  \n",
        "  - **Orientation columns**: Vertical units with shared orientation preference.  \n",
        "  - **Ocular dominance columns**: Alternate input from left/right eyes (layers IVc).  \n",
        "- **Cortical Layers**:  \n",
        "  - Layer IV: Input from LGN (M/P pathways).  \n",
        "  - Layers II/III: Output to higher areas (V2, V4).  \n",
        "\n",
        "#### **V1 Neuron Types**  \n",
        "1. **Simple Cells**:  \n",
        "   - Receptive field: Elongated ON/OFF subregions (e.g., excitatory center, inhibitory flank).  \n",
        "   - Modeled by **Gabor filters**:  \n",
        "$$ g(x,y) = \\exp\\left(-\\frac{x'^2 + \\gamma^2 y'^2}{2\\sigma^2}\\right) \\cdot \\cos\\left(\\frac{2\\pi x'}{\\lambda}\\right) $$  \n",
        "     where $x' = x \\cos\\theta + y \\sin\\theta$, $y' = -x \\sin\\theta + y \\cos\\theta$ ($\\theta$: orientation, $\\lambda$: wavelength, $\\sigma$: scale, $\\gamma$: aspect ratio).  \n",
        "   - Respond to edges/bars at specific locations.  \n",
        "\n",
        "2. **Complex Cells**:  \n",
        "   - Phase-invariant: Respond to preferred orientation *anywhere* in receptive field.  \n",
        "   - Input: Convergent simple cells with shared orientation.  \n",
        "   - Detect motion direction.  \n",
        "\n",
        "3. **Hypercomplex (End-Stopped) Cells**:  \n",
        "   - Respond maximally to bars of *finite length* (inhibit long edges).  \n",
        "   - Curve: Impulses/second vs. slit length (degrees).  \n",
        "\n",
        "#### **Hierarchical Feature Extraction**  \n",
        "- **V1**: Edge detection (Gabor filters → simple cells).  \n",
        "- **V2/V4**: Shape primitives (curves, angles) → object features.  \n",
        "- **IT**: View-invariant object recognition (\"template matching\").  \n",
        "\n",
        "#### **Processing Workflow**  \n",
        "```  \n",
        "LGN → V1 (simple → complex → hypercomplex cells) → V2 (C1/S1 units) → V4 (C2/S2 units) → IT (view-tuned units)  \n",
        "```  \n",
        "*Higher areas integrate features via MAX-like operations.*  \n",
        "\n",
        "Note: V1 transforms LGN inputs into oriented edge representations, initiating a feature hierarchy that culminates in invariant object recognition in IT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TXtVafCRZjM"
      },
      "source": [
        "## **27. Modeling Levels in Primary Visual Cortex**\n",
        "### Descriptive, Mechanistic, and Interpretive Approaches\n",
        "\n",
        "#### **Three Modeling Paradigms**  \n",
        "1. **Descriptive Models**:  \n",
        "   - Quantify neural responses to stimuli (encoding/decoding).  \n",
        "   - Example: Retinal ganglion cells (RGCs) exhibit center-surround receptive fields:  \n",
        "     - *On-center RGCs*: Fire when light hits center (inhibited by surround).  \n",
        "     - *Off-center RGCs*: Fire when dark hits center (inhibited by surround).  \n",
        "   - Mathematical model: **Difference of Gaussians (DoG)** for center-surround antagonism:  \n",
        "$$ \\text{DoG}(x,y) = \\frac{1}{\\sqrt{2\\pi}} \\left( \\frac{1}{\\sigma_1} e^{-(x^2+y^2)/(2\\sigma_1^2)} - \\frac{1}{\\sigma_2} e^{-(x^2+y^2)/(2\\sigma_2^2)} \\right) $$  \n",
        "     where $\\sigma_1 < \\sigma_2$.  \n",
        "\n",
        "2. **Mechanistic Models**:  \n",
        "   - Simulate neural behavior computationally.  \n",
        "   - **V1 simple cells**: Modeled as **Gabor filters** capturing orientation selectivity:  \n",
        "$$ g(x,y) = \\exp\\left(-\\frac{x'^2 + \\gamma^2 y'^2}{2\\sigma^2}\\right) \\cdot \\cos\\left(\\frac{2\\pi x'}{\\lambda}\\right) $$  \n",
        "     with $x' = x \\cos\\theta + y \\sin\\theta$, $y' = -x \\sin\\theta + y \\cos\\theta$ ($\\theta$: orientation, $\\lambda$: spatial frequency, $\\sigma$: scale, $\\gamma$: aspect ratio).  \n",
        "   - Implementation: Convolve input images with Gabor filter banks at multiple orientations/scales.  \n",
        "\n",
        "3. **Interpretive Models**:  \n",
        "   - Explain *why* circuits operate as observed.  \n",
        "   - **Efficient Coding Hypothesis**: V1 simple cells optimize information representation.  \n",
        "     - Goal: Faithfully reconstruct input image $I$ using neural responses $r_i$ and receptive fields $\\text{RF}_i$:  \n",
        "       $$ \\hat{I} = \\sum_i \\text{RF}_i \\cdot r_i $$  \n",
        "     - Optimize $\\text{RF}_i$ to minimize reconstruction error $\\|I - \\hat{I}\\|^2$ while maximizing response independence.  \n",
        "   - Biological validation: Gabor-like RFs emerge when trained on natural images.  \n",
        "\n",
        "#### **Key Insights**  \n",
        "- **V1 circuit**: Integrates LGN inputs (DoG-filtered) into oriented edge detectors (Gabor filters).  \n",
        "- **Computational advantage**: Oriented RFs efficiently encode natural scene statistics.  \n",
        "- **Hierarchy**: Descriptive → Mechanistic → Interpretive models deepen causal understanding.  \n",
        "\n",
        "> Note: V1’s edge-detection strategy balances biological plausibility (mechanistic) with optimal information encoding (interpretive)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6RYnCcbT7Hm"
      },
      "source": [
        "## **28. Neural Connectivity Patterns in Brain Regions**  \n",
        "### Bio-Inspired Computational Models\n",
        "\n",
        "#### **Connectivity Schemes in Neural Models**  \n",
        "- **Full Connectivity**: All neurons connect across layers (common in classification tasks).  \n",
        "- **Random Connectivity**: Sparse connections for efficiency.  \n",
        "- **Convolutional Connectivity**: Local receptive fields enabling weight sharing, sparsity, and equivariance.  \n",
        "- **Pooling Connectivity**: Downsampling for invariance and compression (local/global).  \n",
        "\n",
        "#### **Convolution Operations**  \n",
        "**Continuous 1D Convolution**:  \n",
        "$$ (f * g)(x) = \\int_{-\\infty}^{\\infty} f(u)g(x-u)  du $$  \n",
        "*Represents function overlap (e.g., temporal/spatial filtering).*  \n",
        "\n",
        "**Discrete 1D Convolution**:  \n",
        "$$ (f * g)(x) = \\sum_{\\omega=-\\infty}^{\\infty} f(\\omega)g(x-\\omega) $$  \n",
        "*Vector example*: For $a = [1, 2, 3]^T$, $b = [4, 5, 6]^T$:  \n",
        "$$ a * b = [4, 13, 28, 27, 18]^T $$  \n",
        "\n",
        "**2D Convolution (Image Processing)**:  \n",
        "$$ (f * g)(x,y) = \\sum_{\\omega} \\sum_{v} f(\\omega,v)g(x-\\omega,y-v) $$  \n",
        "*Matrix example*: Kernel $A$ applied to image $B$ yields $c_{33} = \\sum_{i,j} a_{ij} b_{33-i,33-j}$.  \n",
        "\n",
        "**Key Properties**:  \n",
        "- Commutative: $f * g = g * f$  \n",
        "- Associative: $f * (g * h) = (f * g) * h$  \n",
        "- Distributive: $f * (g + h) = f*g + f*h$  \n",
        "\n",
        "#### **Practical Implementation**  \n",
        "- **Stride ($s$)**: Step size for kernel sliding (e.g., $s=2$ halves output dimensions).  \n",
        "  - Input $7\\times7$ → $3\\times3$ output with $s=2$.  \n",
        "- **Padding**: Adds zeros to borders to preserve spatial dimensions.  \n",
        "  - Input $5\\times5$ + padding $1$ → $5\\times5$ output with $3\\times3$ kernel.  \n",
        "\n",
        "#### **Pooling Operations**  \n",
        "**Purpose**: Feature compression and shift invariance.  \n",
        "- **Max Pooling**: Selects maximum value in window (e.g., $2\\times2$ window).  \n",
        "- **Average Pooling**: Computes mean in window.  \n",
        "*Example*:  \n",
        "$$ \\begin{bmatrix} 5 & 2 \\\\ 1 & 8 \\end{bmatrix} \\xrightarrow{\\text{max}} 8, \\quad \\xrightarrow{\\text{avg}} 4 $$  \n",
        "\n",
        "#### **Convolutional Neural Networks (CNNs)**  \n",
        "**Architecture**:  \n",
        "1. **Convolutional Blocks**:  \n",
        "   - Alternating convolution and pooling layers.  \n",
        "   - Feature extraction via learned kernels.  \n",
        "2. **Fully Connected (FC) Layers**:  \n",
        "   - Classification/regression (e.g., FC → softmax).  \n",
        "**Types**:  \n",
        "- *Shallow*: Few convolutional layers.  \n",
        "- *Deep*: Multiple blocks (e.g., AlexNet, VGG).  \n",
        "\n",
        "**Biological Inspiration**:  \n",
        "- Mimics hierarchical processing (V1 → IT cortex).  \n",
        "- Convolution ≈ Local receptive fields in V1.  \n",
        "- Pooling ≈ Complex cell invariance.  \n",
        "\n",
        "> Note: CNNs leverage bio-inspired connectivity (convolution/pooling) to achieve translation invariance and hierarchical feature learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TExFj-DVUIP"
      },
      "source": [
        "## **29. HMAX Computational Model for Object Recognition**\n",
        "### Hierarchical Feature Extraction in the Ventral Stream\n",
        "\n",
        "#### **Biological Foundations**  \n",
        "The HMAX model (Riesenhuber & Poggio, 1999) emulates the ventral visual pathway’s hierarchical processing:  \n",
        "1. **Hierarchical Invariance**: Progressively builds invariance to position, scale, and transformations.  \n",
        "2. **Receptive Field Complexity**: RF size and stimulus complexity increase along the hierarchy (V1 → IT).  \n",
        "3. **Feedforward Dominance**: Initial rapid recognition relies on feedforward signals without feedback.  \n",
        "\n",
        "#### **Model Architecture**  \n",
        "Four-layer alternating *selective* (S) and *invariant* (C) units:  \n",
        "```  \n",
        "Input → S1 (edge detection) → C1 (position/scale invariance) → S2 (feature matching) → C2 (global invariance)  \n",
        "```  \n",
        "\n",
        "#### **S1 Units: V1 Simple Cells**  \n",
        "- **Gabor Filters**: Model orientation-selective RFs:  \n",
        "  $$ F(x,y) = \\exp\\left(-\\frac{x_0^2 + \\gamma^2 y_0^2}{2\\sigma^2}\\right) \\cdot \\cos\\left(\\frac{2\\pi}{\\lambda}x_0\\right) $$  \n",
        "  where $x_0 = x\\cos\\theta + y\\sin\\theta$, $y_0 = -x\\sin\\theta + y\\cos\\theta$.  \n",
        "- **Parameters**:  \n",
        "  - 16 scales ($7\\times7$ to $37\\times37$ pixels).  \n",
        "  - 4 orientations ($0^\\circ, 45^\\circ, 90^\\circ, 135^\\circ$).  \n",
        "  - $\\sigma$, $\\lambda$ scaled with filter size (e.g., $7\\times7$: $\\sigma=2.8$, $\\lambda=3.5$).  \n",
        "\n",
        "#### **C1 Units: V1 Complex Cells**  \n",
        "- **MAX Pooling**: Achieves position/scale invariance:  \n",
        "  $$ r = \\max\\{x_1, x_2, \\dots, x_m\\} $$  \n",
        "  where $x_i$ are S1 responses within a local grid.  \n",
        "- **Scale Bands**: Pool adjacent S1 scales (e.g., Band 1: $7\\times7$ and $9\\times9$ filters).  \n",
        "- **Grid Sparsity**: Responses computed at subsampled locations (overlap $\\Delta s = s/2$).  \n",
        "\n",
        "#### **S2 Units: Feature Matching**  \n",
        "- **Prototype Similarity**: Gaussian tuning to C1 patches:  \n",
        "  $$ r = \\exp(-\\beta \\|X - P_i\\|^2) $$  \n",
        "  where $X$ = input patch, $P_i$ = stored prototype, $\\beta$ = tuning sharpness.  \n",
        "- **Multi-Scale Maps**: Computed for all positions/scale bands per prototype.  \n",
        "\n",
        "#### **C2 Units: Global Invariance**  \n",
        "- **Global MAX**: Invariant response per prototype:  \n",
        "  $$ r_i = \\max_{\\text{positions, scales}} \\text{(S2 response for } P_i) $$  \n",
        "- **Output**: Vector of $N$ C2 features ($N$ = number of prototypes).  \n",
        "\n",
        "#### **Learning and Classification**  \n",
        "1. **Prototype Extraction**:  \n",
        "   - Sample $N$ random patches from C1 activations of *positive* training images.  \n",
        "   - Patch sizes: $4\\times4$, $8\\times8$, $12\\times12$, $16\\times16$ (each includes 4 orientations).  \n",
        "2. **Classification**:  \n",
        "   - C2 feature vector → Linear SVM.  \n",
        "\n",
        "#### **Key Parameters**  \n",
        "| Scale Band | C1 Grid   | $\\Delta s$ | S1 Sizes       | $\\sigma$ | $\\lambda$ |  \n",
        "|------------|-----------|-----------|----------------|----------|-----------|  \n",
        "| Band 1     | $8\\times8$ | 4        | $7\\times7$     | 2.8      | 3.5       |  \n",
        "|            |           |           | $9\\times9$     | 3.6      | 4.6       |  \n",
        "| Band 2     | $10\\times10$ | 5        | $11\\times11$   | 4.5      | 5.6       |  \n",
        "| ...        | ...       | ...       | ...            | ...      | ...       |  \n",
        "\n",
        "> Note: HMAX combines bio-inspired hierarchies (S1/C1 ≈ V1, S2/C2 ≈ V4/IT) with MAX operations for invariance, enabling robust object recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkkH7P2xWFZ1"
      },
      "source": [
        "## **30. Spiking HMAX Model with Learning**\n",
        "### Temporal Coding and Unsupervised Feature Learning\n",
        "\n",
        "#### **Biological Motivation**  \n",
        "The ventral stream achieves rapid invariant recognition (100–112.5 ms) despite variations in position, size, and illumination. Key properties:  \n",
        "- **Hierarchy**: V1 → V2 → V4 → IT with increasing RF size/complexity.  \n",
        "- **Feedforward Dominance**: Initial recognition wave without feedback.  \n",
        "- **Temporal Coding**: First spikes carry maximal information (\"time-to-first-spike\").  \n",
        "\n",
        "#### **Spiking HMAX Architecture**  \n",
        "Masquelier & Thorpe (2007) extend HMAX with spiking neurons and STDP:  \n",
        "```  \n",
        "Input → S1 (edges) → C1 (local MAX) → S2 (STDP learning) → C2 (global MAX) → Classifier  \n",
        "```  \n",
        "**Core Innovations**:  \n",
        "- Spiking integrate-and-fire (IF) neurons.  \n",
        "- Intensity-to-latency input encoding.  \n",
        "- Unsupervised STDP in S2 layer.  \n",
        "- Five processing scales.  \n",
        "\n",
        "#### **Layer Functions**  \n",
        "1. **S1 Layer (V1 Simple Cells)**:  \n",
        "   - **Gabor Filters**: Detect edges at 4 orientations (0°, 45°, 90°, 135°).  \n",
        "   - **Spike Encoding**: First spike latency ∝ input intensity (brighter → earlier spike).  \n",
        "   - **Competition**: Winner-take-all inhibition suppresses neighboring orientations.  \n",
        "\n",
        "2. **C1 Layer (V1 Complex Cells)**:  \n",
        "   - **Local MAX Operation**: Propagates first spike from S1 groups (position/scale invariance).  \n",
        "\n",
        "3. **S2 Layer (Feature Learning)**:  \n",
        "   - **Input**: Convergent C1 spikes (bars of different orientations).  \n",
        "   - **Receptive Field**: Restricted $4 \\times s \\times s$ patches ($s$ = scale).  \n",
        "   - **STDP Learning**: Flat-STDP (sign-based) strengthens weights to frequent patterns.  \n",
        "   - **Competition**: Winner-take-all within $s/2 \\times s/2$ neighborhood.  \n",
        "\n",
        "4. **C2 Layer (Global Invariance)**:  \n",
        "   - **Global MAX**: First spike across all positions/scales → position/scale-invariant response.  \n",
        "\n",
        "#### **Key Mechanisms**  \n",
        "- **Input Encoding**: Pixel intensity → spike latency (e.g., high intensity → 10 ms latency).  \n",
        "- **STDP in S2**:  \n",
        "  - If presynaptic spike *before* postsynaptic: LTP (weight increase).  \n",
        "  - If presynaptic spike *after* postsynaptic: LTD (weight decrease).  \n",
        "- **Feature Learning**: S2 neurons become selective to intermediate-complexity patterns (e.g., face parts, motorbike contours).  \n",
        "\n",
        "#### **Computational Workflow**  \n",
        "1. **Feedforward Pass**:  \n",
        "   - Image → latency-encoded spikes.  \n",
        "   - S1/C1: Edge detection → local invariance.  \n",
        "   - S2/C2: Feature matching → global invariance.  \n",
        "2. **Unsupervised Learning**:  \n",
        "   - STDP updates S2 weights during exposure to input streams.  \n",
        "   - Learned prototypes support categorization (e.g., faces vs. motorbikes).  \n",
        "\n",
        "#### **Performance Insights**  \n",
        "- **Speed**: First output spikes in C2 within 20–30 ms (matches biological 100 ms IT response).  \n",
        "- **Accuracy**: Learns diagnostic features (e.g., eyes for faces, wheels for motorbikes).  \n",
        "- **Robustness**: Invariance to translation, scale, and noise.  \n",
        "\n",
        "> Note: Spiking HMAX combines temporal coding (first-spike) and STDP to achieve rapid unsupervised learning of invariant object representations, mimicking the ventral stream’s efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiR9ytFlWcaR"
      },
      "source": [
        "## 31. **Shallow Spiking Neural Networks**\n",
        "### Efficient Object Recognition with Temporal Coding\n",
        "\n",
        "#### **Model Architecture**  \n",
        "Kheradpisheh et al. (2016) extend the spiking HMAX framework:  \n",
        "```  \n",
        "Input → S1 (edge detection) → C1 (local MAX) → S2 (STDP learning) → C2 (global MAX) → SVM Classifier  \n",
        "```  \n",
        "**Key Components**:  \n",
        "- **S1 Layer**:  \n",
        "  - Gabor filters at 4 orientations ($0^\\circ, 45^\\circ, 90^\\circ, 135^\\circ$).  \n",
        "  - Intensity-to-latency encoding: Spike latency $\\propto$ input intensity.  \n",
        "  - Competition: Winner-take-all inhibition across orientations.  \n",
        "- **C1 Layer**:  \n",
        "  - Propagates first spike from S1 groups (position/scale invariance).  \n",
        "- **S2 Layer**:  \n",
        "  - Learns features via **Flat-STDP**:  \n",
        "    - LTP if presynaptic spike before postsynaptic.  \n",
        "    - LTD if presynaptic spike after postsynaptic.  \n",
        "  - Receptive fields: $4 \\times s \\times s$ patches ($s$ = scale).  \n",
        "  - Competition: Winner-take-all within $s/2 \\times s/2$ neighborhood.  \n",
        "- **C2 Layer**:  \n",
        "  - Global MAX operation: First spike across positions/scales → invariant response vector.  \n",
        "\n",
        "**Scale Processing**:  \n",
        "Five image scales (e.g., scale factors 0.25, 0.5, 0.7) processed in parallel.  \n",
        "\n",
        "#### **Experimental Setup**  \n",
        "**Datasets**:  \n",
        "1. **3D-Object Dataset**:  \n",
        "   - 10 classes (bicycle, car, cellphone, etc.).  \n",
        "   - 72 conditions/viewpoint (8 angles × 3 scales × 3 tilts).  \n",
        "2. **ETH-80 Dataset**:  \n",
        "   - 8 categories (apple, car, horse, etc.).  \n",
        "   - 41 viewpoints/object.  \n",
        "\n",
        "**Training**:  \n",
        "- 5 instances/class for training (remainder for testing).  \n",
        "- Cross-validation.  \n",
        "- Linear SVM classifier on C2 features.  \n",
        "\n",
        "#### **Performance Results**  \n",
        "**3D-Object Dataset**:  \n",
        "\n",
        "| Model       | 200 Features | 500 Features | 1000 Features |  \n",
        "|-------------|-------------|-------------|--------------|  \n",
        "| Shallow SNN | 76.1%       | **96.0%**   | 62.4%*       |  \n",
        "| HMAX        | 58.2%       | 61.9%       | 62.4%        |  \n",
        "\n",
        "*Note: Performance peaks at 500 features.*\n",
        "\n",
        "**ETH-80 Dataset**:\n",
        "\n",
        "| Model       | 500 Features | 1000 Features | 5000 Features |  \n",
        "|-------------|-------------|--------------|--------------|  \n",
        "| Shallow SNN | 75.3%       | **80.7%**    | 81.1%        |  \n",
        "| HMAX        | 66.3%       | 68.9%        | 69.0%        |  \n",
        "\n",
        "**Learned S2 Features**:  \n",
        "- Diagnostic object parts (e.g., wheels for bicycles, handles for cups).  \n",
        "- Invariant to viewpoint/scale (verified by Representational Dissimilarity Matrices).  \n",
        "\n",
        "#### **Advantages over HMAX**  \n",
        "1. **Biological Plausibility**:  \n",
        "   - Spiking neurons + STDP mimic cortical learning.  \n",
        "2. **Efficiency**:  \n",
        "   - Fewer features needed for higher accuracy (e.g., 96% with 500 features vs. HMAX’s 62.4% with 9000).  \n",
        "3. **Speed**:  \n",
        "   - First-spike coding enables rapid processing (~20–30 ms).  \n",
        "  \n",
        "> Note: Shallow SNNs leverage temporal coding and STDP to achieve superior object recognition with minimal features, outperforming classical HMAX while adhering to biological constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oa-Qyj-W3e8"
      },
      "source": [
        "## **32. Deep Spiking Neural Networks**\n",
        "### Hierarchical Unsupervised Learning with STDP\n",
        "\n",
        "#### **Model Architecture**  \n",
        "Kheradpisheh et al. (2018) propose a bio-inspired DSNN:  \n",
        "```  \n",
        "Input → Temporal Coding (DoG) → Conv1 (STDP) → Pool1 (MAX) → Conv2 (STDP) → Pool2 (MAX) → ... → Global Pooling → SVM  \n",
        "```  \n",
        "**Core Innovations**:  \n",
        "- **Temporal Coding Layer**:  \n",
        "  - Applies DoG filters for center-surround processing.  \n",
        "  - Intensity-to-latency encoding: Spike latency $\\propto$ local contrast.  \n",
        "  - Rank-order coding (efficient edge detection).  \n",
        "- **Convolutional Layers**:  \n",
        "  - STDP-based unsupervised learning (Flat-STDP).  \n",
        "  - Weight sharing and winner-take-all inhibition.  \n",
        "  - Features evolve from edges (Conv1) to complex patterns (Conv3+).  \n",
        "- **Pooling Layers**:  \n",
        "  - Local MAX operation: Propagates first spike in neighborhood.  \n",
        "  - Achieves translation invariance and compression.  \n",
        "- **Global Pooling**:  \n",
        "  - Outputs position-invariant features for SVM classification.  \n",
        "\n",
        "**Learning Convergence Criterion**:  \n",
        "For layer $l$, stop training when:  \n",
        "$$ C_l = \\sum_i \\sum_j w_{r,i}(1 - w_{r,i}) / n_w < 0.01 $$  \n",
        "where $w_{r,i}$ = synaptic weight, $n_w$ = total weights.  \n",
        "\n",
        "#### **Biological Plausibility vs. DCNNs**  \n",
        "| **Aspect**          | **Deep SNN**                          | **Traditional DCNNs**              |  \n",
        "|----------------------|---------------------------------------|-----------------------------------|  \n",
        "| **Neural Coding**    | Spikes (temporal)                     | Floating-point activations        |  \n",
        "| **Learning Rule**    | Unsupervised STDP                     | Supervised backpropagation        |  \n",
        "| **Training Data**    | Unlabeled images                      | Millions of labeled examples      |  \n",
        "| **Invariance**       | MAX pooling (first-spike propagation) | Learned via backpropagation       |  \n",
        "\n",
        "#### **Experimental Results**  \n",
        "**Caltech Face/Motorbike**:  \n",
        "- **Accuracy**: 99.1% (vs. 66.3% with random features).  \n",
        "- **Noise Robustness**:  \n",
        "\n",
        "\n",
        "  | Noise ($\\alpha$) | 0%   | 30%  | 50%  |  \n",
        "  |------------------|------|------|------|  \n",
        "  | Accuracy         | 99.1%| 63.7%| 54.2%|  \n",
        "\n",
        "**ETH-80 Dataset**:  \n",
        "\n",
        "| **Model**              | **Accuracy** |  \n",
        "|------------------------|-------------|  \n",
        "| Shallow SNN            | 81.1%       |  \n",
        "| Pre-trained AlexNet    | 79.5%       |  \n",
        "| **Deep SNN (Proposed)**| **82.8%**   |  \n",
        "\n",
        "**MNIST Dataset**:\n",
        "\n",
        "| **Model**                     | **Learning**   | **Accuracy** |  \n",
        "|-------------------------------|---------------|-------------|  \n",
        "| STDP-based SNN (Diehl & Cook) | Unsupervised  | 95.0%       |  \n",
        "| Supervised DCNN (Diehl et al.)| Supervised    | 99.1%       |  \n",
        "| **Deep SNN (Proposed)**       | Unsupervised  | **98.4%**   |  \n",
        "\n",
        "**Learned Features**:  \n",
        "- Conv1: Gabor-like edge detectors.  \n",
        "- Conv3: Object-specific patterns (e.g., wheels, handles).  \n",
        "\n",
        "#### **Key Advantages**  \n",
        "1. **Unsupervised Learning**:  \n",
        "   - STDP extracts discriminative features without labels.  \n",
        "2. **Efficiency**:  \n",
        "   - Faster convergence than backpropagation-based DCNNs.  \n",
        "3. **Biological Fidelity**:  \n",
        "   - Spiking neurons + temporal coding mimic cortical dynamics.  \n",
        "4. **Robustness**:  \n",
        "   - Maintains accuracy under 20% input noise.  \n",
        "\n",
        "> Note: DSNNs bridge computational efficiency and biological realism, achieving near-supervised performance with unsupervised STDP learning in hierarchical architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loZq6YwaXgMq"
      },
      "source": [
        "## **33. Reinforcement Learning in Shallow Spiking Neural Networks**\n",
        "### Reward-Modulated STDP for Adaptive Object Recognition\n",
        "\n",
        "#### **Reward-Modulated STDP (R-STDP)**  \n",
        "Dopaminergic signaling modifies STDP dynamics:  \n",
        "- Modulates STDP window and polarity.  \n",
        "- **R-STDP Rule**:  \n",
        "  - *Reward received*: Apply standard STDP:  \n",
        "$$ \\Delta w_{ij} = \\begin{cases} a_r^+ w_{ij}(1-w_{ij}) & \\text{if } t_{C1}^f(j) - t_{S2}^f(i) \\leq 0 \\\\ a_r^- w_{ij}(1-w_{ij}) & \\text{otherwise} \\end{cases} $$  \n",
        "  - *Punishment received*: Apply *anti-STDP*:  \n",
        "$$ \\Delta w_{ij} = \\begin{cases} a_p^+ w_{ij}(1-w_{ij}) & \\text{if } t_{C1}^f(j) - t_{S2}^f(i) > 0 \\\\ a_p^- w_{ij}(1-w_{ij}) & \\text{otherwise} \\end{cases} $$  \n",
        "Parameters: $a_r^+, a_r^-, a_p^+, a_p^-$ scale updates (e.g., $a_r^+/a_t = 0.005$ for Caltech).  \n",
        "\n",
        "#### **Network Architecture**  \n",
        "Mozafari et al. (2018) integrate R-STDP into shallow SNN:  \n",
        "```  \n",
        "Input → S1 (Gabor filters) → C1 (local MAX) → S2 (R-STDP) → C2 (decision)  \n",
        "```  \n",
        "- **S2 Layer**:  \n",
        "  - Learns discriminative features via R-STDP.  \n",
        "  - Adaptive neurons switch selectivity with environmental changes.  \n",
        "- **Decision Mechanism**:  \n",
        "  - C2 category-specific neurons emit spikes.  \n",
        "  - *Earliest spike* determines object class (e.g., face vs. motorbike).  \n",
        "\n",
        "#### **Key Mechanisms**  \n",
        "- **Plastic Neurons**:  \n",
        "  - Reusable units adapt to new tasks (e.g., Task 1 → Task 2 target swap).  \n",
        "  - Video evidence: Neurons rewire selectivity dynamically.  \n",
        "- **Resource Optimization**:  \n",
        "  - Dropout ($P_{drop}=0.4–0.5$) prevents overfitting.  \n",
        "  - Involvement rate metrics: Track neuron utility per category.  \n",
        "\n",
        "#### **Experimental Results**  \n",
        "**Caltech Face/Motorbike**:  \n",
        "- **Accuracy**: 98.9% (R-STDP) vs. 96.4% (STDP).  \n",
        "- **Feature Evolution**: S2 neurons converge to diagnostic parts (e.g., eyes, wheels).  \n",
        "\n",
        "**ETH-80 and NORB Datasets**:\n",
        "\n",
        "| **Model**       | **ETH-80** | **NORB** |  \n",
        "|-----------------|------------|----------|  \n",
        "| R-STDP SNN      | 89.5%      | 88.4%    |  \n",
        "| STDP SNN        | 72.9%      | 62.7%    |  \n",
        "| Shallow CNN     | 87.1%      | 85.5%    |  \n",
        "\n",
        "- **Confusion Matrix Asymmetry**:  \n",
        "  $$ \\text{asym}(A) = \\frac{\\sum_i \\sum_j |A_{ij} - A_{ji}|}{2} $$  \n",
        "  R-STDP: asym = 0.16 (ETH-80); STDP: asym = 0.98.  \n",
        "\n",
        "**Parameter Settings**:  \n",
        "\n",
        "| **Dataset**  | $\\boldsymbol{\\omega_{c1}}$ | $\\boldsymbol{n}$ (neurons) | $\\boldsymbol{a_r^+/a_t}$ |  \n",
        "|--------------|----------------------------|----------------------------|--------------------------|  \n",
        "| Caltech      | 7                          | 20                         | 0.005                    |  \n",
        "| ETH-80       | 5                          | 80                         | 0.01                     |  \n",
        "\n",
        "#### **Advantages**  \n",
        "1. **Discriminative Features**:  \n",
        "   - R-STDP prioritizes diagnostic over frequent patterns.  \n",
        "2. **Ultra-Rapid Recognition**:  \n",
        "   - First-spike decision ≤ 100 ms (matches biological speed).  \n",
        "3. **Plasticity & Reusability**:  \n",
        "   - Neurons adapt to changing tasks without retraining.  \n",
        "4. **Hardware Efficiency**:  \n",
        "   - Event-driven processing; no multipliers needed.  \n",
        "\n",
        "#### **Limitations**  \n",
        "1. **Shallow Architecture**:  \n",
        "   - Struggles with large-scale images (e.g., ImageNet).  \n",
        "2. **Scale Sensitivity**:  \n",
        "   - Requires multi-scale processing paths.  \n",
        "3. **Color Blindness**:  \n",
        "   - Lacks chromatic processing (grayscale only).  \n",
        "4. **Overfitting Risk**:  \n",
        "   - Mitigated via dropout, but persists in small datasets.  \n",
        "\n",
        "> Note: R-STDP enables adaptive, efficient object recognition by combining dopaminergic reward signaling with temporal coding, outperforming standard STDP while maintaining biological plausibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sy28VwkYRB1"
      },
      "source": [
        "## **34. Reinforcement Learning in Deep Spiking Neural Networks**\n",
        "### Hierarchical Reward-Modulated STDP for Complex Recognition\n",
        "\n",
        "#### **R-STDP Learning Rule**  \n",
        "Dopaminergic reward signals modulate synaptic plasticity:  \n",
        "- **Reward Received**: Standard STDP applied:  \n",
        "  $$ \\Delta w_{ij} = \\begin{cases} a_r^+ w_{ij}(1-w_{ij}) & t_{pre}^f(j) - t_{post}^f(i) \\leq 0 \\\\ a_r^- w_{ij}(1-w_{ij}) & \\text{otherwise} \\end{cases} $$  \n",
        "- **Punishment Received**: Anti-STDP applied:  \n",
        "  $$ \\Delta w_{ij} = \\begin{cases} a_p^+ w_{ij}(1-w_{ij}) & t_{pre}^f(j) - t_{post}^f(i) > 0 \\\\ a_p^- w_{ij}(1-w_{ij}) & \\text{otherwise} \\end{cases} $$  \n",
        "*Parameters*: $a_r^+, a_r^-, a_p^+, a_p^-$ control update magnitudes (e.g., $a_r^+/a_t = 0.005$).  \n",
        "\n",
        "#### **Network Architecture**  \n",
        "Mozafari et al. (2019) bio-inspired DSNN:  \n",
        "```  \n",
        "Input → Temporal Coding (DoG) → Conv1 (R-STDP) → Pool1 (MAX) → Conv2 (R-STDP) → ... → Global Pooling (Decision)  \n",
        "```  \n",
        "- **Temporal Coding Layer**:  \n",
        "  - DoG filtering → contrast-based latency coding.  \n",
        "  - Rank-order encoding for efficiency.  \n",
        "- **Convolutional Layers**:  \n",
        "  - Feature complexity increases hierarchically (edges → parts).  \n",
        "  - Winner-take-all inhibition triggers R-STDP.  \n",
        "- **Pooling Layers**:  \n",
        "  - Local MAX: Propagates first spike for translation invariance.  \n",
        "- **Decision Layer**:  \n",
        "  - Global MAX over category-specific neurons.  \n",
        "  - *Earliest spike* determines class (no external classifier).  \n",
        "\n",
        "#### **Key Mechanisms**  \n",
        "- **Diagnostic Feature Extraction**:  \n",
        "  - R-STDP prioritizes discriminative over frequent patterns.  \n",
        "  - Example: In 3-class task, discards non-discriminative features (Fig. 8a-b).  \n",
        "- **Plastic Neurons**:  \n",
        "  - Units adapt selectivity to environmental changes (e.g., digit task swaps).  \n",
        "- **Hardware Efficiency**:  \n",
        "  - **SpykeTorch Framework**: PyTorch-based simulator for event-driven SNNs.  \n",
        "  - Supports TTFS encoding, STDP/R-STDP.  \n",
        "\n",
        "#### **Experimental Results**  \n",
        "**Digit Recognition (MNIST)**:  \n",
        "\n",
        "| **Model**                     | **Learning**   | **Accuracy** |  \n",
        "|-------------------------------|---------------|-------------|  \n",
        "| Multi-layer SNN (Beyeler 2013)| Unsupervised  | 95.0%       |  \n",
        "| Two-layer SNN (Diehl 2015)    | Unsupervised  | 98.4%       |  \n",
        "| **RL-based Deep SNN**         | R-STDP        | **98.4%**   |  \n",
        "\n",
        "**2-Digit Classification**:  \n",
        "- **R-STDP vs. STDP**:  \n",
        "  - With 4 feature maps: R-STDP ≈80% vs. STDP ≈65% (Fig. 9).  \n",
        "- **Feature Efficiency**:  \n",
        "  - R-STDP achieves higher accuracy with fewer features.  \n",
        "\n",
        "**Handmade 3-Class Problem**:  \n",
        "- R-STDP solves complex discrimination where STDP fails (Fig. 8b).  \n",
        "\n",
        "#### **Advantages**  \n",
        "1. **Ultra-Rapid Recognition**:  \n",
        "   - First-spike decision ≤ 100 ms (matches biological IT cortex).  \n",
        "2. **Hierarchical Plasticity**:  \n",
        "   - Multi-layer R-STDP enables reuse across tasks.  \n",
        "3. **Feature Efficiency**:  \n",
        "   - Discards non-diagnostic patterns (e.g., background clutter).  \n",
        "4. **Hardware Compatibility**:  \n",
        "   - Event-driven processing; no multipliers (energy-efficient).  \n",
        "\n",
        "#### **Implementation Tool**  \n",
        "**SpykeTorch**:  \n",
        "- PyTorch-based SNN simulator.  \n",
        "- Supports TTFS encoding, STDP/R-STDP.  \n",
        "- URL: http://cnrl.ut.ac.ir/tools-softwares/  \n",
        "\n",
        "> Note: R-STDP in deep SNNs combines hierarchical feature learning with reward-driven plasticity, enabling robust, efficient object recognition that surpasses shallow and non-reinforcement models."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
