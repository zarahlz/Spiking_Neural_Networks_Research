{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://assets.cambridge.org/97811076/35197/cover/9781107635197.jpg\" width=\"180\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <font size=\"8\"><b>Neuronal Dynamics Summary</b></font><br>\n",
        "  <font size=\"5\"><i>SNN Models, Spike Encoding, Neuron Dynamics, Spiking Behavior, Membrane Potential and More</i></font><br><br>\n",
        "  <font size=\"4\"><b>Author: Zahra Helalizadeh</b></font>\n",
        "</p>\n",
        "\n",
        "<blockquote>\n",
        "  This notebook provides a structured and high-efficiency overview of the book <b>Neuronal Dynamics</b> by Wulfram Gerstner, Werner M. Kistler, Richard Naud, and Liam Paninski.  \n",
        "  It captures the key principles of spiking neural networks (SNNs), neuron modeling, temporal dynamics, and the mathematical foundations of neuronal computation. Topics include spike generation, membrane voltage dynamics, integrate-and-fire models, and signal encoding in time.\n",
        "</blockquote>\n"
      ],
      "metadata": {
        "id": "Ixvbg7tFMQuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Table of Content**\n",
        "\n",
        "1. **Spiking behavior, membrane potential, Simulation of LIF model**\n",
        "2. **nonlinear integrate-and-fire models**\n",
        "3. **Hodgkin-Huxley model**\n",
        "4. **Spike-rate vs temporal coding**\n",
        "5. **Synaptic plasticity (STDP)**\n"
      ],
      "metadata": {
        "id": "tW9IofUsX3Al"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Spiking behavior, membrane potential, Simulation of LIF model"
      ],
      "metadata": {
        "id": "hzMqmUzYDKre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Elements of Neuronal Systems\n",
        "\n",
        "Over the last century, extensive biological research has unveiled detailed insights into brain structure and function. The fundamental processing units of the central nervous system are **neurons**, interconnected in highly complex networks. Ramón y Cajal’s early microscopic drawings illustrate only a small fraction of these neurons, which in reality number more than $10^4$ cell bodies per cubic millimeter in the cortex, connected by kilometers of neural \"wires.\"\n",
        "\n",
        "Neurons come in various shapes and sizes, but the cortex is not made solely of neurons. It also contains **glia cells**, which support energy supply and structural stability but do not participate directly in information processing. This work focuses exclusively on **spiking neurons**, the neurons that communicate via discrete electrical pulses.\n",
        "\n",
        "### 1.1.1 The Ideal Spiking Neuron\n",
        "\n",
        "A typical neuron consists of three main parts:\n",
        "\n",
        "- **Dendrites**: These receive signals from other neurons.\n",
        "- **Soma (cell body)**: This central unit integrates inputs nonlinearly and triggers output if inputs exceed a threshold.\n",
        "- **Axon**: This transmits the output signal to other neurons.\n",
        "\n",
        "The connection point between neurons is called a **synapse**. The sending neuron is the **presynaptic cell**, and the receiving neuron is the **postsynaptic cell**. A single cortical neuron may connect to over $10^4$ postsynaptic neurons. Axons can branch locally or extend over long distances to reach neurons in distant brain areas.\n",
        "\n",
        "### 1.1.2 Spike Trains\n",
        "\n",
        "Neuronal signals are short, stereotyped electrical pulses known as **action potentials** or **spikes**, typically lasting 1–2 ms with an amplitude around 100 mV. These pulses propagate unchanged along the axon.\n",
        "\n",
        "A sequence of spikes emitted by a neuron is called a **spike train**. Since each spike has an identical waveform, information is encoded in the **timing** and **number** of spikes rather than their shape.\n",
        "\n",
        "Between spikes, neurons have an **absolute refractory period** during which no new spike can be generated, followed by a **relative refractory period** where spike generation is more difficult but still possible.\n",
        "\n",
        "### 1.1.3 Synapses\n",
        "\n",
        "Synapses are predominantly **chemical** in vertebrates. When an action potential reaches the presynaptic terminal, it triggers the release of **neurotransmitters** into the synaptic cleft. These molecules bind to receptors on the postsynaptic membrane, causing ion channels to open and ions to flow, thereby changing the postsynaptic membrane potential.\n",
        "\n",
        "This postsynaptic voltage change is called the **postsynaptic potential**.\n",
        "\n",
        "Neurons can also communicate via **electrical synapses** (gap junctions), where direct electrical connections allow ion flow between cells, potentially synchronizing neuronal activity.\n",
        "\n",
        "### 1.1.4 Neurons as Components of a Large System\n",
        "\n",
        "Neurons exist within vast networks of billions of neurons and glial cells forming brain tissue, organized into regions and cortical areas with specialized functions.\n",
        "\n",
        "Neurons in sensory areas respond selectively to stimuli within a limited spatial domain called the **receptive field**. For example, neurons in the primary visual cortex fire in response to light dots appearing in specific regions of the visual field.\n",
        "\n",
        "The receptive fields of **simple cells** in the visual cortex contain subregions that either excite or inhibit neuronal activity. These neurons respond best to oriented stimuli, such as moving bars of light aligned with their preferred orientation.\n",
        "\n",
        "Beyond sensory cortices, receptive fields become less spatially defined. For instance, neurons in the inferotemporal cortex respond to objects independent of their size or position, and neurons in frontal cortex show activity during working memory tasks even without external stimuli."
      ],
      "metadata": {
        "id": "t5uBGPY2DFD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Elements of Neuronal Dynamics\n",
        "\n",
        "The membrane potential $u(t)$ of a neuron is the potential difference measured between the interior of the cell and its surroundings using an intracellular electrode. At rest, without input, the neuron maintains a constant membrane potential $u_{\\text{rest}}$. When a presynaptic spike arrives, the membrane potential changes transiently and then decays back to $u_{\\text{rest}}$. A positive change in potential corresponds to an excitatory synapse, while a negative change indicates an inhibitory synapse.\n",
        "\n",
        "At rest, the membrane is typically polarized around -65 mV. An excitatory input reduces this negative polarization (depolarization), whereas an inhibitory input increases it further (hyperpolarization).\n",
        "\n",
        "### 1.2.1 Postsynaptic Potentials\n",
        "\n",
        "Consider the membrane potential $u_i(t)$ of a postsynaptic neuron $i$. Before receiving any input spike, $u_i(t) = u_{\\text{rest}}$. Suppose a presynaptic neuron $j$ fires at time $t=0$. For $t > 0$, the postsynaptic response can be expressed as\n",
        "\n",
        "$$\n",
        "u_i(t) - u_{\\text{rest}} =: \\varepsilon_{ij}(t)\n",
        "$$\n",
        "\n",
        "where $\\varepsilon_{ij}(t)$ is the postsynaptic potential (PSP) evoked by the spike from neuron $j$ onto neuron $i$. A positive PSP ($\\varepsilon_{ij}(t) > 0$) corresponds to an excitatory postsynaptic potential (EPSP), while a negative PSP corresponds to an inhibitory postsynaptic potential (IPSP).\n",
        "\n",
        "### 1.2.2 Firing Threshold and Action Potential\n",
        "\n",
        "Suppose two presynaptic neurons $j=1, 2$ send spikes to the postsynaptic neuron $i$. Let neuron 1 fire spikes at times $t_1, t_1', \\dots$ and neuron 2 fire at times $t_2, t_2', \\dots$. Each spike induces a PSP, $\\varepsilon_{i1}$ or $\\varepsilon_{i2}$, respectively. When spikes are sparse, the membrane potential is approximately the linear sum of individual PSPs plus the resting potential:\n",
        "\n",
        "$$\n",
        "u_i(t) = \\sum_j \\sum_{f} \\varepsilon_{ij}(t - t_j^f) + u_{\\text{rest}}\n",
        "$$\n",
        "\n",
        "where $t_j^f$ denotes the firing times of neuron $j$. This linear summation describes how membrane potential responds to inputs in a simple additive manner.\n",
        "\n",
        "However, this linearity breaks down when many input spikes arrive in quick succession. If the membrane potential reaches a critical threshold $\\vartheta$, the neuron generates an **action potential** (or spike), characterized by a large voltage excursion (~100 mV). This spike propagates along the axon to communicate with downstream neurons.\n",
        "\n",
        "After the spike, the membrane potential undergoes a hyperpolarizing phase called the **spike-afterpotential**, where the potential dips below the resting value before returning to $u_{\\text{rest}}$.\n",
        "\n",
        "Typically, single EPSPs have an amplitude around 1 mV. The threshold for spike initiation is about 20-30 mV above the resting potential. Due to this, several (on the order of 20-50) presynaptic spikes must arrive within a short time window to depolarize the membrane sufficiently to trigger an action potential. Fewer spikes, such as four, are generally insufficient for firing."
      ],
      "metadata": {
        "id": "TEgBcWhJDWNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Integrate-and-Fire Models\n",
        "\n",
        "Neuronal dynamics can be approximated as a summation or integration of inputs combined with a threshold-triggered mechanism for action potentials. When the membrane potential $u_i(t)$ of neuron $i$ reaches a critical threshold voltage $\\vartheta$ from below, the neuron emits a spike at the firing time $t_i^f$. Since action potentials have a stereotypical shape, integrate-and-fire models abstract spikes as instantaneous events occurring at threshold crossings, ignoring the detailed shape of the spike itself.\n",
        "\n",
        "Integrate-and-fire models consist of two components:  \n",
        "1. An equation describing the evolution of the membrane potential $u_i(t)$.  \n",
        "2. A mechanism generating spikes when $u_i(t)$ crosses the threshold $\\vartheta$.\n",
        "\n",
        "The simplest member of this model class is the **leaky integrate-and-fire (LIF) model**, which uses a linear differential equation for membrane potential dynamics combined with a spike threshold.\n",
        "\n",
        "### 1.3.1 Integration of Inputs\n",
        "\n",
        "The membrane potential $u_i$ fluctuates around a resting value $u_{\\mathrm{rest}}$. External inputs such as injected currents $I(t)$ or synaptic inputs cause deviations from this resting potential.\n",
        "\n",
        "The neuron’s membrane acts electrically like a capacitor $C$ in parallel with a resistor $R$ and a battery at $u_{\\mathrm{rest}}$ (resting potential). The current $I(t)$ splits into:  \n",
        "$$\n",
        "I(t) = I_R + I_C,\n",
        "$$  \n",
        "where $I_R$ is the current through the resistor and $I_C$ charges the capacitor.\n",
        "\n",
        "By Ohm's law, the resistive current is  \n",
        "$$\n",
        "I_R = \\frac{u - u_{\\mathrm{rest}}}{R},\n",
        "$$  \n",
        "and the capacitive current is  \n",
        "$$\n",
        "I_C = C \\frac{du}{dt}.\n",
        "$$\n",
        "\n",
        "Combining these, the membrane potential dynamics follow  \n",
        "$$\n",
        "I(t) = \\frac{u(t) - u_{\\mathrm{rest}}}{R} + C \\frac{du}{dt}.\n",
        "$$\n",
        "\n",
        "Multiplying both sides by $R$ and defining the membrane time constant $\\tau_m = RC$, we get the standard LIF equation:  \n",
        "$$\n",
        "\\tau_m \\frac{du}{dt} = -[u(t) - u_{\\mathrm{rest}}] + R I(t).\n",
        "$$\n",
        "\n",
        "### Solution Without Input\n",
        "\n",
        "If no input current is applied, $I(t) = 0$, and with initial condition $u(t_0) = u_{\\mathrm{rest}} + \\Delta u$, the solution is  \n",
        "$$\n",
        "u(t) - u_{\\mathrm{rest}} = \\Delta u \\, e^{-\\frac{t - t_0}{\\tau_m}}, \\quad t > t_0.\n",
        "$$\n",
        "\n",
        "This represents an exponential decay of the membrane potential back to resting value with time constant $\\tau_m$, typically around 10 ms for biological neurons.\n",
        "\n",
        "### 1.3.2 Pulse Input\n",
        "\n",
        "Consider a step current input $I(t) = I_0$ applied from $t=0$ to $t = \\Delta$ with $u(0) = u_{\\mathrm{rest}}$. The membrane potential evolves as:  \n",
        "$$\n",
        "u(t) = u_{\\mathrm{rest}} + R I_0 \\left(1 - e^{-\\frac{t}{\\tau_m}}\\right), \\quad 0 < t < \\Delta.\n",
        "$$\n",
        "\n",
        "If the input persisted indefinitely, $u(t)$ would approach the steady-state  \n",
        "$$\n",
        "u(\\infty) = u_{\\mathrm{rest}} + R I_0.\n",
        "$$\n",
        "\n",
        "For short pulses with duration $\\Delta \\ll \\tau_m$, using the first-order Taylor expansion of the exponential, the voltage deflection approximates to:  \n",
        "$$\n",
        "u(\\Delta) - u_{\\mathrm{rest}} \\approx R I_0 \\frac{\\Delta}{\\tau_m}.\n",
        "$$\n",
        "\n",
        "If we shorten the pulse duration $\\Delta$ while increasing the current amplitude such that the total charge $q = I_0 \\Delta$ remains constant, the membrane potential change at the end of the pulse remains the same:  \n",
        "$$\n",
        "u(\\Delta) - u_{\\mathrm{rest}} = \\frac{q}{C}.\n",
        "$$\n",
        "\n",
        "In the limit $\\Delta \\to 0$, the input current becomes an idealized impulse or **Dirac delta function**:  \n",
        "$$\n",
        "I(t) = q \\delta(t).\n",
        "$$\n",
        "\n",
        "Here, $\\delta(t)$ satisfies  \n",
        "$$\n",
        "\\delta(x) = 0 \\text{ for } x \\neq 0, \\quad \\int_{-\\infty}^{\\infty} \\delta(x) \\, dx = 1.\n",
        "$$\n",
        "\n",
        "The delta function represents an instantaneous charge injection causing an immediate jump in membrane potential by $q/C$. After this, the potential relaxes back exponentially according to the free solution.\n",
        "\n",
        "The full equation with an impulse input is  \n",
        "$$\n",
        "\\tau_m \\frac{du}{dt} = -[u(t) - u_{\\mathrm{rest}}] + R q \\delta(t),\n",
        "$$  \n",
        "with solution  \n",
        "$$\n",
        "u(t) = \\begin{cases} u_{\\mathrm{rest}}, & t \\leq 0, \\\\[6pt]\n",
        "u_{\\mathrm{rest}} + \\frac{q}{C} e^{-\\frac{t}{\\tau_m}}, & t > 0.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "This solution is the **impulse-response function** or **Green's function** of the linear differential equation.\n",
        "\n",
        "### 1.3.3 Threshold for Spike Firing\n",
        "\n",
        "In the leaky integrate-and-fire model, the **firing time** $t_f$ is defined by the membrane potential reaching the threshold $\\vartheta$:  \n",
        "$$\n",
        "u(t_f) = \\vartheta.\n",
        "$$\n",
        "\n",
        "When the membrane potential crosses this threshold from below, the neuron emits a spike event at $t_f$. After the spike, the membrane potential is typically reset to a value $u_r$ to model the spike's aftereffects."
      ],
      "metadata": {
        "id": "zZadzNvnDrJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Limitations of the Leaky Integrate-and-Fire Model\n",
        "\n",
        "The leaky integrate-and-fire (LIF) model described previously is a simplified representation of neuronal dynamics. It integrates input currents linearly and independently of the postsynaptic neuron's state, following the differential equation:\n",
        "\n",
        "$$\n",
        "\\tau_m \\frac{du}{dt} = -[u(t) - u_{\\text{rest}}] + R I(t),\n",
        "$$\n",
        "\n",
        "where $u(t)$ is the membrane potential, $\\tau_m$ is the membrane time constant, $u_{\\text{rest}}$ is the resting potential, $R$ the resistance, and $I(t)$ the input current. When $u(t)$ reaches the threshold $\\vartheta$, the potential is reset instantly:\n",
        "\n",
        "$$\n",
        "\\text{if } u(t) = \\vartheta \\quad \\Rightarrow \\quad \\lim_{\\delta \\to 0^+} u(t+\\delta) = u_r,\n",
        "$$\n",
        "\n",
        "losing any memory of previous spikes. This section discusses key limitations of the LIF model and outlines phenomena that require extended models.\n",
        "\n",
        "### 1.4.1 Adaptation, Bursting, and Inhibitory Rebound\n",
        "\n",
        "Experimentally, neurons stimulated by a step current from $I_1=0$ to $I_2 > 0$ often show spike trains with increasing interspike intervals before reaching a steady periodic firing. Neurons exhibiting this gradual increase are called *regularly firing neurons*. Adaptation arises as a slow process accumulating over several spikes. The standard LIF model cannot capture adaptation because it resets the voltage to the same value after each spike, erasing prior spike history.\n",
        "\n",
        "To model adaptation, additional components such as a slow variable or a refractoriness filter $\\eta$ with a longer time constant than $\\tau_m$ must be introduced. This approach allows integrating the effects of multiple past spikes and is detailed in later chapters.\n",
        "\n",
        "Fast-spiking neurons, typically inhibitory, exhibit little to no adaptation and can be approximated well by the standard non-adapting LIF model.\n",
        "\n",
        "Other neuron types include bursting and stuttering neurons. Bursting neurons fire spikes in periodic bursts interrupted by silent intervals, while stuttering neurons show irregular interruptions. Neither bursting nor stuttering can be described by memoryless LIF models but can be captured by extending the model to include suitable filtering mechanisms.\n",
        "\n",
        "An additional phenomenon is *post-inhibitory rebound*: when an inhibitory current $I_1 < 0$ is switched off (to $I_2=0$), some neurons produce rebound spikes triggered by release from inhibition.\n",
        "\n",
        "### 1.4.2 Shunting Inhibition and Reversal Potential\n",
        "\n",
        "In a network, synaptic inputs affect the postsynaptic neuron differently depending on its current membrane potential $u_0$. The postsynaptic current (PSC) elicited by a presynaptic spike is proportional to\n",
        "\n",
        "$$\n",
        "\\text{PSC} \\propto [u_0 - E_{\\text{syn}}],\n",
        "$$\n",
        "\n",
        "where $E_{\\text{syn}}$ is the synaptic reversal potential. Thus, the amplitude and sign of the postsynaptic potential depend on $u_0$.\n",
        "\n",
        "For inhibitory synapses, $E_{\\text{syn}}$ lies close to but below the resting potential. If the neuron is at rest, inhibitory inputs have little effect; if depolarized above rest, inhibitory inputs produce larger hyperpolarizing responses; if hyperpolarized below $E_{\\text{syn}}$, the inhibitory input can even depolarize the neuron. This reversal creates a voltage-dependent effect called *shunting inhibition*, where inhibitory synapses increase local membrane conductance and effectively \"shunt\" excitatory inputs, especially when located near the soma or dendritic shaft.\n",
        "\n",
        "Excitatory synapses have reversal potentials significantly above rest. Their postsynaptic potentials (EPSPs) decrease in amplitude with strong depolarization, saturating at high voltages.\n",
        "\n",
        "### 1.4.3 Conductance Changes After a Spike\n",
        "\n",
        "The postsynaptic potential evoked by a presynaptic spike depends on the timing relative to previous action potentials. If the presynaptic spike arrives shortly after a postsynaptic spike (within the refractory period), ion channels involved in the previous spike are still open, reducing the amplitude of the postsynaptic potential. If the spike arrives much later, a normal-sized response occurs.\n",
        "\n",
        "This history dependence is not captured by the simple LIF model but will be addressed by more detailed conductance-based models.\n",
        "\n",
        "### 1.4.4 Spatial Structure\n",
        "\n",
        "Postsynaptic potentials vary with synapse location on the dendritic tree. Synapses farther from the soma evoke smaller potentials at the soma due to dendritic attenuation. Multiple inputs on the same dendritic branch arriving close in time interact nonlinearly, leading to saturation or amplification of responses. Such nonlinear interactions are neglected in the LIF model, which assumes linear integration.\n",
        "\n",
        "Active dendritic regions, or \"hot spots,\" can generate dendritic spikes lasting tens of milliseconds, distinct from somatic action potentials. These complex spatial and nonlinear dynamics require extended neuron models beyond the scope of the LIF framework."
      ],
      "metadata": {
        "id": "pVw3gr0cD3pC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 nonlinear integrate-and-fire models"
      ],
      "metadata": {
        "id": "GNq-8wVQE1D2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Exponential Integrate-and-Fire Model\n",
        "\n",
        "### Model Definition\n",
        "\n",
        "The exponential integrate-and-fire (EIF) model describes the membrane potential $u$ dynamics with the differential equation\n",
        "\n",
        "$$\n",
        "\\tau \\frac{du}{dt} = -(u - u_{rest}) + \\Delta_T \\exp\\left(\\frac{u - \\vartheta_{rh}}{\\Delta_T}\\right) + R I,\n",
        "$$\n",
        "\n",
        "where $\\tau$ is the membrane time constant, $u_{rest}$ the resting potential, $R$ the resistance, and $I$ the input current. The term $-(u - u_{rest})$ models the passive leak of the membrane. The nonlinear exponential term with sharpness parameter $\\Delta_T$ and rheobase threshold $\\vartheta_{rh}$ introduces a sharp upswing in membrane potential close to threshold.\n",
        "\n",
        "The spike firing time $t_f$ is defined as when $u$ reaches a numerical threshold $\\theta_{reset}$, after which $u$ is reset to $u_r$ and held in an absolute refractory period $\\Delta_{abs}$ before integration resumes. Typically, $0 < \\Delta_{abs} < 5$ ms. When $\\theta_{reset}$ is chosen sufficiently high, its exact value is negligible because the exponential upswing becomes effectively infinite.\n",
        "\n",
        "### Rheobase Threshold and Fixed Points\n",
        "\n",
        "The EIF model is a special case of a general nonlinear integrate-and-fire model with nonlinear function\n",
        "\n",
        "$$\n",
        "f(u) = - (u - u_{rest}) + \\Delta_T \\exp\\left(\\frac{u - \\vartheta_{rh}}{\\Delta_T}\\right).\n",
        "$$\n",
        "\n",
        "At zero input ($I=0$), the system has two fixed points where $f(u) = 0$: a stable fixed point near $u_{rest}$ where the exponential term is negligible for $u < \\vartheta_{rh}$, and an unstable fixed point above $\\vartheta_{rh}$ which acts as the threshold for spiking.\n",
        "\n",
        "As the input current $I$ increases slowly, these fixed points approach each other and merge at a bifurcation point at $u = \\vartheta_{rh}$, justifying the interpretation of $\\vartheta_{rh}$ as the rheobase threshold for constant current.\n",
        "\n",
        "### Relation to Leaky Integrate-and-Fire Model\n",
        "\n",
        "In the limit $\\Delta_T \\to 0$, the sharpness of the exponential term increases, and the EIF model reduces to the leaky integrate-and-fire (LIF) model with\n",
        "\n",
        "$$\n",
        "f(u) = -(u - u_{rest}), \\quad \\text{for} \\quad u < \\vartheta_{rh},\n",
        "$$\n",
        "\n",
        "and firing occurs when $u$ reaches $\\vartheta_{rh}$. Thus, the EIF model generalizes the LIF model by smoothing the threshold.\n",
        "\n",
        "### 2.1.1 Extracting the Nonlinearity from Data\n",
        "\n",
        "The EIF nonlinearity can be empirically determined by rearranging the nonlinear integrate-and-fire equation\n",
        "\n",
        "$$\n",
        "C \\frac{du}{dt} = I(t) - f(u(t)),\n",
        "$$\n",
        "\n",
        "where $C = \\frac{\\tau}{R}$ is the membrane capacitance, and defining\n",
        "\n",
        "$$\n",
        "\\tilde{f}(u) = \\frac{f(u)}{\\tau}.\n",
        "$$\n",
        "\n",
        "Experimental data is collected by injecting a time-dependent current $I(t)$ and measuring the voltage $u(t)$ and its derivative $\\frac{du}{dt}$. Plotting\n",
        "\n",
        "$$\n",
        "\\frac{I(t)}{C} - \\frac{du}{dt}\n",
        "$$\n",
        "\n",
        "against $u(t)$ for many time points yields the empirical function $\\tilde{f}(u)$ by averaging over data points with the same voltage.\n",
        "\n",
        "This function is well approximated by\n",
        "\n",
        "$$\n",
        "\\tilde{f}(u) = -\\frac{u - u_{rest}}{\\tau} + \\frac{\\Delta_T}{\\tau} \\exp\\left(\\frac{u - \\vartheta_{rh}}{\\Delta_T}\\right),\n",
        "$$\n",
        "\n",
        "which justifies the choice of the exponential nonlinearity.\n",
        "\n",
        "The slope of $\\tilde{f}(u)$ at $u_{rest}$ relates to the membrane time constant $\\tau$, and $\\vartheta_{rh}$ is where $\\tilde{f}(u)$ attains its minimum, corresponding to threshold.\n",
        "\n",
        "### Refractory Exponential Integrate-and-Fire Model\n",
        "\n",
        "Extending the analysis to time windows shortly after a spike shows increased threshold $\\vartheta_{rh}$ and altered slope at $u_{rest}$ due to refractoriness (e.g., sodium channel inactivation). These parameters gradually return to baseline within tens of milliseconds.\n",
        "\n",
        "By allowing parameters to depend on the time since the last spike, the refractory EIF model accurately predicts membrane voltage responses to novel stimuli with similar input statistics.\n",
        "\n",
        "### 2.1.2 From Hodgkin–Huxley to Exponential Integrate-and-Fire\n",
        "\n",
        "Starting from the two-dimensional reduced Hodgkin–Huxley equations:\n",
        "\n",
        "$$\n",
        "\\frac{du}{dt} = F(u, w) + I, \\quad \\frac{dw}{dt} = \\varepsilon G(u, w),\n",
        "$$\n",
        "\n",
        "with $u$ voltage and $w$ gating variables, and assuming timescale separation $\\varepsilon \\ll 1$, the slow variable $w$ can be approximated as constant at its resting value $w_{rest}$. Then,\n",
        "\n",
        "$$\n",
        "\\frac{du}{dt} = F(u, w_{rest}) + I = f(u) + I,\n",
        "$$\n",
        "\n",
        "which forms a nonlinear integrate-and-fire model. The function $f(u)$ has three zero-crossings: a stable resting fixed point $u_{rest}$, a middle unstable threshold $\\vartheta$, and a high stable point limiting spike upswing.\n",
        "\n",
        "The spike is modeled by resetting $u$ to $u_r$ when $u$ reaches $\\theta_{reset}$ between the middle and upper fixed points, replacing the detailed downswing dynamics.\n",
        "\n",
        "In the subthreshold regime ($u < \\theta_{reset}$), $f(u)$ is well approximated by the EIF nonlinearity.\n",
        "\n",
        "### Biophysical Interpretation: Exponential Sodium Channel Activation\n",
        "\n",
        "The exponential term in EIF models arises from the voltage-dependent activation of sodium channels in Hodgkin–Huxley dynamics, which exhibit an exponential increase in conductance near threshold, supporting the EIF's biophysical foundation.\n"
      ],
      "metadata": {
        "id": "J9a848bhEaa4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Quadratic Integrate-and-Fire Model\n",
        "\n",
        "The quadratic integrate-and-fire (QIF) model is a specific nonlinear integrate-and-fire neuron model defined by the differential equation\n",
        "\n",
        "$$\n",
        "\\frac{du}{dt} = a_0 (u - u_{\\text{rest}})(u - u_c) + RI,\n",
        "$$\n",
        "\n",
        "where $a_0 > 0$ and $u_c > u_{\\text{rest}}$ are parameters. This model describes the membrane potential $u$ dynamics with the following behavior: when $I=0$ and $u < u_c$, the voltage $u$ decays towards the resting potential $u_{\\text{rest}}$. If $u > u_c$, the voltage increases and triggers an action potential, with $u_c$ acting as a critical threshold for spike initiation.\n",
        "\n",
        "For numerical simulations, the integration is stopped when $u$ reaches a threshold $\\theta_{\\text{reset}}$ and reset to a value $u_r$. Mathematically, the model is often analyzed assuming $\\theta_{\\text{reset}} \\to \\infty$ and $u_r \\to -\\infty$.\n",
        "\n",
        "Although experimental data often favor an exponential nonlinearity in neuron models, near the threshold for repetitive firing the quadratic and exponential integrate-and-fire models behave similarly. The QIF model can be seen as a simpler approximation of the exponential model, especially near the rheobase current (the minimal current to trigger repetitive firing).\n",
        "\n",
        "### Relation to the Exponential Integrate-and-Fire Model\n",
        "\n",
        "When an exponential integrate-and-fire model is driven by a current that shifts its equilibrium potential $u_{\\text{eff}}$ close to the rheobase threshold $\\vartheta_{rh}$, the stable and unstable fixed points lie symmetrically around $\\vartheta_{rh}$. In this regime, the nonlinear function $f(u)$ can be approximated by a quadratic function, making the QIF and exponential models effectively identical.\n",
        "\n",
        "Increasing the input current beyond a critical value $I_c$ causes the fixed points to merge and disappear in a bifurcation, which is a hallmark of type I neuron dynamics. Hence, near this bifurcation point, any type I neuron model can be approximated by the QIF model, often called the \"canonical\" type I integrate-and-fire neuron.\n",
        "\n",
        "### 5.3.1 Canonical Type I Model\n",
        "\n",
        "The QIF model is mathematically equivalent to the canonical type I phase neuron model given by\n",
        "\n",
        "$$\n",
        "\\frac{d\\phi}{dt} = [1 - \\cos \\phi] + \\Delta I [1 + \\cos \\phi],\n",
        "$$\n",
        "\n",
        "where $\\phi$ is the phase variable and $\\Delta I$ is the difference between the input current $I$ and the minimal current $I_{\\theta}$ necessary for repetitive firing of the QIF neuron.\n",
        "\n",
        "By a suitable voltage shift and choosing $I = I_{\\theta} + \\Delta I$, the QIF equation can be written as\n",
        "\n",
        "$$\n",
        "\\frac{du}{dt} = u^2 + \\Delta I.\n",
        "$$\n",
        "\n",
        "For $\\Delta I > 0$, $u$ increases until reaching the firing threshold $\\vartheta$ and then resets to $u_r \\approx -\\infty$. The firing times are essentially insensitive to the exact values of $\\vartheta$ and $u_r$ because $u(t)$ diverges in finite time with hyperbolic growth.\n",
        "\n",
        "The equivalence between the QIF model and the canonical phase model is established by the transformation\n",
        "\n",
        "$$\n",
        "u(t) = \\tan \\frac{\\phi(t)}{2}.\n",
        "$$\n",
        "\n",
        "Differentiating and substituting $\\frac{d\\phi}{dt}$ yields\n",
        "\n",
        "$$\n",
        "\\frac{du}{dt} = \\frac{1}{2} \\sec^2 \\frac{\\phi}{2} \\frac{d\\phi}{dt} = \\tan^2 \\frac{\\phi}{2} + \\Delta I = u^2 + \\Delta I,\n",
        "$$\n",
        "\n",
        "which matches the QIF dynamics exactly.\n",
        "\n",
        "Thus, the QIF neuron in the limit $\\vartheta \\to \\infty$ and $u_r \\to -\\infty$ is mathematically equivalent to the generic canonical type I neuron phase model.\n"
      ],
      "metadata": {
        "id": "Le0fx1cVEcat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Hodgkin-Huxley model"
      ],
      "metadata": {
        "id": "x5UebYvPD61Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Equilibrium Potential  \n",
        "\n",
        "Neurons, like all cells, are enclosed by a membrane that separates the cell interior from the extracellular space. Ion concentrations differ on each side of this membrane, creating an electrical potential crucial for neuronal function. This section explains the concept of equilibrium potential and its biophysical basis.\n",
        "\n",
        "### 3.1.1 Nernst Potential  \n",
        "\n",
        "From thermodynamics, the probability of a molecule occupying a state with energy $E$ follows the Boltzmann distribution:  \n",
        "$$p(E) \\propto \\exp\\left(-\\frac{E}{kT}\\right),$$  \n",
        "where $k$ is Boltzmann's constant and $T$ is absolute temperature.\n",
        "\n",
        "For positive ions of charge $q$ in an electrical potential field $u(x)$, the energy at position $x$ is  \n",
        "$$E(x) = q\\, u(x).$$  \n",
        "The ion density $n(x)$, proportional to the probability of finding ions near $x$, is thus  \n",
        "$$n(x) \\propto \\exp\\left(-\\frac{q\\, u(x)}{kT}\\right).$$  \n",
        "\n",
        "Considering two points $x_1$ and $x_2$, the ratio of ion densities is  \n",
        "$$\\frac{n(x_1)}{n(x_2)} = \\exp\\left(-\\frac{q(u(x_1) - u(x_2))}{kT}\\right).$$  \n",
        "\n",
        "Defining the potential difference $\\Delta u = u(x_1) - u(x_2)$, this can be rearranged to express the equilibrium voltage generated by an ion concentration difference:  \n",
        "$$\\Delta u = \\frac{kT}{q} \\ln\\frac{n_2}{n_1},$$  \n",
        "where $n_1$ and $n_2$ are the ion concentrations at the two points. This voltage is called the **Nernst potential**.\n",
        "\n",
        "### 3.1.2 Reversal Potential  \n",
        "\n",
        "The cell membrane is a lipid bilayer acting as an electrical insulator, embedded with proteins forming ion pumps and channels. Ion pumps actively transport ions, maintaining concentration gradients; ion channels allow passive ion flow.\n",
        "\n",
        "For instance, in mammalian neurons, sodium concentration inside the cell is about 10 mM, much lower than outside (~145 mM), whereas potassium concentration is about 140 mM inside, higher than outside (~5 mM).\n",
        "\n",
        "At equilibrium, these concentration differences produce specific Nernst potentials. For sodium ions ($Na^+$), the Nernst potential $E_{Na}$ is approximately +67 mV, meaning the inside of the cell is positively charged relative to the outside.\n",
        "\n",
        "If the membrane voltage difference $\\Delta u$ is less than $E_{Na}$, sodium ions flow inward to reduce the concentration gradient. If $\\Delta u > E_{Na}$, ions flow outward. This reversal of ion flow direction at $E_{Na}$ defines it as the **reversal potential**.\n",
        "\n",
        "#### Example: Potassium Reversal Potential  \n",
        "\n",
        "Potassium ions ($K^+$), with charge $q = 1.6 \\times 10^{-19}$ C, have higher intracellular concentration (~140 mM) than extracellular (~5 mM). Using the Nernst equation:  \n",
        "$$E_K = \\frac{kT}{q} \\ln \\frac{[K^+]_{out}}{[K^+]_{in}},$$  \n",
        "and substituting Boltzmann constant $k = 1.4 \\times 10^{-23} \\text{ J/K}$ at room temperature yields  \n",
        "$$E_K \\approx -83 \\text{ mV},$$  \n",
        "a negative reversal potential.\n",
        "\n",
        "#### Example: Resting Potential  \n",
        "\n",
        "Real neurons contain multiple ion species simultaneously. Experimentally, the resting membrane potential is about $u_{rest} \\approx -65$ mV, which lies between $E_K$ and $E_{Na}$. At rest, potassium ions tend to flow out, sodium ions flow in, and active ion pumps maintain this dynamic equilibrium, balancing ion flows to sustain the resting potential.\n",
        "\n",
        "The resting potential results from the interplay between passive ion channel permeability and active ion transport by pumps, ensuring stable intracellular and extracellular ion concentrations."
      ],
      "metadata": {
        "id": "Y43GQhiAEHWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Hodgkin–Huxley Model\n",
        "\n",
        "### 3.2.1 Definition of the Model\n",
        "\n",
        "Hodgkin and Huxley (1952) studied the giant squid axon and identified three main types of ion currents: sodium (Na⁺), potassium (K⁺), and a leak current mainly from chloride ions (Cl⁻). Specific voltage-dependent ion channels regulate the flow of these ions across the cell membrane, while the leak current accounts for other nonspecific ion channels.\n",
        "\n",
        "The cell membrane acts as a capacitor separating intracellular and extracellular fluids. When an input current $I(t)$ is injected, it either charges the membrane capacitor or passes through ion channels, modeled as resistors with voltage-dependent resistance values for sodium ($R_{Na}$), potassium ($R_K$), and leak channels ($R$).\n",
        "\n",
        "Because of ion concentration differences across the membrane, each ion type has its own Nernst (reversal) potential, represented by batteries with voltages $E_{Na}$, $E_K$, and $E_L$.\n",
        "\n",
        "The conservation of charge on the membrane gives the fundamental equation:\n",
        "\n",
        "$$\n",
        "I(t) = I_C(t) + \\sum_k I_k(t)\n",
        "$$\n",
        "\n",
        "where $I_C$ is the capacitive current and $I_k$ are the ionic currents for each channel $k$. Since the membrane capacitance $C = \\frac{q}{u}$ relates charge $q$ and voltage $u$, the capacitive current is:\n",
        "\n",
        "$$\n",
        "I_C = C \\frac{du}{dt}\n",
        "$$\n",
        "\n",
        "which leads to the membrane equation:\n",
        "\n",
        "$$\n",
        "C \\frac{du}{dt} = -\\sum_k I_k(t) + I(t)\n",
        "$$\n",
        "\n",
        "Here, $u$ is the membrane voltage, and $\\sum_k I_k$ is the total ionic current.\n",
        "\n",
        "The leak current is voltage-independent with conductance $g_L = \\frac{1}{R}$, and by Ohm's law:\n",
        "\n",
        "$$\n",
        "I_L = g_L (u - E_L)\n",
        "$$\n",
        "\n",
        "Sodium and potassium channels have voltage- and time-dependent conductances $g_{Na} m^3 h$ and $g_K n^4$, respectively. The gating variables $m$, $h$, and $n$ represent the probability of channel subunits being open or closed: $m$ and $h$ regulate sodium activation and inactivation, and $n$ controls potassium activation.\n",
        "\n",
        "The total ionic current is:\n",
        "\n",
        "$$\n",
        "\\sum_k I_k = g_{Na} m^3 h (u - E_{Na}) + g_K n^4 (u - E_K) + g_L (u - E_L)\n",
        "$$\n",
        "\n",
        "The gating variables evolve according to:\n",
        "\n",
        "$$\n",
        "\\frac{dx}{dt} = -\\frac{x - x_0(u)}{\\tau_x(u)}\n",
        "$$\n",
        "\n",
        "where $x \\in \\{m, n, h\\}$, $x_0(u)$ is the voltage-dependent steady-state value, and $\\tau_x(u)$ is the voltage-dependent time constant.\n",
        "\n",
        "### Example: Voltage Step Response\n",
        "\n",
        "If the membrane voltage is held at $u_0$ for $t < t_0$ and then stepped to $u_1$ at $t_0$, the gating variables evolve as:\n",
        "\n",
        "$$\n",
        "m(t) = m_0(u_1) + [m_0(u_0) - m_0(u_1)] e^{-\\frac{t - t_0}{\\tau_m(u_1)}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "h(t) = h_0(u_1) + [h_0(u_0) - h_0(u_1)] e^{-\\frac{t - t_0}{\\tau_h(u_1)}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "n(t) = n_0(u_1) + [n_0(u_0) - n_0(u_1)] e^{-\\frac{t - t_0}{\\tau_n(u_1)}}\n",
        "$$\n",
        "\n",
        "Using these, the time-dependent sodium and potassium currents following the voltage step are:\n",
        "\n",
        "$$\n",
        "I_{Na}(t) = g_{Na} m(t)^3 h(t) (u_1 - E_{Na})\n",
        "$$\n",
        "\n",
        "$$\n",
        "I_K(t) = g_K n(t)^4 (u_1 - E_K)\n",
        "$$\n",
        "\n",
        "These relations allowed Hodgkin and Huxley to fit experimental data to extract the functions $x_0(u)$ and $\\tau_x(u)$ as well as the powers (exponents) on the gating variables.\n",
        "\n",
        "### Activation and Inactivation Variables\n",
        "\n",
        "- The activation variable $m$ is near zero at resting potential ($u = -65$ mV), meaning sodium channels are closed at rest. When $u$ increases, $m$ rises, \"activating\" the channel and allowing sodium current to flow.\n",
        "- The inactivation variable $h$ starts near one at rest, and decreases (channel \"inactivates\") when $u$ increases above approximately $-40$ mV. Returning $u$ to rest allows $h$ to increase again (\"de-inactivation\").\n",
        "\n",
        "This gating distinction is important for channel dynamics: a channel may be deactivated ($m \\approx 0$, $h \\approx 1$) or inactivated ($h \\approx 0$).\n",
        "\n",
        "### 3.2.2 Stochastic Channel Opening\n",
        "\n",
        "Ion channels open and close stochastically due to their finite number in a membrane patch. Experimentally, the recorded current shows fluctuating, step-like behavior varying across trials. The Hodgkin–Huxley model’s gating variables represent the average behavior over many channels or repeated trials. Stochastic effects can be included by adding noise to the gating dynamics.\n",
        "\n",
        "### Alternative Formulation: Transition Rates\n",
        "\n",
        "The gating variables can also be described by voltage-dependent transition rates $\\alpha_x(u)$ and $\\beta_x(u)$ as:\n",
        "\n",
        "$$\n",
        "\\frac{dm}{dt} = \\alpha_m(u) (1 - m) - \\beta_m(u) m\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{dn}{dt} = \\alpha_n(u) (1 - n) - \\beta_n(u) n\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{dh}{dt} = \\alpha_h(u) (1 - h) - \\beta_h(u) h\n",
        "$$\n",
        "\n",
        "These are equivalent to the previous formulation with the relations:\n",
        "\n",
        "$$\n",
        "x_0(u) = \\frac{\\alpha_x(u)}{\\alpha_x(u) + \\beta_x(u)}, \\quad \\tau_x(u) = \\frac{1}{\\alpha_x(u) + \\beta_x(u)}\n",
        "$$\n",
        "\n",
        "The empirical functions $\\alpha_x(u)$ and $\\beta_x(u)$ define the voltage-dependent gating kinetics measured experimentally."
      ],
      "metadata": {
        "id": "1bQi8nQrEMq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Spike-rate vs temporal coding"
      ],
      "metadata": {
        "id": "VmHNYfoqF_nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Spike-Train Variability\n",
        "\n",
        "Neuronal models such as the Hodgkin–Huxley or integrate-and-fire models, when driven by a sufficiently strong constant current, generate highly regular spike sequences. Models with adaptation currents may exhibit a brief transient phase, but eventually produce constant interspike intervals. However, spike trains recorded from neurons in vivo typically show much more irregular behavior. The origin of this irregularity—whether due to thermal noise, microscopic chaos, or a complex neural coding scheme—is still an open question. This section reviews evidence of neuronal variability and discusses possible noise sources.\n",
        "\n",
        "### 4.1.1 Are Neurons Noisy?\n",
        "\n",
        "Many in vivo experiments demonstrate noisy behavior in cortical neurons. For instance, in the visual cortex, when a slowly moving bar enters a neuron's receptive field, the firing rate increases, yet the spike train varies significantly across repeated trials. Similarly, neurons in sensory cortices of rodents respond to whisker movements with trial-to-trial variability. Even in the absence of external stimuli, spontaneous spikes occur, and the membrane voltage fluctuates with variable interspike intervals.\n",
        "\n",
        "While these observations suggest noise, the recorded neuron receives inputs from many sources, making the effective input current uncertain despite a constant external stimulus. In vitro experiments provide better control: injecting a known time-dependent current into isolated neurons still results in trial-to-trial variability of spike timing (Fig. 7.2). This variability is prominent only when the stimulating current is nearly constant (Fig. 7.3).\n",
        "\n",
        "Interestingly, neurons respond more reliably to rapidly fluctuating inputs than to constant or slowly varying stimuli. For example, retinal ganglion cells produce nearly identical spike trains to repeated presentations of spatially uniform random flicker (Berry et al., 1997). Similar deterministic-like responses are observed in motion-sensitive neurons in flies and monkeys (de Ruyter van Steveninck et al., 1997; Bair and Koch, 1996). Therefore, neuronal reliability depends partly on the nature of the stimulus.\n",
        "\n",
        "Noise sources can be classified into intrinsic noise—arising from the neuron's own dynamics even in isolation—and extrinsic noise from network and synaptic transmission effects present in vivo.\n",
        "\n",
        "### 4.1.2 Intrinsic Noise Sources\n",
        "\n",
        "One universal intrinsic noise source is thermal noise, also known as Johnson noise. Because of the discrete nature of electric charges, the voltage $u$ across a resistor $R$ fluctuates at finite temperature $T$. The variance of these voltage fluctuations at rest is proportional to\n",
        "\n",
        "$$\n",
        "\\langle \\Delta u^2 \\rangle \\propto R k T B\n",
        "$$\n",
        "\n",
        "where $k$ is the Boltzmann constant and $B$ is the system's bandwidth. Neuronal membranes, modeled as electrical circuits with resistors, exhibit corresponding membrane potential fluctuations. However, Johnson noise is minor compared to other neuronal noise sources.\n",
        "\n",
        "A more significant source arises from the finite number of ion channels on a neuronal membrane patch. Ion channels typically switch between two states: open or closed. For ion type $i$, at a given membrane potential $u$, the probability that a channel is open is $P_i(u)$. The number of open channels fluctuates around $N_i P_i(u)$, where $N_i$ is the total number of ion channels of type $i$. The classical Hodgkin–Huxley model assumes large $N_i$ so fluctuations are negligible. In reality, finite $N_i$ cause fluctuations in conductance and membrane potential, especially near the spike threshold. Channel noise can therefore critically affect spike generation.\n",
        "\n",
        "Models incorporating finite channel numbers reproduce observed spike variability under intracellular stimulation, showing less spike jitter with rapidly fluctuating inputs but more variability for constant inputs.\n",
        "\n",
        "### 4.1.3 Noise from the Network\n",
        "\n",
        "Beyond intrinsic sources, extrinsic noise arises from synaptic transmission and network effects. Synaptic failures impose substantial transmission limitations; only about 10–30% of presynaptic spikes evoke postsynaptic responses.\n",
        "\n",
        "Network dynamics also contribute: even deterministic networks with fixed random connectivity can generate highly irregular spike trains. For example, networks of excitatory and inhibitory leaky integrate-and-fire neurons produce irregular spike timing without any stochastic noise input. Such networks cause each neuron to receive irregular spike trains that can be modeled as stochastic spike arrivals.\n",
        "\n",
        "The greater variability observed in vivo compared to in vitro intracellular stimulation can thus be partly attributed to network-induced noise."
      ],
      "metadata": {
        "id": "8nm_ZPZoEirT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 The Problem of Neural Coding\n",
        "\n",
        "Neural coding investigates how neurons transmit information through their spike trains. While previous sections covered measures like interval distribution, autocorrelation, noise spectrum, and firing rate, the key question remains: do neurons actually use these quantities as codes to convey information?\n",
        "\n",
        "### 4.2.1 Limits of Rate Codes\n",
        "\n",
        "Rate codes interpret neural activity by the number of spikes in a given time window. However, several limitations challenge this concept.\n",
        "\n",
        "#### Limitations of the Spike Count Code\n",
        "\n",
        "Although experimenters can classify neural firing by counting spikes over intervals (e.g., 500 ms), this may not reflect the brain's actual coding strategy. Behavioral data indicate fast reaction times that are incompatible with long averaging windows. For instance, flies react to stimuli in 30–40 ms, and humans recognize visual scenes within a few hundred milliseconds—times too short for spike count averaging.\n",
        "\n",
        "In rate coding, spikes serve as analog signals encoding a firing rate $\\nu$. The ideal rate code would be a regular spike train with interspike intervals of $1/\\nu$, enabling rate estimation after just two spikes. Irregular spike timing observed in cortex is thus considered noise, requiring averaging over many spikes to extract reliable rates.\n",
        "\n",
        "Temporal averaging is only effective when stimuli are constant or slowly varying. Real-world inputs change rapidly, e.g., during saccades where retinal images shift every few hundred milliseconds. Postsynaptic neurons cannot average over many spikes in such dynamic settings, prompting examination of whether peristimulus time histograms (PSTHs) could be neural codes.\n",
        "\n",
        "#### Limitations of the PSTH\n",
        "\n",
        "The PSTH relies on averaging spike trains over many trials to estimate instantaneous firing rates, which neurons cannot perform internally. For example, a frog catching a fly cannot wait for multiple repeated trajectories.\n",
        "\n",
        "PSTH-based codes may be plausible when large populations of similar neurons respond to the same stimulus. Instead of averaging across repeated trials of one neuron, the brain might average across neurons in a population. This assumes homogeneity among neurons.\n",
        "\n",
        "#### Variability and Population Rate Codes\n",
        "\n",
        "Defining firing rate as an average over a neuronal population requires near-identical neurons, an unrealistic assumption. Real populations exhibit heterogeneity in parameters and connectivity. A more general approach uses a weighted population average to estimate stimulus location.\n",
        "\n",
        "If neurons $j$ represent stimulus vectors $x_j$, the stimulus estimate $x(t)$ over interval $[t, t+\\Delta t]$ is given by:\n",
        "\n",
        "$$\n",
        "x(t) = \\frac{\\int_t^{t+\\Delta t} \\sum_j x_j \\delta(t - t_j) dt}{\\int_t^{t+\\Delta t} \\sum_j \\delta(t - t_j) dt}\n",
        "$$\n",
        "\n",
        "where $t_j$ are spike times. This weighted average reflects population activity but requires normalization by division, which may be biologically challenging for neurons to perform.\n",
        "\n",
        "### 4.2.2 Candidate Temporal Codes\n",
        "\n",
        "Besides rate codes, temporal coding schemes exploit precise spike timing.\n",
        "\n",
        "#### Time-to-First-Spike (Latency) Code\n",
        "\n",
        "Consider a neuron receiving an abrupt input at time $t_0$. The timing of its first spike after $t_0$ can encode stimulus strength—earlier spikes indicate stronger stimuli, later spikes weaker ones.\n",
        "\n",
        "In this scheme, each neuron emits only one spike per stimulus, with subsequent spikes ignored, possibly enforced by inhibitory mechanisms that silence the neuron until the next stimulus onset.\n",
        "\n",
        "Experimental evidence supports latency coding:  \n",
        "- Touch receptors in fingertips encode touch strength and direction by first spike timing.  \n",
        "- Retinal neurons encode images through relative first spike latencies.  \n",
        "- Studies show most stimulus-related information is contained within the first 20–50 ms after stimulus onset.\n",
        "\n",
        "#### Phase Coding\n",
        "\n",
        "If the reference signal is a periodic oscillation rather than a single event, spike timing relative to the phase of this oscillation can encode information. This phase code is evident in hippocampal neurons, where spike phase during theta oscillations conveys spatial information beyond firing rate.\n",
        "\n",
        "#### Coding by Correlations and Synchrony\n",
        "\n",
        "Spikes of different neurons may synchronize to signal relevant events. Synchrony could serve as a “label” linking neurons representing the same object or feature. More generally, precise spatio-temporal spike patterns among groups of neurons can represent distinct stimuli.\n",
        "\n",
        "Examples include:  \n",
        "- Synchrony signaling “belonging together” of neurons coding the same object.  \n",
        "- Specific temporal patterns across neurons encoding stimulus conditions with relative spike delays.  \n",
        "- Correlations in auditory and visual neuron activity that carry stimulus information beyond firing rates.\n",
        "\n",
        "#### Stimulus Reconstruction and Reverse Correlation\n",
        "\n",
        "When a neuron responds to a time-dependent stimulus $s(t)$, the stimulus segments preceding spikes can be averaged (reverse correlation) to estimate the stimulus features triggering spikes. This differs from PSTH, which averages neural responses to repeated stimuli; reverse correlation averages stimuli conditioned on spikes.\n",
        "\n",
        "This approach reveals characteristic stimulus features that elicit spiking, aiding interpretation of neural coding strategies."
      ],
      "metadata": {
        "id": "sZDN_M6PEpcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 From Microscopic to Macroscopic Neural Activity\n",
        "\n",
        "### 4.3.1 Transition from Single Neurons to Population Activity\n",
        "\n",
        "This section presents the foundational idea of connecting the behavior of individual spiking neurons to the collective activity of a homogeneous population of neurons, focusing on **stationary activity**. The goal is to understand how the average firing of single neurons relates to the overall firing rate of the population without yet addressing temporal dynamics or stability, which require more advanced mathematical tools covered later.\n",
        "\n",
        "In a homogeneous population where every neuron receives similar input from the others (including from itself in fully connected networks), the input current to each neuron reflects the **population activity** $A(t)$. This is formalized for single and multiple populations in prior equations (12.4 and 12.11). Under stationary assumptions, the firing rate of each neuron is constant and equal to the population activity $A(t)$:\n",
        "\n",
        "$$\n",
        "A(t) = \\nu,\n",
        "$$\n",
        "\n",
        "where $\\nu$ is the mean firing rate of a single neuron.\n",
        "\n",
        "This equality holds regardless of the specific neuron model used (Hodgkin–Huxley, adaptive exponential integrate-and-fire, spike response with noise, etc.).\n",
        "\n",
        "### 4.3.2 Stationary Activity and Asynchronous Firing\n",
        "\n",
        "**Asynchronous firing** is defined as a macroscopic state with constant population activity:\n",
        "\n",
        "$$\n",
        "A(t) = A_0.\n",
        "$$\n",
        "\n",
        "This means that despite individual neurons spiking irregularly, the averaged population firing rate remains constant over time.\n",
        "\n",
        "Population activity $A(t)$ is empirically measured by counting spikes over a finite time window $\\Delta t$ or by smoothing spike trains with a filter $\\gamma(s)$, typically an exponential. As the population size $N$ grows, the filtered activity converges to a constant value $A_0$, indicating stationary asynchronous firing (Fig. 12.14).\n",
        "\n",
        "### 4.3.3 Population Activity Equals Single-Neuron Firing Rate\n",
        "\n",
        "The key result is that the stationary population activity $A_0$ equals the mean firing rate $\\nu_i$ of a single neuron $i$ in the population:\n",
        "\n",
        "$$\n",
        "A_0 = \\nu_i.\n",
        "$$\n",
        "\n",
        "This equality follows from simple counting: If $N$ neurons generate a total spike count $S$ over time $T$, then the population activity is\n",
        "\n",
        "$$\n",
        "A_0 = \\frac{S}{N T},\n",
        "$$\n",
        "\n",
        "and each neuron has firing rate\n",
        "\n",
        "$$\n",
        "\\nu_i = \\frac{S}{N T} = A_0.\n",
        "$$\n",
        "\n",
        "Because neurons are homogeneous and statistically identical, temporal and spatial averaging coincide. For finite $N$, the empirical activity fluctuates, but the expected value satisfies\n",
        "\n",
        "$$\n",
        "\\langle A_0 \\rangle = \\nu_i.\n",
        "$$\n",
        "\n",
        "The mean firing rate of a neuron under stationary input $I_0$ and noise amplitude $\\sigma$ is given by the **gain function**\n",
        "\n",
        "$$\n",
        "\\nu_i = g_{\\sigma}(I_0),\n",
        "$$\n",
        "\n",
        "where the shape of $g_\\sigma$ depends on noise level $\\sigma$. Thus, the expected population activity in a stationary state can be predicted solely from single-neuron properties:\n",
        "\n",
        "$$\n",
        "\\langle A_0 \\rangle = g_\\sigma(I_0).\n",
        "$$\n",
        "\n",
        "### 4.3.4. Observing and Comparing Population Activity\n",
        "\n",
        "The theoretical population activity $A_0$ corresponds to the expectation of the observed activity $A(t)$, defined as a sum over spike delta-functions. In simulations with finite $N$, $A(t)$ fluctuates around $A_0$.\n",
        "\n",
        "To compare theory and observation, the observed activity should be smoothed with a test function $\\gamma(s)$ normalized as\n",
        "\n",
        "$$\n",
        "\\int_0^{s_{\\max}} \\gamma(s) ds = 1,\n",
        "$$\n",
        "\n",
        "yielding the filtered activity\n",
        "\n",
        "$$\n",
        "A(t) = \\int_0^{s_{\\max}} \\gamma(s) A(t - s) ds.\n",
        "$$\n",
        "\n",
        "Asynchronous firing is characterized by decreasing fluctuations $\\langle |A(t) - A_0|^2 \\rangle$ as $N$ increases.\n",
        "\n",
        "### 4.3.5 Activity of a Fully Connected Network\n",
        "\n",
        "In a fully connected, homogeneous population, the firing rate of a neuron depends on its total input current $I$, which is the sum of external input and recurrent input from other neurons:\n",
        "\n",
        "$$\n",
        "I(t) = w_0 N \\int_0^\\infty \\alpha(s) A(t - s) ds + I_{\\text{ext}}(t).\n",
        "$$\n",
        "\n",
        "Here, $w_0$ is the synaptic weight, $N$ the number of neurons, and $\\alpha(s)$ the postsynaptic current kernel normalized as\n",
        "\n",
        "$$\n",
        "\\int_0^\\infty \\alpha(s) ds = 1.\n",
        "$$\n",
        "\n",
        "Under stationarity, the convolution reduces to\n",
        "\n",
        "$$\n",
        "\\int_0^\\infty \\alpha(s) A(t - s) ds = A_0,\n",
        "$$\n",
        "\n",
        "and with constant external input $I_{\\text{ext}}(t) = I_{0\\text{ext}}$, the total input current is constant:\n",
        "\n",
        "$$\n",
        "I_0 = w_0 N A_0 + I_{0\\text{ext}}.\n",
        "$$\n",
        "\n",
        "Using the noise-free gain function $g_0$, the population activity satisfies the implicit self-consistency equation:\n",
        "\n",
        "$$\n",
        "A_0 = g_0(J_0 A_0 + I_{0\\text{ext}}),\n",
        "$$\n",
        "\n",
        "where $J_0 = w_0 N$ is the effective coupling strength.\n",
        "\n",
        "This equation determines fixed points of population activity and is independent of neuron model details.\n",
        "\n",
        "### 4.3.6 Graphical Interpretation and Stability\n",
        "\n",
        "The fixed points $A_0$ are found graphically as intersections between:\n",
        "\n",
        "- The gain function $A_0 = g_0(I_0)$ (firing rate vs. input current),\n",
        "- The linear function $A_0 = \\frac{I_0 - I_{0\\text{ext}}}{J_0}$ derived from input decomposition.\n",
        "\n",
        "Multiple solutions can exist, corresponding to potentially multiple stable or unstable firing states, analogous to mean-field theories in physics such as the Curie–Weiss model of ferromagnetism.\n",
        "\n",
        "Stability and dynamic behavior require population dynamics analysis (Chapters 13 and 14).\n",
        "\n",
        "### Example: Leaky Integrate-and-Fire Model with Diffusive Noise\n",
        "\n",
        "Consider a large, fully connected network of leaky integrate-and-fire neurons with homogeneous weights $w_{ij} = \\frac{J_0}{N}$ and normalized synaptic kernel:\n",
        "\n",
        "$$\n",
        "\\int_0^\\infty \\alpha(s) ds = 1.\n",
        "$$\n",
        "\n",
        "The input current for a typical neuron is\n",
        "\n",
        "$$\n",
        "I_0 = I_{0\\text{ext}} + J_0 A_0,\n",
        "$$\n",
        "\n",
        "and neurons receive independent Gaussian noise of variance $\\sigma^2$ representing stochastic input.\n",
        "\n",
        "The single-neuron gain function with diffusive noise, $g_\\sigma(I_0)$, can be computed analytically (Siegert, 1951; Eq. 8.54), linking stationary population activity to neuron-level parameters.\n"
      ],
      "metadata": {
        "id": "c8GwBDLsE9C_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.7 Activity of a Randomly Connected Network\n",
        "\n",
        "In previous sections, the stationary state of a large neuron population under a fixed noise level was studied, where noise was modeled as diffusive and the input current to each neuron was constant and uniform across the population ($I_i = I_0$). However, in a **randomly connected network**, the synaptic input current fluctuates around a mean value $I_0$, even in stationary asynchronous activity. This section develops a mathematical framework to incorporate the additional noise arising from network interactions.\n",
        "\n",
        "#### Model Assumptions and Setup\n",
        "\n",
        "Consider a network of $N$ neurons, each receiving input from $C_{\\text{pre}}$ presynaptic partners with equal synaptic weight $w_{ij} = w$. Assuming the network is in a stationary asynchronous state where each neuron fires stochastically and independently at a rate $\\nu$, we analyze the firing rate self-consistently.\n",
        "\n",
        "The mean input current to neuron $i$ from its presynaptic partners is:\n",
        "$$\n",
        "I_0 = C_{\\text{pre}} \\, w \\, \\nu \\, q + I_0^{\\text{ext}},\n",
        "$$\n",
        "where $q = \\int_0^\\infty \\alpha(s) ds$ is the total charge delivered by a single spike (postsynaptic current integral).\n",
        "\n",
        "The variance of this input current, due to fluctuations, is:\n",
        "$$\n",
        "\\sigma_I^2 = C_{\\text{pre}} w^2 q^2 \\nu,\n",
        "$$\n",
        "where $q^2 = \\int_0^\\infty \\alpha^2(s) ds$.\n",
        "\n",
        "Using a known single-neuron gain function $g_{\\sigma}(I_0)$, which gives the firing rate $\\nu$ for given input mean and noise level $\\sigma_I$, the self-consistency equation becomes:\n",
        "$$\n",
        "\\nu = g_{\\sigma}(I_0).\n",
        "$$\n",
        "This implicit equation can be solved numerically for $\\nu$, yielding the mean population activity $A_0 = \\nu$.\n",
        "\n",
        "#### Generality of the Framework\n",
        "\n",
        "This self-consistency approach applies to any neuron model (Hodgkin–Huxley type or integrate-and-fire), provided the gain function $g_{\\sigma}(I_0)$ is known or can be numerically estimated. It extends naturally to multiple interacting populations by including their contributions to the input current mean and variance.\n",
        "\n",
        "#### Extension to Random Connectivity\n",
        "\n",
        "For networks with random connectivity of fixed connection probability $p$ and synaptic scaling $w_{ij} = J_0 / N$, similar arguments hold (Amari, 1972; Sompolinsky et al., 1988; van Vreeswijk and Sompolinsky, 1996).\n",
        "\n",
        "#### Brunel Network: Excitatory and Inhibitory Populations\n",
        "\n",
        "Consider two interacting populations of leaky integrate-and-fire neurons: excitatory ($N_E$ neurons) and inhibitory ($N_I$ neurons). Resting potential is set to zero ($u_{\\text{rest}}=0$). Both populations have identical parameters $\\vartheta$, $\\tau_m$, $R$, and $u_r$, and are driven by a common external current $I^{\\text{ext}}$.\n",
        "\n",
        "Each neuron receives $C_E$ excitatory synapses with weight $w_E > 0$ and $C_I$ inhibitory synapses with weight $w_I < 0$. The membrane potential jump caused by an input spike is\n",
        "$$\n",
        "\\Delta u_E = \\frac{w_E q R}{\\tau_m}, \\quad \\Delta u_I = \\frac{w_I}{w_E} \\Delta u_E.\n",
        "$$\n",
        "\n",
        "Define ratios:\n",
        "$$\n",
        "\\gamma = \\frac{C_I}{C_E}, \\quad g = - \\frac{w_I / \\Delta u_I}{w_E / \\Delta u_E} = -\\frac{w_I}{w_E}.\n",
        "$$\n",
        "\n",
        "Assuming a common firing rate $\\nu$ for both populations, the mean input current is:\n",
        "$$\n",
        "I_0 = I_0^{\\text{ext}} + q \\nu w_E C_E (1 - \\gamma g).\n",
        "$$\n",
        "\n",
        "Noise is more conveniently measured by the variance of the membrane potential $\\sigma_u^2$ (rather than input variance $\\sigma_I^2$):\n",
        "$$\n",
        "\\sigma_u^2 = 0.5 \\sigma^2, \\quad \\text{where} \\quad \\sigma^2 = \\nu \\tau_m (\\Delta u_E)^2 C_E (1 + \\gamma g^2).\n",
        "$$\n",
        "\n",
        "The stationary firing rate $A_0 = \\nu$ is given by the leaky integrate-and-fire gain function:\n",
        "$$\n",
        "\\frac{1}{A_0} = \\nu = g_{\\sigma}(I_0) = \\tau_m \\sqrt{\\pi} \\int_{\\frac{u_r - R I_0}{\\sigma}}^{\\frac{\\vartheta - R I_0}{\\sigma}} e^{x^2} [1 + \\mathrm{erf}(x)] dx.\n",
        "$$\n",
        "\n",
        "To find the stationary state, the equations must be solved simultaneously for $\\nu$ and $\\sigma$. Numerical methods are required as the gain function depends on noise level $\\sigma$.\n",
        "\n",
        "#### Example: Balanced Excitation-Inhibition Network\n",
        "\n",
        "Parameters: $\\vartheta=1$, $q=1$, $R=1$, $\\tau_m=10\\,ms$, $h = R I_0$. Target mean input $h_0=0.8$, variance $\\sigma=0.2$, corresponding firing rate $\\nu \\approx 16\\, Hz$ (Fig. 12.17a).\n",
        "\n",
        "Set $\\Delta u_E = 0.025$ (40 simultaneous spikes needed to trigger firing). Equal excitation and inhibition: $w_I = -w_E$, hence $g=1$, and $C_E = C_I$, so $\\gamma=1$. This balance leads to mean input $I^{\\text{ext}} = 0.8$ and $C_E = C_I = 200$ to satisfy variance constraint.\n",
        "\n",
        "Both silent and active states are possible solutions; stability analysis is beyond this section.\n",
        "\n",
        "#### Example: Inhibition-Dominated Network\n",
        "\n",
        "Cortical-like parameters: $N_E=8000$, $N_I=2000$, connection probability $p=0.1$, $C_E=800$, $C_I=200$, so $\\gamma=1/4$. Voltage jumps $\\Delta u_E=0.025$, $\\Delta u_I=-0.125$ imply $g=5$. Strong inhibition dominates feedback ($\\gamma g > 1$).\n",
        "\n",
        "For spontaneous firing $\\nu=8\\,Hz$, variance $\\sigma \\approx 0.54$. The gain function relates this to $h_0 \\approx 0.2$. The effective coupling is:\n",
        "$$\n",
        "J_{\\text{eff}} = \\tau_m C_E \\Delta u_E (1 - \\gamma g),\n",
        "$$\n",
        "yielding an external input\n",
        "$$\n",
        "h_0^{\\text{ext}} = h_0 - J_{\\text{eff}} A_0 \\approx 0.6.\n",
        "$$\n",
        "\n",
        "External input can represent stochastic spikes from other brain areas, modeled as:\n",
        "$$\n",
        "h_0 = \\tau_m \\nu \\Delta u_E C_E (1 - \\gamma g) + \\tau_m \\nu^{\\text{ext}} \\Delta u^{\\text{ext}} C^{\\text{ext}},\n",
        "$$\n",
        "with increased membrane potential variance:\n",
        "$$\n",
        "\\sigma_u^2 = 0.5 \\tau_m \\nu (\\Delta u_E)^2 C_E (1 + \\gamma g^2) + 0.5 \\tau_m \\nu^{\\text{ext}} (\\Delta u^{\\text{ext}})^2 C^{\\text{ext}}.\n",
        "$$\n",
        "\n",
        "These coupled equations can be solved numerically to characterize stationary asynchronous firing states in randomly connected excitatory-inhibitory networks."
      ],
      "metadata": {
        "id": "NDecy9MCFiVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 Synaptic plasticity (STDP)"
      ],
      "metadata": {
        "id": "_WtG-kElHPem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Hebb Rule and Experiments\n",
        "\n",
        "### Hebb’s Postulate on Synaptic Plasticity\n",
        "\n",
        "Since the 1970s, extensive experimental evidence has supported the concept of synaptic plasticity inspired by Hebb’s postulate (Hebb, 1949). Hebb proposed that when a presynaptic neuron A repeatedly contributes to firing a postsynaptic neuron B, synaptic changes occur to increase the efficacy of A's connection to B. Formally:\n",
        "\n",
        "\"When an axon of cell A is near enough to excite cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased.\"\n",
        "\n",
        "This principle is often summarized as: neurons that \"fire together, wire together\" (Shatz, 1992). Unlike the simplified phrase, Hebb's original formulation has an inherent temporal asymmetry: the presynaptic neuron must be active slightly before the postsynaptic neuron to contribute causally to its firing.\n",
        "\n",
        "Hebbian learning is unsupervised; synaptic changes occur solely based on the correlated activity patterns of pre- and postsynaptic neurons without external feedback or reward signals.\n",
        "\n",
        "### Long-Term Potentiation (LTP)\n",
        "\n",
        "Long-Term Potentiation (LTP) is the classic experimental demonstration of Hebbian synaptic modification. The typical experimental paradigm is as follows:\n",
        "\n",
        "1. A weak test pulse applied to presynaptic fibers elicits a postsynaptic potential without generating action potentials.\n",
        "2. A high-frequency stimulation train applied to the presynaptic fibers triggers postsynaptic firing.\n",
        "3. Subsequent test pulses evoke postsynaptic potentials with significantly increased amplitude.\n",
        "\n",
        "This increase in synaptic strength persists for hours, hence the term \"long-term\" potentiation.\n",
        "\n",
        "Mathematically, if $w_{ij}$ denotes the synaptic weight from presynaptic neuron $j$ to postsynaptic neuron $i$, LTP corresponds to an increase in $w_{ij}$ after correlated high-frequency activation.\n",
        "\n",
        "#### Voltage Dependence of LTP\n",
        "\n",
        "Intracellular recordings reveal that postsynaptic membrane potential critically influences LTP induction. Specifically, if presynaptic spikes coincide with strong postsynaptic depolarization, LTP is induced. Conversely, if postsynaptic depolarization is weak during presynaptic spikes, Long-Term Depression (LTD) occurs.\n",
        "\n",
        "Thus, synaptic changes depend on the postsynaptic voltage $V$ during spike timing:\n",
        "\n",
        "- Strong depolarization: $w_{ij}$ increases (LTP)\n",
        "- Weak depolarization: $w_{ij}$ decreases (LTD)\n",
        "\n",
        "This voltage dependence aligns with Hebb’s principle, where postsynaptic activity (depolarization) is necessary for synaptic strengthening.\n",
        "\n",
        "### Spike-Timing-Dependent Plasticity (STDP)\n",
        "\n",
        "STDP refines Hebbian learning by quantifying how synaptic changes depend on the precise timing difference between pre- and postsynaptic spikes.\n",
        "\n",
        "Consider spike times $t_j^f$ (presynaptic neuron $j$) and $t_i^f$ (postsynaptic neuron $i$). The synaptic weight change $\\Delta w_{ij}$ depends on the difference $\\Delta t = t_j^f - t_i^f$:\n",
        "\n",
        "- If $t_j^f < t_i^f$ (presynaptic spike precedes postsynaptic spike by a few milliseconds), then $\\Delta w_{ij} > 0$ (potentiation).\n",
        "- If $t_j^f > t_i^f$ (presynaptic spike follows postsynaptic spike), then $\\Delta w_{ij} < 0$ (depression).\n",
        "\n",
        "This defines an asymmetric temporal window of plasticity, often modeled as:\n",
        "\n",
        "$$\n",
        "\\Delta w_{ij}(\\Delta t) =\n",
        "\\begin{cases}\n",
        "A_+ e^{-\\frac{\\Delta t}{\\tau_+}}, & \\text{if } \\Delta t > 0 \\\\\n",
        "- A_- e^{\\frac{\\Delta t}{\\tau_-}}, & \\text{if } \\Delta t < 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "where $A_+, A_-$ are scaling constants, and $\\tau_+, \\tau_-$ are time constants of potentiation and depression, respectively.\n",
        "\n",
        "This temporal asymmetry captures the causal relation inherent in Hebb’s postulate: only presynaptic spikes that slightly precede postsynaptic spikes contribute to strengthening.\n",
        "\n",
        "#### Variations and Anti-Hebbian Plasticity\n",
        "\n",
        "While Hebbian STDP is common in many brain areas, some synapses exhibit anti-Hebbian plasticity, where the synapse weakens if presynaptic spikes precede postsynaptic spikes (e.g., cerebellar parallel fiber-Purkinje cell synapses).\n"
      ],
      "metadata": {
        "id": "iX0a-3V8FxUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Models of Hebbian Learning\n",
        "\n",
        "### Overview of Correlation-Based Learning\n",
        "\n",
        "Before discussing spike-based learning rules, Hebbian learning can be understood in a firing rate framework. Firing rate models describe neural activity by average firing rates rather than precise spike timing and have been widely used in artificial neural networks.\n",
        "\n",
        "### 5.2.1 Mathematical Formulation of Hebb's Rule\n",
        "\n",
        "Consider a synapse from presynaptic neuron $j$ to postsynaptic neuron $i$ with synaptic efficacy $w_{ij}$. The presynaptic and postsynaptic firing rates are denoted by $\\nu_j$ and $\\nu_i$, respectively. Hebbian plasticity obeys two key principles:\n",
        "\n",
        "1. **Locality:** Synaptic changes depend only on local variables available at the synapse—namely, $\\nu_i$, $\\nu_j$, and $w_{ij}$.\n",
        "2. **Joint activity:** Changes require simultaneous activity of pre- and postsynaptic neurons.\n",
        "\n",
        "The general learning rule can be written as a differential equation:\n",
        "\n",
        "$$\n",
        "\\frac{d w_{ij}}{dt} = F(w_{ij}; \\nu_i, \\nu_j)\n",
        "$$\n",
        "\n",
        "where $F$ is an unknown function of local variables.\n",
        "\n",
        "Expanding $F$ in a Taylor series around zero activity ($\\nu_i = \\nu_j = 0$):\n",
        "\n",
        "$$\n",
        "\\frac{d w_{ij}}{dt} = c_0(w_{ij}) + c_1(w_{ij}) \\nu_j + c_1(w_{ij}) \\nu_i + c_2(w_{ij}) \\nu_j^2 + c_2(w_{ij}) \\nu_i^2 + c_{11}^{\\text{corr}}(w_{ij}) \\nu_i \\nu_j + \\mathcal{O}(\\nu^3)\n",
        "$$\n",
        "\n",
        "The bilinear term $c_{11}^{\\text{corr}}(w_{ij}) \\nu_i \\nu_j$ captures the joint activity requirement of Hebb's postulate. A learning rule without this term is considered **non-Hebbian**.\n",
        "\n",
        "#### Prototype Hebbian Learning Rule\n",
        "\n",
        "Setting all terms except the bilinear one to zero and fixing $c_{11}^{\\text{corr}} = \\gamma_2 > 0$, we obtain:\n",
        "\n",
        "$$\n",
        "\\frac{d w_{ij}}{dt} = \\gamma_2 \\nu_i \\nu_j\n",
        "$$\n",
        "\n",
        "If $\\gamma_2 < 0$, the rule is **anti-Hebbian**, causing synaptic weakening when both neurons are active.\n",
        "\n",
        "#### Weight Saturation\n",
        "\n",
        "To prevent unbounded growth of synaptic weights, $c_{11}^{\\text{corr}}$ can depend on $w_{ij}$. Two common approaches:\n",
        "\n",
        "- **Hard bound:** $\\gamma_2$ constant for $0 < w_{ij} < w_{\\max}$, zero otherwise; weight growth stops abruptly at $w_{\\max}$.\n",
        "- **Soft bound:** $\\gamma_2$ decreases smoothly as $w_{ij}$ approaches $w_{\\max}$, e.g.,\n",
        "\n",
        "$$\n",
        "c_{11}^{\\text{corr}}(w_{ij}) = \\gamma_2 (w_{\\max} - w_{ij})^{\\beta}\n",
        "$$\n",
        "\n",
        "with $\\beta > 0$ (usually $\\beta=1$).\n",
        "\n",
        "#### Combining Potentiation and Depression\n",
        "\n",
        "Introducing a decay term $c_0(w_{ij}) = -\\gamma_0 w_{ij}$ with $w_{\\max} = \\beta = 1$, the learning rule becomes:\n",
        "\n",
        "$$\n",
        "\\frac{d w_{ij}}{dt} = \\gamma_2 (1 - w_{ij}) \\nu_i \\nu_j - \\gamma_0 w_{ij}\n",
        "$$\n",
        "\n",
        "Here, synapses decay toward zero in the absence of activity, allowing both potentiation and depression.\n",
        "\n",
        "### Examples of Hebbian Learning Rules\n",
        "\n",
        "#### Covariance Rule\n",
        "\n",
        "Sejnowski proposed the covariance rule which uses deviations from mean firing rates $\\langle \\nu_i \\rangle$, $\\langle \\nu_j \\rangle$:\n",
        "\n",
        "$$\n",
        "\\frac{d w_{ij}}{dt} = \\gamma (\\nu_i - \\langle \\nu_i \\rangle)(\\nu_j - \\langle \\nu_j \\rangle)\n",
        "$$\n",
        "\n",
        "This emphasizes learning from correlated fluctuations rather than absolute firing rates.\n",
        "\n",
        "#### Oja's Rule\n",
        "\n",
        "Adding a quadratic decay term, Oja's rule is:\n",
        "\n",
        "$$\n",
        "\\frac{d w_{ij}}{dt} = \\gamma \\left[ \\nu_i \\nu_j - w_{ij} \\nu_i^2 \\right]\n",
        "$$\n",
        "\n",
        "Oja's rule normalizes synaptic weights to maintain $\\sum_j w_{ij}^2 = 1$, introducing competition among synapses onto the same neuron.\n",
        "\n",
        "#### Bienenstock–Cooper–Munro (BCM) Rule\n",
        "\n",
        "The BCM rule introduces a nonlinear function $\\phi$ with a sliding threshold $\\nu_{\\theta}$:\n",
        "\n",
        "$$\n",
        "\\frac{d w_{ij}}{dt} = \\phi(\\nu_i - \\nu_{\\theta}) \\nu_j\n",
        "$$\n",
        "\n",
        "A common choice is:\n",
        "\n",
        "$$\n",
        "\\frac{d w_{ij}}{dt} = \\eta \\nu_i (\\nu_i - \\nu_{\\theta}) \\nu_j = c_{21} \\nu_i^2 \\nu_j - c_{11}^{\\text{corr}} \\nu_i \\nu_j\n",
        "$$\n",
        "\n",
        "where $c_{21} = \\eta$ and $c_{11}^{\\text{corr}} = - \\eta \\nu_{\\theta}$. The threshold $\\nu_{\\theta}$ is typically adaptive, depending on the average postsynaptic activity $\\langle \\nu_i \\rangle$, which stabilizes the learning and avoids runaway synaptic growth.\n",
        "\n",
        "The BCM rule explains synaptic depression at moderate postsynaptic activity and potentiation at high activity, consistent with experimental observations. It has been successfully applied to receptive field development."
      ],
      "metadata": {
        "id": "ncre_GN_F816"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2.2 Pair-based Models of Spike-Timing-Dependent Plasticity (STDP)\n",
        "\n",
        "Traditional rate-based models of synaptic plasticity are replaced here by spike-based descriptions. Consider a presynaptic spike occurring at time $t_{\\text{pre}}$ and a postsynaptic spike at time $t_{\\text{post}}$. Pair-based STDP models update synaptic weights based on the temporal difference $\\Delta t = t_{\\text{post}} - t_{\\text{pre}}$.\n",
        "\n",
        "The simplest pair-based update rule is given by:  \n",
        "For $t_{\\text{pre}} < t_{\\text{post}}$ (post follows pre, potentiation):  \n",
        "$$\n",
        "\\Delta w^+ = A^+(w) \\cdot \\exp\\left(-\\frac{|\\Delta t|}{\\tau_+}\\right)\n",
        "$$\n",
        "\n",
        "For $t_{\\text{post}} < t_{\\text{pre}}$ (pre follows post, depression):  \n",
        "$$\n",
        "\\Delta w^- = A^-(w) \\cdot \\exp\\left(-\\frac{|\\Delta t|}{\\tau_-}\\right)\n",
        "$$\n",
        "\n",
        "Here, $A^{\\pm}(w)$ denote the amplitude parameters which may depend on the current synaptic weight $w$, and $\\tau_{\\pm}$ are time constants governing the decay of the learning window.\n",
        "\n",
        "Updates happen immediately after each presynaptic spike ($t_{\\text{pre}}$) and postsynaptic spike ($t_{\\text{post}}$). A full pair-based model is specified by:  \n",
        "(i) the weight dependence of $A^{\\pm}(w)$,  \n",
        "(ii) which spike pairs are considered (e.g., all pairs or only nearest pairs).  \n",
        "\n",
        "Because of the exponential decay, spike pairs with large $|\\Delta t|$ contribute little to weight change. Alternative formulations replace the exponential decay with arbitrary learning windows $W^{\\pm}(s)$ for Long-Term Potentiation (LTP) and Long-Term Depression (LTD).\n",
        "\n",
        "Expressing spike trains as $S_j(t) = \\sum_f \\delta(t - t_j^f)$ for presynaptic neuron $j$ and $S_i(t) = \\sum_f \\delta(t - t_i^f)$ for postsynaptic neuron $i$, the weight update rule can be written as (Kistler and van Hemmen, 2000):  \n",
        "$$\n",
        "\\frac{dw_{ij}(t)}{dt} = S_j(t) \\left[ a_{\\text{pre}} + \\int_0^\\infty A^-(w_{ij}) W^-(s) S_i(t - s) ds \\right] + S_i(t) \\left[ a_{\\text{post}} + \\int_0^\\infty A^+(w_{ij}) W^+(s) S_j(t - s) ds \\right]\n",
        "$$\n",
        "\n",
        "Here, $a_{\\text{pre}}$ and $a_{\\text{post}}$ are non-Hebbian contributions analogous to parameters in rate-based models. The standard pair-based STDP corresponds to $W^{\\pm}(s) = \\exp(-s/\\tau_{\\pm})$ and $a_{\\text{pre}} = a_{\\text{post}} = 0$.\n",
        "\n",
        "#### Implementation by Local Variables\n",
        "\n",
        "Pair-based STDP can be implemented using traces representing low-pass filtered spike trains. Define:  \n",
        "- Presynaptic trace $x_j(t)$ evolving as  \n",
        "$$\n",
        "\\frac{dx_j}{dt} = -\\frac{x_j}{\\tau_+} + \\sum_f \\delta(t - t_j^f)\n",
        "$$  \n",
        "- Postsynaptic trace $y_i(t)$ evolving as  \n",
        "$$\n",
        "\\frac{dy_i}{dt} = -\\frac{y_i}{\\tau_-} + \\sum_f \\delta(t - t_i^f)\n",
        "$$\n",
        "\n",
        "At the moment of a presynaptic spike, weight depression is proportional to the postsynaptic trace $y_i(t)$. At the moment of a postsynaptic spike, potentiation is proportional to the presynaptic trace $x_j(t)$:\n",
        "\n",
        "$$\n",
        "\\frac{dw_{ij}}{dt} = -A^-(w_{ij}) y_i(t) \\sum_f \\delta(t - t_j^f) + A^+(w_{ij}) x_j(t) \\sum_f \\delta(t - t_i^f)\n",
        "$$\n",
        "\n",
        "The traces $x_j(t)$ and $y_i(t)$ correspond to the exponential factors in the original update rule, and $A^{\\pm}$ can implement hard or soft bounds on synaptic weights.\n",
        "\n",
        "### 5.2.3 Generalized STDP Models\n",
        "\n",
        "Pair-based STDP models fail to fully reproduce experimental observations, especially frequency-dependent effects and responses to spike triplets or quadruplets.\n",
        "\n",
        "For example, increasing the repetition frequency $\\rho$ of spike pairs experimentally leads to increased potentiation, but pair-based STDP predicts more depression and less potentiation. Moreover, triplet protocols such as pre-post-pre and post-pre-post sequences experimentally show distinct results despite containing the same number of pre-post and post-pre pairs.\n",
        "\n",
        "#### Triplet STDP Model (Pfister and Gerstner, 2006)\n",
        "\n",
        "This model introduces a triplet interaction, considering sets of three spikes (one presynaptic and two postsynaptic). It is implemented by local variables:\n",
        "\n",
        "- Presynaptic trace $x_j(t)$ as before:\n",
        "$$\n",
        "\\frac{dx_j}{dt} = -\\frac{x_j}{\\tau_+} + \\sum_f \\delta(t - t_j^f)\n",
        "$$\n",
        "\n",
        "- Two postsynaptic traces, a fast trace $y_{i,1}(t)$ and a slow trace $y_{i,2}(t)$:\n",
        "$$\n",
        "\\frac{dy_{i,1}}{dt} = -\\frac{y_{i,1}}{\\tau_1} + \\sum_f \\delta(t - t_i^f)\n",
        "$$\n",
        "$$\n",
        "\\frac{dy_{i,2}}{dt} = -\\frac{y_{i,2}}{\\tau_2} + \\sum_f \\delta(t - t_i^f)\n",
        "$$\n",
        "with $\\tau_1 < \\tau_2$.\n",
        "\n",
        "The weight update rule for potentiation is triplet-based:  \n",
        "$$\n",
        "\\Delta w^+ = A^+(w_{ij}) \\, x_j(t_i^f) \\, y_{i,2}(t_i^{f-})\n",
        "$$  \n",
        "where $y_{i,2}(t_i^{f-})$ is evaluated immediately before the postsynaptic spike at $t_i^f$.\n",
        "\n",
        "Depression is similar to pair-based STDP and proportional to the fast postsynaptic trace $y_{i,1}$ at presynaptic spikes.\n",
        "\n",
        "This triplet rule matches frequency-dependent experimental data and maps onto the BCM theory, reproducing key properties such as input selectivity and a sliding threshold between potentiation and depression.\n",
        "\n",
        "### Example: Voltage-Dependent Plasticity Model (Clopath et al., 2010)\n",
        "\n",
        "Beyond spike timing, synaptic plasticity depends on the postsynaptic membrane voltage $u(t)$. The Clopath model separates LTP and LTD contributions with voltage dependence.\n",
        "\n",
        "For LTD, the synaptic weight $w_{ij}$ decreases at each presynaptic spike by an amount proportional to the rectified depolarization above threshold $\\theta_-$:  \n",
        "$$\n",
        "\\frac{d w_{ij}^{\\text{LTD}}}{dt} = -A_{\\text{LTD}}(\\bar{u}_i) S_j(t) \\left[ u_{i,-}(t) - \\theta_- \\right]_+\n",
        "$$  \n",
        "provided $w_{ij} > w_{\\min}$, where $[x]_+ = \\max(x,0)$ and $S_j(t) = \\sum_f \\delta(t - t_j^f)$.\n",
        "\n",
        "Here, $u_{i,-}(t)$ is a low-pass filtered postsynaptic membrane potential with time constant $\\tau_-$:  \n",
        "$$\n",
        "\\frac{d u_{i,-}}{dt} = -\\frac{u_{i,-}}{\\tau_-} + u_i(t)\n",
        "$$\n",
        "\n",
        "The amplitude parameter $A_{\\text{LTD}}(\\bar{u}_i)$ depends on the slow average depolarization $\\bar{u}_i$ over ~1 second.\n",
        "\n",
        "This model integrates voltage-dependence with spike timing to capture more complex synaptic plasticity phenomena."
      ],
      "metadata": {
        "id": "8QfZhyzZGHKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Unsupervised Learning in Computational Neuroscience\n",
        "\n",
        "Unsupervised learning in artificial neural networks involves changes in synaptic connections driven purely by the statistics of input stimuli, without explicit target outputs or reward signals. This contrasts with supervised or reward-based learning, where parameters are adjusted to optimize a specific output behavior for each stimulus. Hebbian learning rules represent a canonical form of unsupervised learning, where synaptic strengths are modified based on the correlation of pre- and postsynaptic activity.\n",
        "\n",
        "### Voltage-Dependent Plasticity: The Clopath Model\n",
        "\n",
        "Synaptic plasticity can depend on both presynaptic activity and postsynaptic membrane potential dynamics. The Clopath model describes such voltage-dependent plasticity:\n",
        "\n",
        "- **Long-Term Depression (LTD)** occurs if a presynaptic spike $x_j$ arrives when the low-pass filtered postsynaptic membrane potential $u_{i,-}$ exceeds a threshold $\\theta_-$.\n",
        "  \n",
        "- **Long-Term Potentiation (LTP)** requires three conditions:  \n",
        "  1. The instantaneous postsynaptic potential $u_i$ is above a higher threshold $\\theta_+$.  \n",
        "  2. The low-pass filtered potential $u_{i,+}$ is above $\\theta_-$.  \n",
        "  3. The presynaptic low-pass filtered activity $\\bar{x}_j$ is positive.\n",
        "\n",
        "This model captures the dependency of synaptic changes on the timing and voltage state of neurons.\n",
        "\n",
        "### Static Pattern Scenario\n",
        "\n",
        "Consider $N$ input neurons indexed by $j = 1, \\dots, N$ each with firing rates $\\nu_j$. Inputs are drawn from $P$ firing rate patterns $\\xi^\\mu = (\\xi_1^\\mu, \\dots, \\xi_N^\\mu)$, where $\\mu = 1, \\dots, P$. At any time, the input rates equal a particular pattern $\\nu = \\xi^\\mu$. After time $\\Delta t$, a new pattern is randomly chosen, constituting the static pattern scenario.\n",
        "\n",
        "### 5.3.1 Competitive Learning\n",
        "\n",
        "A Hebbian learning rule can be defined as:\n",
        "\n",
        "$$\n",
        "\\frac{d w_{ij}}{dt} = \\gamma \\nu_i \\left[ \\nu_j - \\nu_\\theta(w_{ij}) \\right]\n",
        "$$\n",
        "\n",
        "where $\\gamma > 0$ is the learning rate, $\\nu_i$ is the postsynaptic firing rate, and $\\nu_\\theta(w_{ij})$ is a reference function dependent on $w_{ij}$.\n",
        "\n",
        "When $\\nu_i > 0$, synapses from highly active presynaptic neurons ($\\nu_j > \\nu_\\theta$) to postsynaptic neuron $i$ are strengthened (LTP), while inactive synapses weaken (heterosynaptic LTD). This dual effect enforces competition among synapses.\n",
        "\n",
        "A special case is when:\n",
        "\n",
        "$$\n",
        "\\nu_\\theta(w_{ij}) = w_{ij}\n",
        "$$\n",
        "\n",
        "which leads to the learning rule:\n",
        "\n",
        "$$\n",
        "\\frac{d w_{ij}}{dt} = \\gamma \\nu_i (\\nu_j - w_{ij})\n",
        "$$\n",
        "\n",
        "At equilibrium, $w_{ij} = \\nu_j$ for active postsynaptic neurons, meaning the synaptic weights store the input firing pattern.\n",
        "\n",
        "#### Implementation of Competitive Learning\n",
        "\n",
        "In a network of $K$ postsynaptic neurons receiving input from the same $N$ presynaptic neurons, strong lateral inhibition ensures only one postsynaptic neuron \"wins\" per stimulus. The winning neuron updates its weights according to the Hebbian rule, moving its weight vector $w_i$ closer to the input pattern $\\nu$. Different neurons specialize for different clusters of input patterns.\n",
        "\n",
        "#### Example: Developmental Learning with Spike-Timing Dependent Plasticity (STDP)\n",
        "\n",
        "In a simulation of the Clopath model with 10 excitatory neurons and 3 inhibitory neurons receiving inputs from 500 Poisson spike trains with spatially correlated firing rates, neurons developed localized receptive fields. Feedforward and recurrent synaptic weights organized so that neurons with similar receptive fields formed strongly connected subnetworks, demonstrating soft competitive learning through inhibition and plasticity.\n",
        "\n",
        "### 5.3.2 Learning Equations for Rate Models\n",
        "\n",
        "Consider a single postsynaptic neuron receiving input from $N$ presynaptic neurons with firing rates $\\nu_j$ and synaptic weights $w_j$ (index $i$ omitted for clarity). The postsynaptic firing rate $\\nu_{\\text{post}}$ is a function of the weighted input sum:\n",
        "\n",
        "$$\n",
        "\\nu_{\\text{post}} = g\\left( \\sum_{j=1}^N w_j \\nu_j \\right)\n",
        "$$\n",
        "\n",
        "For simplicity, consider a linear gain function:\n",
        "\n",
        "$$\n",
        "\\nu_{\\text{post}} = \\sum_{j=1}^N w_j \\nu_j = \\mathbf{w} \\cdot \\mathbf{\\nu}\n",
        "$$\n",
        "\n",
        "A simple Hebbian learning rule is:\n",
        "\n",
        "$$\n",
        "\\Delta w_i = \\gamma \\nu_{\\text{post}} \\nu_i\n",
        "$$\n",
        "\n",
        "where $\\gamma$ is a small learning rate.\n",
        "\n",
        "If patterns $\\xi^\\mu$ are presented in sequence, the weight update becomes:\n",
        "\n",
        "$$\n",
        "\\Delta w_i = \\gamma \\sum_{j=1}^N w_j \\xi_j^\\mu \\xi_i^\\mu\n",
        "$$\n",
        "\n",
        "This iteration:\n",
        "\n",
        "$$\n",
        "w_i(n+1) = w_i(n) + \\gamma \\sum_{j=1}^N w_j(n) \\xi_j^{\\mu_n} \\xi_i^{\\mu_n}\n",
        "$$\n",
        "\n",
        "leads the weight vector to evolve in the direction of the input pattern statistics.\n",
        "\n",
        "### Relation to Principal Component Analysis (PCA)\n",
        "\n",
        "If input patterns $\\xi^\\mu$ have a non-zero mean, weights evolve toward the mean vector. After centering input data (zero mean), the weights align with the first principal component of the input data covariance matrix. Thus, unsupervised Hebbian learning can perform PCA-like feature extraction, learning the dominant variance direction of the input space."
      ],
      "metadata": {
        "id": "muo2sjBAGSMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3.3 Learning Equations for STDP Models\n",
        "\n",
        "In the pair-based spike-timing-dependent plasticity (STDP) model, the evolution of synaptic weights can be analyzed by modeling pre- and postsynaptic spike trains as Poisson processes.\n",
        "\n",
        "#### Postsynaptic Neuron Firing Rate Model\n",
        "\n",
        "The postsynaptic neuron output is modeled as an inhomogeneous Poisson process with instantaneous firing rate\n",
        "\n",
        "$$\n",
        "\\nu_i(u_i) = [\\alpha u_i - \\nu_0]_+\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is a scaling factor, $\\nu_0$ is a threshold, and $u_i(t)$ is the membrane potential defined as\n",
        "\n",
        "$$\n",
        "u_i(t) = \\sum_j w_{ij} \\epsilon(t - t_j^f)\n",
        "$$\n",
        "\n",
        "with $\\epsilon(t)$ representing the excitatory postsynaptic potential (EPSP) generated by presynaptic spikes at times $t_j^f$. The notation $[x]_+ = x$ if $x > 0$, and 0 otherwise; here, we assume the argument is positive, allowing us to omit the brackets.\n",
        "\n",
        "Assuming all presynaptic inputs are independent Poisson processes with constant firing rates $\\nu_j$, the expected postsynaptic firing rate is\n",
        "\n",
        "$$\n",
        "\\langle \\nu_i \\rangle = -\\nu_0 + \\alpha \\bar{\\epsilon} \\sum_j w_{ij} \\nu_j\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\bar{\\epsilon} = \\int \\epsilon(s) \\, ds\n",
        "$$\n",
        "\n",
        "is the total area under the EPSP curve.\n",
        "\n",
        "The conditional postsynaptic firing rate, given a presynaptic spike at time $t_j$, becomes\n",
        "\n",
        "$$\n",
        "\\nu_i(t) = -\\nu_0 + \\alpha \\bar{\\epsilon} \\sum_j w_{ij} \\nu_j + \\alpha w_{ij} \\epsilon(t - t_j^f)\n",
        "$$\n",
        "\n",
        "indicating a correlation between pre- and postsynaptic spike trains.\n",
        "\n",
        "#### Spike Train Correlations\n",
        "\n",
        "The correlation function between the postsynaptic spike train $S_i(t)$ and presynaptic spike train $S_j(t)$ at lag $s$ is\n",
        "\n",
        "$$\n",
        "\\Gamma_{ji}(s) = \\langle S_i(t+s) S_j(t) \\rangle = \\langle \\nu_i \\rangle \\nu_j + \\alpha w_{ij} \\nu_j \\epsilon(s)\n",
        "$$\n",
        "\n",
        "#### Expected Weight Evolution under STDP\n",
        "\n",
        "Using the pair-based STDP plasticity rule, the expected evolution of synaptic weights is given by\n",
        "\n",
        "$$\n",
        "\\langle \\dot{w}_{ij} \\rangle = \\nu_j \\langle \\nu_i \\rangle [-A_-(w_{ij}) \\tau_- + A_+(w_{ij}) \\tau_+] + \\alpha w_{ij} \\nu_j A_+(w_{ij}) \\int W_+(s) \\epsilon(s) \\, ds\n",
        "$$\n",
        "\n",
        "Here, the first term corresponds to rate-based Hebbian learning with a coefficient related to the integral over the learning window. The second term captures the effect of pre-before-post spike timing, which is absent in purely rate-based models.\n",
        "\n",
        "#### Example: Stabilization of Postsynaptic Firing Rate\n",
        "\n",
        "When all input spike rates are equal $\\nu_j = \\nu$ and weights are uniform $w_{ij} = w$, the learning equation simplifies to\n",
        "\n",
        "$$\n",
        "\\dot{w} = \\nu_i [-A_-(w) \\tau_- + A_+(w) \\tau_+] + \\alpha w A_+(w) \\nu \\int W_+(s) \\epsilon(s) \\, ds\n",
        "$$\n",
        "\n",
        "with the postsynaptic firing rate\n",
        "\n",
        "$$\n",
        "\\nu_i = -\\nu_0 + \\alpha \\nu \\bar{\\epsilon} N w\n",
        "$$\n",
        "\n",
        "where $N$ is the number of presynaptic inputs.\n",
        "\n",
        "Introducing\n",
        "\n",
        "$$\n",
        "C = A_-(w) \\tau_- - A_+(w) \\tau_+\n",
        "$$\n",
        "\n",
        "and assuming the integral of the learning window is negative so that $C > 0$ (long-term depression dominates), a fixed point firing rate $\\nu_{\\text{FP}}$ exists:\n",
        "\n",
        "$$\n",
        "\\nu_{\\text{FP}} = \\frac{C \\nu_0}{N C \\nu \\bar{\\epsilon} - C_{\\text{ss}}}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "C_{\\text{ss}} = A_+(w) \\int W_+(s) \\epsilon(s) \\, ds\n",
        "$$\n",
        "\n",
        "is the contribution from spike-timing correlations.\n",
        "\n",
        "Under general conditions, this fixed point stabilizes the postsynaptic firing rate, implying that spike-timing-dependent plasticity can regulate neural activity homeostatically. The stability and existence of this fixed point do not critically depend on soft or hard bounds on synaptic weights, as output rate stabilization inherently normalizes the total synaptic weight."
      ],
      "metadata": {
        "id": "QAv2VLBeGXq2"
      }
    }
  ]
}